{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bb192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.stats import gamma as gamma_dist\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob\n",
    "import sys\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "project_root = Path.cwd().parents[1] # Up from learningnotebooks/phase4_sar_codec/\n",
    "print(project_root)\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from utils.io import load_sar_image, find_all_sar_files, get_info\n",
    "from data.preprocessing import preprocess_sar_complete, extract_patches\n",
    "\n",
    "\n",
    "rawdir = project_root/\"data\"/\"raw\"\n",
    "outputdir = project_root/\"data\"/\"patches\"\n",
    "checkpointdir = project_root/\"checkpoints\"\n",
    "patchsize = 256\n",
    "stride = 128\n",
    "print(rawdir, outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a75cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_files = find_all_sar_files(rawdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84050e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_folders = list(rawdir.glob(\"*.SAFE\"))\n",
    "print(f\"Total .SAFE folders: {len(safe_folders)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = []\n",
    "incomplete = []\n",
    "\n",
    "for folder in safe_folders:\n",
    "    measurement_dir = folder / \"measurement\"\n",
    "    if measurement_dir.exists():\n",
    "        tiffs = list(measurement_dir.glob(\"*.tiff\"))\n",
    "        if len(tiffs) >= 2:\n",
    "            complete.append(folder.name)\n",
    "        else:\n",
    "            incomplete.append((folder.name, len(tiffs)))\n",
    "    else:\n",
    "        incomplete.append((folder.name, \"no measurement folder\"))\n",
    "\n",
    "print(f\"Complete: {len(complete)}\")\n",
    "print(f\"Incomplete: {len(incomplete)}\\n\")\n",
    "\n",
    "if incomplete:\n",
    "    print(\"Incomplete folders:\")\n",
    "    for name, issue in incomplete:\n",
    "        print(f\"  {name}: {issue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = rawdir / \"S1A_IW_GRDH_1SDV_20260117T061452_20260117T061517_062803_07E08D_824A.SAFE\"\n",
    "\n",
    "print(f\"Folder exists: {folder.exists()}\")\n",
    "print(f\"\\nContents:\")\n",
    "for item in folder.iterdir():\n",
    "    print(f\"  {item.name}\")\n",
    "    \n",
    "# Check if measurement exists with different casing\n",
    "print(f\"\\nMeasurement folder check:\")\n",
    "print(f\"  'measurement' exists: {(folder / 'measurement').exists()}\")\n",
    "print(f\"  'MEASUREMENT' exists: {(folder / 'MEASUREMENT').exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(sar_files)} TIFF files:\\n\")\n",
    "for f in sar_files:\n",
    "    info = get_info(f)\n",
    "    print(f\"  {info['satellite'].upper()} | {info.get('date', '?')} | {Path(f).name[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = sar_files[0]\n",
    "import rasterio\n",
    "import time\n",
    "with rasterio.open(path) as src:\n",
    "    print(f\"Shape: {src.shape}\")\n",
    "    print(f\"Dtype: {src.dtypes[0]}\")\n",
    "    image = src.read(1).astype(np.float32)\n",
    "    print(f\"Array size: {image.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540eb65",
   "metadata": {},
   "source": [
    "## Plotting bands of the image test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def showImage(raster,tindex,bandnbr,vmin=None,vmax=None): \n",
    "#     fig = plt.figure(figsize=(16,8)) \n",
    "#     ax1 = fig.add_subplot( 2 ) \n",
    "#     2 = fig.add_subplot( 22) \n",
    "#     ax1.imshow(raster,cmap='gray',vmin=vmin,vmax=vmax) \n",
    "#     ax1.set_title('Image Band {} {}'.format(bandnbr, \n",
    "#     tindex[bandnbr-1].date())) \n",
    "#     vmin=np.percentile(raster,2) if vmin==None else vmin \n",
    "#     vmax=np.percentile(raster,98) if vmax==None else vmax \n",
    "#     ax1.xaxis.set_label_text( \n",
    "#     'Linear stretch Min={} Max={}'.format(vmin,vmax)) \n",
    "#     h = 2.hist(raster.flatten(),bins=100,range=(0,8000)) \n",
    "#     2.xaxis.set_label_text('Amplitude (Uncalibrated DN Values)') \n",
    "#     2.set_title('Histogram Band {} {}'.format(bandnbr, \n",
    "#     tindex[bandnbr-1].date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4b25c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "testtiff = rasterio.open(f'{rawdir}\\S1A_IW_GRDH_1SDV_20260116T113541_20260116T113606_062792_07E02D_AC54.SAFE\\measurement\\s1a-iw-grd-vh-20260116t113541-20260116t113606-062792-07e02d-002.tiff')\n",
    "print(testtiff.count, testtiff.mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10275cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testtiff.width, testtiff.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69edd5d",
   "metadata": {},
   "source": [
    "## Continuing preprocessing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_file = checkpointdir / \"global_bounds.npy\"\n",
    "\n",
    "if bounds_file.exists():\n",
    "    bounds = np.load(bounds_file, allow_pickle=True).item()\n",
    "    global_vmin = bounds[\"vmin\"]\n",
    "    global_vmax = bounds[\"vmax\"]\n",
    "    print(f\"Loaded bounds: [{global_vmin:.2f}, {global_vmax:.2f}] dB\")\n",
    "else:\n",
    "    # Run the scanning loop to compute them\n",
    "    print(\"No saved bounds, computing...\")\n",
    "\n",
    "    all_vmins = []\n",
    "    all_vmaxs = []\n",
    "\n",
    "\n",
    "\n",
    "    for path in tqdm(sar_files, desc=\"Scanning\"):\n",
    "        print(f\"\\n{path}\")\n",
    "        \n",
    "        t0 = time.time()\n",
    "        with rasterio.open(path) as src:\n",
    "            image = src.read(1).astype(np.float32)\n",
    "        print(f\"  Read: {time.time() - t0:.1f}s\")\n",
    "        \n",
    "        t0 = time.time()\n",
    "        valid = image[image > 0]\n",
    "        del image\n",
    "        gc.collect()\n",
    "        print(f\"  Valid mask: {time.time() - t0:.1f}s\")\n",
    "        \n",
    "        if len(valid) == 0:\n",
    "            continue\n",
    "        \n",
    "        t0 = time.time()\n",
    "        if len(valid) > 100_000:\n",
    "            idx = np.random.randint(0, len(valid), 100_000)\n",
    "            valid = valid[idx]\n",
    "        print(f\"  Subsample: {time.time() - t0:.1f}s\")\n",
    "        \n",
    "        t0 = time.time()\n",
    "        image_db = 10 * np.log10(np.maximum(valid, 1e-10))\n",
    "        all_vmins.append(np.percentile(image_db, 1))\n",
    "        all_vmaxs.append(np.percentile(image_db, 99))\n",
    "        print(f\"  Percentiles: {time.time() - t0:.1f}s\")\n",
    "        \n",
    "        del valid, image_db\n",
    "        gc.collect()\n",
    "\n",
    "    global_vmin = np.median(all_vmins)\n",
    "    global_vmax = np.median(all_vmaxs)\n",
    "\n",
    "\n",
    "    print(f\"\\nGlobal bounds: [{global_vmin:.2f}, {global_vmax:.2f}] dB\")\n",
    "    np.save(checkpointdir / \"global_bounds.npy\", {\n",
    "        \"vmin\" : global_vmin,\n",
    "        \"vmax\" : global_vmax,\n",
    "        \"all_vmins\" : all_vmins,\n",
    "        \"all_vmaxs\" : all_vmaxs\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ce603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_valid = 0.9\n",
    "\n",
    "all_patches = []\n",
    "filestats = []\n",
    "\n",
    "print(\"Pass 2 Extracting Patches...\\n\")\n",
    "\n",
    "for path in tqdm(sar_files, desc = \"Processing\"):\n",
    "    filename = Path(path).stem\n",
    "\n",
    "    mem_gb = psutil.Process().memory_info().rss / 1e9\n",
    "    if mem_gb > 25:  # bail out before crash\n",
    "        print(f\"WARNING: Memory at {mem_gb:.1f} GB, stopping early\")\n",
    "        break\n",
    "\n",
    "    image = load_sar_image(path)\n",
    "    image_shape=image.shape\n",
    "\n",
    "    normalised, params = preprocess_sar_complete(\n",
    "        image, \n",
    "        vmin=global_vmin, \n",
    "        vmax=global_vmax\n",
    "    )\n",
    "    del image\n",
    "    gc.collect()\n",
    "    \n",
    "    # Extract patches\n",
    "    patches, positions = extract_patches(\n",
    "        normalised,\n",
    "        patch_size=patchsize,\n",
    "        stride=stride,\n",
    "        min_valid=min_valid\n",
    "    )\n",
    "    del normalised\n",
    "    gc.collect()\n",
    "    \n",
    "    n_patches = len(patches)\n",
    "    filestats.append({\n",
    "        'filename': filename,\n",
    "        'shape': image_shape,\n",
    "        'patches': n_patches\n",
    "    })\n",
    "    if n_patches > 0:\n",
    "        np.save(outputdir / f\"{filename}_patches.npy\", patches)\n",
    "\n",
    "    del patches, positions\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    tqdm.write(f\"  {filename[:40]}... : {n_patches} patches | Mem: {mem_gb:.1f} GB\")\n",
    "    \n",
    "\n",
    "print(f\"\\nProcessed {len(sar_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"patches\")\n",
    "\n",
    "# Check what's done\n",
    "existing = list(output_dir.glob(\"*.npy\"))\n",
    "print(f\"Files saved: {len(existing)}\")\n",
    "\n",
    "# Check disk space\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage(\"D:/\")\n",
    "print(f\"Free space: {free / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d2478-da26-4cdc-b7ed-c58e95f3803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = set(f.stem.replace(\"_patches\", \"\") for f in outputdir.glob(\"*.npy\"))\n",
    "print(f\"Already saved: {len(existing)} files\")\n",
    "\n",
    "# Find remaining files\n",
    "remaining = [p for p in sar_files if Path(p).stem not in existing]\n",
    "print(f\"Remaining: {len(remaining)} files\")\n",
    "\n",
    "# Process only remaining files\n",
    "for path in tqdm(remaining, desc=\"Processing\"):\n",
    "    filename = Path(path).stem\n",
    "    \n",
    "    mem_gb = psutil.Process().memory_info().rss / 1e9\n",
    "    if mem_gb > 25:\n",
    "        print(f\"WARNING: Memory at {mem_gb:.1f} GB, stopping early\")\n",
    "        break\n",
    "    \n",
    "    image = load_sar_image(path)\n",
    "    image_shape = image.shape\n",
    "    \n",
    "    normalised, params = preprocess_sar_complete(\n",
    "        image, vmin=global_vmin, vmax=global_vmax\n",
    "    )\n",
    "    del image\n",
    "    gc.collect()\n",
    "    \n",
    "    patches, positions = extract_patches(\n",
    "        normalised, patch_size=patchsize, stride=stride, min_valid=min_valid\n",
    "    )\n",
    "    del normalised\n",
    "    gc.collect()\n",
    "    \n",
    "    n_patches = len(patches)\n",
    "    filestats.append({\n",
    "        'filename': filename,\n",
    "        'shape': image_shape,\n",
    "        'patches': n_patches\n",
    "    })\n",
    "    \n",
    "    if n_patches > 0:\n",
    "        np.save(outputdir / f\"{filename}_patches.npy\", patches)\n",
    "    \n",
    "    del patches, positions\n",
    "    gc.collect()\n",
    "    \n",
    "    tqdm.write(f\"  {filename[:40]}... : {n_patches} patches | Mem: {mem_gb:.1f} GB\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56847338-b82c-48d5-bb16-6c8f75610c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tiff filenames (stems)\n",
    "tiff_stems = set(Path(p).stem for p in sar_files)\n",
    "\n",
    "# Get all saved patch filenames (remove \"_patches\" suffix)\n",
    "saved_stems = set(f.stem.replace(\"_patches\", \"\") for f in outputdir.glob(\"*.npy\"))\n",
    "\n",
    "# Find missing\n",
    "missing = tiff_stems - saved_stems\n",
    "\n",
    "print(f\"Total TIFFs: {len(tiff_stems)}\")\n",
    "print(f\"Saved patches: {len(saved_stems)}\")\n",
    "print(f\"Missing: {len(missing)}\")\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nMissing files:\")\n",
    "    for m in sorted(missing):\n",
    "        print(f\"  {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e3cc2-16fa-4d75-a300-81ee110440f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def get_npy_shape(filepath):\n",
    "    \"\"\"Read shape from .npy header without loading data\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        # Skip magic number and version\n",
    "        f.read(8)\n",
    "        # Read header length\n",
    "        header_len = struct.unpack('<H', f.read(2))[0]\n",
    "        # Read header\n",
    "        header = f.read(header_len).decode('latin1')\n",
    "        # Parse shape from header string\n",
    "        shape_start = header.find('(') + 1\n",
    "        shape_end = header.find(')')\n",
    "        shape_str = header[shape_start:shape_end]\n",
    "        shape = tuple(int(x.strip()) for x in shape_str.split(',') if x.strip())\n",
    "        return shape\n",
    "\n",
    "total_patches = 0\n",
    "for f in outputdir.glob(\"*_patches.npy\"):\n",
    "    shape = get_npy_shape(f)\n",
    "    total_patches += shape[0]\n",
    "    \n",
    "print(f\"Total patches: {total_patches:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e1639-d549-4ada-b298-925e88a7ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bytes = sum(f.stat().st_size for f in outputdir.glob(\"*_patches.npy\"))\n",
    "print(f\"Total size: {total_bytes / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1cf27-f2df-43aa-802d-c7fa982c47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_files = list(outputdir.glob(\"*patches.npy\"))\n",
    "\n",
    "sample = np.load(patch_files[0], mmap_mode=\"r\")\n",
    "patch_shape = sample.shape[1:]\n",
    "print(f\"Patch shape: {patch_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb62188-d7bd-46e6-b017-7b2f493cb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata only\n",
    "metadata = {\n",
    "    'vmin': global_vmin,\n",
    "    'vmax': global_vmax,\n",
    "    'patch_size': patchsize,\n",
    "    'stride': stride,\n",
    "    'min_valid': min_valid,\n",
    "    'num_patches': total_patches,\n",
    "}\n",
    "np.save(outputdir / 'metadata.npy', metadata, allow_pickle=True)\n",
    "\n",
    "# Save shuffle index\n",
    "shuffle_idx = np.random.permutation(total_patches)\n",
    "np.save(outputdir / \"shuffle_idx.npy\", shuffle_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e914035-ab64-469e-8376-62d7cd11c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the old combined file\n",
    "(outputdir / \"all_patches.npy\").unlink()\n",
    "\n",
    "# Re-run indexing (exclude all_patches.npy)\n",
    "patch_files = sorted(f for f in outputdir.glob(\"*_patches.npy\") if f.name != \"all_patches.npy\")\n",
    "\n",
    "file_index = []\n",
    "for f in tqdm(patch_files, desc=\"Indexing\"):\n",
    "    shape = get_npy_shape(f)\n",
    "    file_index.append((f, shape[0]))\n",
    "\n",
    "total_patches = sum(n for _, n in file_index)\n",
    "print(f\"Total patches: {total_patches:,}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "shuffle_idx = np.random.permutation(total_patches)\n",
    "np.save(outputdir / \"shuffle_idx.npy\", shuffle_idx)\n",
    "\n",
    "metadata = {\n",
    "    'vmin': global_vmin,\n",
    "    'vmax': global_vmax,\n",
    "    'patch_size': patchsize,\n",
    "    'stride': stride,\n",
    "    'min_valid': min_valid,\n",
    "    'num_patches': total_patches,\n",
    "    'file_index': [(str(f), n) for f, n in file_index]\n",
    "}\n",
    "np.save(outputdir / 'metadata.npy', metadata, allow_pickle=True)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b2db2-211d-4ffb-ba77-740495ee878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all patch files and their sizes\n",
    "for f in sorted(outputdir.glob(\"*_patches.npy\")):\n",
    "    shape = get_npy_shape(f)\n",
    "    print(f\"{f.name}: {shape[0]:,} patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e9605-297a-4137-84cb-66c97b628e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Verify\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load metadata\n",
    "metadata = np.load(outputdir / 'metadata.npy', allow_pickle=True).item()\n",
    "shuffle_idx = np.load(outputdir / \"shuffle_idx.npy\")\n",
    "\n",
    "print(f\"Total patches: {metadata['num_patches']:,}\")\n",
    "print(f\"Global bounds: [{metadata['vmin']:.2f}, {metadata['vmax']:.2f}] dB\")\n",
    "print(f\"Patch size: {metadata['patch_size']}\")\n",
    "print(f\"Files: {len(metadata['file_index'])}\")\n",
    "\n",
    "# Load a few random patches for visualization\n",
    "def load_random_patches(n=12):\n",
    "    \"\"\"Load n random patches from the dataset\"\"\"\n",
    "    patches = []\n",
    "    \n",
    "    # Build cumsum for file lookup\n",
    "    cumsum = [0]\n",
    "    for _, count in metadata['file_index']:\n",
    "        cumsum.append(cumsum[-1] + count)\n",
    "    \n",
    "    random_indices = np.random.choice(len(shuffle_idx), n, replace=False)\n",
    "    \n",
    "    # Group by file to minimize loads\n",
    "    file_requests = {}\n",
    "    for idx in random_indices:\n",
    "        real_idx = shuffle_idx[idx]\n",
    "        for i, (start, end) in enumerate(zip(cumsum[:-1], cumsum[1:])):\n",
    "            if start <= real_idx < end:\n",
    "                local_idx = real_idx - start\n",
    "                if i not in file_requests:\n",
    "                    file_requests[i] = []\n",
    "                file_requests[i].append(local_idx)\n",
    "                break\n",
    "    \n",
    "    # Load from each file\n",
    "    for file_idx, local_indices in file_requests.items():\n",
    "        fpath, _ = metadata['file_index'][file_idx]\n",
    "        data = np.load(fpath)  # Full load, no mmap\n",
    "        for local_idx in local_indices:\n",
    "            patches.append(data[local_idx])\n",
    "        del data\n",
    "        gc.collect()\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Show random samples\n",
    "samples = load_random_patches(12)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "for ax, patch in zip(axes.flatten(), samples):\n",
    "    ax.imshow(patch, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(f\"Random samples from {metadata['num_patches']:,} patches\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b149d2f5-3ba5-477d-bd58-3ff57e0d92d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
