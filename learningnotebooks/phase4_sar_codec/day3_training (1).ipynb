{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Training Pipeline & Loss Functions\n",
    "\n",
    "**Goals:**\n",
    "- Test and verify your loss function implementations\n",
    "- Audit your training loop for best practices\n",
    "- Implement training diagnostics\n",
    "\n",
    "**Time:** 6 hours\n",
    "\n",
    "**Approach:** Instructions only. Write all code yourself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import PyTorch, torch.nn, torch.nn.functional (as F), numpy, and matplotlib. Also import your loss modules and trainer from `src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Theory Questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.1: SSIM Implementation Details\n",
    "\n",
    "The SSIM formula includes stability constants C1 and C2:\n",
    "\n",
    "```\n",
    "SSIM = [(2μ_xμ_y + C1)(2σ_xy + C2)] / [(μ_x² + μ_y² + C1)(σ_x² + σ_y² + C2)]\n",
    "```\n",
    "\n",
    "**a)** What numerical problem do C1 and C2 prevent? What would happen if you set them to zero?\n",
    "\n",
    "**b)** SSIM uses a Gaussian-weighted window for computing local statistics. Why Gaussian instead of a uniform (box) filter?\n",
    "\n",
    "**c)** What would happen if you set window_size=1? What would SSIM measure then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer:\n",
    "\n",
    "**a)**\n",
    "\n",
    "**b)**\n",
    "\n",
    "**c)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2: Loss Balancing\n",
    "\n",
    "Your combined loss is: L = α × MSE + β × SSIM_loss\n",
    "\n",
    "Suppose during training you observe:\n",
    "- MSE ≈ 0.01\n",
    "- SSIM_loss (which is 1 - SSIM) ≈ 0.15\n",
    "\n",
    "You're using α = 1.0 and β = 0.1.\n",
    "\n",
    "**a)** What is the total loss?\n",
    "\n",
    "**b)** What fraction of the total loss comes from MSE vs SSIM? Which term dominates gradient updates?\n",
    "\n",
    "**c)** If you wanted SSIM to have equal influence as MSE, how would you adjust the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer:\n",
    "\n",
    "**a)**\n",
    "\n",
    "**b)**\n",
    "\n",
    "**c)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.3: Optimizer Choice\n",
    "\n",
    "**a)** Why is Adam generally preferred over vanilla SGD for training autoencoders?\n",
    "\n",
    "**b)** Adam has parameters (β1, β2). The default β1 is 0.9. What does this parameter control? (Hint: it relates to momentum of the first moment estimate.)\n",
    "\n",
    "**c)** When would you use AdamW instead of Adam? What's the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer:\n",
    "\n",
    "**a)**\n",
    "\n",
    "**b)**\n",
    "\n",
    "**c)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.4: Training Dynamics\n",
    "\n",
    "You're training your autoencoder and observe:\n",
    "- Training loss decreases steadily over 50 epochs\n",
    "- Validation loss decreases until epoch 30, then starts increasing\n",
    "\n",
    "**a)** What phenomenon is occurring after epoch 30?\n",
    "\n",
    "**b)** What strategies could you employ to address this? List at least 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer:\n",
    "\n",
    "**a)**\n",
    "\n",
    "**b)**\n",
    "1. \n",
    "2. \n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.5: Gradient Clipping\n",
    "\n",
    "**a)** Why might you clip gradients to max_norm=1.0 during training?\n",
    "\n",
    "**b)** What training symptoms might indicate you need gradient clipping? (Think about loss curves and what happens when gradients explode.)\n",
    "\n",
    "**c)** What's the potential downside of clipping gradients too aggressively (e.g., max_norm=0.01)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer:\n",
    "\n",
    "**a)**\n",
    "\n",
    "**b)**\n",
    "\n",
    "**c)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Loss Function Testing (1.5 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Create Test Data\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Create three test tensors:\n",
    "1. `img1`: Random tensor of shape (4, 1, 64, 64) with values in [0, 1]\n",
    "2. `img2`: A noisy version of img1 (add Gaussian noise with std=0.1, then clamp to [0, 1])\n",
    "3. `img1_copy`: An exact clone of img1\n",
    "\n",
    "These will be used to test all loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: Test MSE Loss\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Compute MSE between img1 and img1_copy using PyTorch's F.mse_loss. This should be essentially 0.\n",
    "\n",
    "2. Compute MSE between img1 and img2. This should be positive (around 0.01 for std=0.1 noise).\n",
    "\n",
    "3. Now test YOUR MSE loss implementation from `src/losses/mse.py`. Instantiate your loss class and compute the same two comparisons.\n",
    "\n",
    "4. Verify that your implementation matches PyTorch's F.mse_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MSE loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3: Implement Reference SSIM\n",
    "\n",
    "Before testing your SSIM, implement a reference version to understand the algorithm.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Write a function `ssim_reference(img1, img2, window_size=11)` that:\n",
    "\n",
    "1. Creates a 1D Gaussian kernel with the given window_size and σ=1.5:\n",
    "   ```\n",
    "   coords = torch.arange(window_size) - window_size // 2\n",
    "   g = torch.exp(-coords² / (2 × 1.5²))\n",
    "   g = g / g.sum()\n",
    "   ```\n",
    "\n",
    "2. Creates a 2D window by outer product: `window = g.unsqueeze(0) @ g.unsqueeze(1)`\n",
    "\n",
    "3. Reshapes window to (1, 1, window_size, window_size) for use with F.conv2d.\n",
    "\n",
    "4. Computes local means μ1, μ2 by convolving with the window.\n",
    "\n",
    "5. Computes local variances σ1², σ2² and covariance σ12 using:\n",
    "   - σ1² = conv(img1²) - μ1²\n",
    "   - σ12 = conv(img1 × img2) - μ1 × μ2\n",
    "\n",
    "6. Applies SSIM formula with C1=(0.01)² and C2=(0.03)².\n",
    "\n",
    "7. Returns the mean SSIM over all pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement reference SSIM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4: Test SSIM Loss\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Use your reference SSIM to compute:\n",
    "   - SSIM(img1, img1_copy) - should be ~1.0\n",
    "   - SSIM(img1, img2) - should be less than 1.0\n",
    "\n",
    "2. Test YOUR SSIM loss implementation from `src/losses/ssim.py`.\n",
    "\n",
    "3. Note: Your loss probably returns (1 - SSIM) since we minimize loss. Verify this convention.\n",
    "\n",
    "4. If possible, compare against scikit-image's structural_similarity function to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SSIM loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5: Test Combined Loss\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Import or instantiate your combined loss from `src/losses/combined.py`.\n",
    "\n",
    "2. Compute the combined loss between img1 and img2.\n",
    "\n",
    "3. Manually compute what the loss should be by combining MSE and SSIM with the weights your implementation uses.\n",
    "\n",
    "4. Verify they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test combined loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.6: Test Gradient Flow Through Loss\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Create img1 with `requires_grad=True`.\n",
    "\n",
    "2. Compute your combined loss between img1 and img2 (which doesn't need gradients).\n",
    "\n",
    "3. Call `loss.backward()`.\n",
    "\n",
    "4. Verify that `img1.grad` is not None.\n",
    "\n",
    "5. Check that the gradient has the same shape as img1.\n",
    "\n",
    "6. Check that the gradient contains no NaN or Inf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Trainer Audit (1.5 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.7: Trainer Checklist\n",
    "\n",
    "Open your `src/training/trainer.py` file and carefully read through it.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Check each item and note whether it's present in your implementation. If something is missing, you'll implement it later.\n",
    "\n",
    "### Training Loop Essentials\n",
    "- [ ] `model.train()` called before training epoch\n",
    "- [ ] `optimizer.zero_grad()` called before each forward pass\n",
    "- [ ] `loss.backward()` called after computing loss\n",
    "- [ ] `optimizer.step()` called after backward\n",
    "- [ ] Gradient clipping with `torch.nn.utils.clip_grad_norm_`\n",
    "\n",
    "### Validation\n",
    "- [ ] `model.eval()` called before validation\n",
    "- [ ] `torch.no_grad()` context used during validation\n",
    "- [ ] No `optimizer.step()` during validation\n",
    "\n",
    "### Logging\n",
    "- [ ] Training loss recorded per epoch\n",
    "- [ ] Validation loss recorded per epoch\n",
    "- [ ] Learning rate tracked\n",
    "- [ ] Progress indication (print statements or progress bar)\n",
    "\n",
    "### Checkpointing\n",
    "- [ ] Best model saved when validation loss improves\n",
    "- [ ] Checkpoint includes model state dict\n",
    "- [ ] Checkpoint includes optimizer state dict\n",
    "- [ ] Checkpoint includes epoch number\n",
    "- [ ] Checkpoint includes best loss value\n",
    "\n",
    "### Scheduling & Stopping\n",
    "- [ ] Learning rate scheduler implemented\n",
    "- [ ] Scheduler.step() called at appropriate time\n",
    "- [ ] Early stopping implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Audit Results:\n",
    "\n",
    "**Present:**\n",
    "\n",
    "**Missing:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.8: Implement Training Diagnostics\n",
    "\n",
    "Before running a full training, it's wise to run diagnostics to catch problems early.\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Write a function `diagnose_training(model, dataloader, loss_fn, device)` that:\n",
    "\n",
    "1. **Input Statistics**: Get one batch, print shape, min, max, mean.\n",
    "\n",
    "2. **Output Statistics**: Pass batch through model, print output range and mean.\n",
    "\n",
    "3. **Gradient Statistics**: \n",
    "   - Compute loss on one batch\n",
    "   - Call backward()\n",
    "   - For each parameter, compute gradient norm\n",
    "   - Print total gradient norm (sqrt of sum of squared norms)\n",
    "   - Flag any layers with zero gradient\n",
    "\n",
    "4. **Overfit Test**:\n",
    "   - Try to overfit a single batch for 100 iterations\n",
    "   - Use a fresh optimizer with lr=1e-3\n",
    "   - Record loss at iteration 0 and 100\n",
    "   - If loss doesn't decrease significantly (>50%), something is wrong\n",
    "\n",
    "Return a dict with all the diagnostic info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement diagnose_training function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.9: Test Diagnostics\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Create a simple DataLoader with synthetic data (or use your actual data if available):\n",
    "   - Create a TensorDataset with random tensors of shape (N, 1, 256, 256)\n",
    "   - Wrap in DataLoader with batch_size=4\n",
    "\n",
    "2. Instantiate your model and loss function.\n",
    "\n",
    "3. Run your diagnostic function and interpret the results.\n",
    "\n",
    "4. Did the overfit test pass? If not, investigate why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test diagnostics on your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Implement Missing Training Features (1.5 hours)\n",
    "\n",
    "Based on your audit, implement any missing features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.10: Implement Gradient Clipping\n",
    "\n",
    "If your trainer doesn't have gradient clipping:\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Show where in the training loop you would add:\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "Write a small test demonstrating that it works:\n",
    "1. Create a tensor with requires_grad=True\n",
    "2. Create a \"loss\" that would produce large gradients (e.g., `loss = 1000 * tensor.sum()`)\n",
    "3. Call backward\n",
    "4. Print gradient norm before clipping\n",
    "5. Apply clipping\n",
    "6. Print gradient norm after clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient clipping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.11: Implement Learning Rate Scheduler\n",
    "\n",
    "If your trainer doesn't have LR scheduling:\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Choose an appropriate scheduler. For autoencoders, `ReduceLROnPlateau` works well:\n",
    "   - Reduces LR when validation loss plateaus\n",
    "   - Patience of 5-10 epochs is typical\n",
    "\n",
    "2. Show how to create the scheduler:\n",
    "   ```python\n",
    "   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "       optimizer, mode='min', factor=0.5, patience=10\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. Show where in the training loop to call `scheduler.step(val_loss)`.\n",
    "\n",
    "4. Write code to track and print the current learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate LR scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.12: Implement Early Stopping\n",
    "\n",
    "If your trainer doesn't have early stopping:\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Implement an EarlyStopping class that:\n",
    "\n",
    "1. Tracks the best validation loss seen so far.\n",
    "\n",
    "2. Counts epochs since last improvement.\n",
    "\n",
    "3. Has a `patience` parameter (e.g., 20 epochs).\n",
    "\n",
    "4. Has a `__call__(val_loss)` method that:\n",
    "   - Updates best loss if this is an improvement\n",
    "   - Resets counter on improvement\n",
    "   - Increments counter if no improvement\n",
    "   - Returns True if training should stop (counter >= patience)\n",
    "\n",
    "5. Optionally: saves the best model when improvement happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement EarlyStopping class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.13: Implement Checkpointing\n",
    "\n",
    "If your trainer doesn't save checkpoints properly:\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Write functions for saving and loading checkpoints:\n",
    "\n",
    "```python\n",
    "def save_checkpoint(path, model, optimizer, epoch, val_loss, config=None):\n",
    "    # Save dict with all necessary info\n",
    "    pass\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None):\n",
    "    # Load and restore state\n",
    "    # Return epoch and val_loss so training can resume\n",
    "    pass\n",
    "```\n",
    "\n",
    "Test by saving a checkpoint, creating fresh model/optimizer instances, and loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement checkpointing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test save and load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Short Training Test\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.14: Run a Short Training\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "Run a short training (5-10 epochs) to verify everything works together:\n",
    "\n",
    "1. Load or create your training and validation data.\n",
    "\n",
    "2. Instantiate model, optimizer, loss function, scheduler.\n",
    "\n",
    "3. Run training for 5-10 epochs.\n",
    "\n",
    "4. Plot training and validation loss curves.\n",
    "\n",
    "5. Verify:\n",
    "   - Losses decrease over time\n",
    "   - No NaN or Inf losses\n",
    "   - Validation loss tracks training loss reasonably\n",
    "\n",
    "If you encounter issues, debug them before Day 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short training test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 3 Checklist\n",
    "\n",
    "- [ ] Answered all theory questions (Q3.1 - Q3.5)\n",
    "- [ ] MSE loss tested and verified\n",
    "- [ ] SSIM reference implementation written\n",
    "- [ ] SSIM loss tested and verified\n",
    "- [ ] Combined loss tested\n",
    "- [ ] Gradient flow through loss verified\n",
    "- [ ] Trainer audited against checklist\n",
    "- [ ] Training diagnostics implemented and run\n",
    "- [ ] Gradient clipping implemented/verified\n",
    "- [ ] Learning rate scheduler implemented/verified\n",
    "- [ ] Early stopping implemented/verified\n",
    "- [ ] Checkpointing implemented/verified\n",
    "- [ ] Short training test completed successfully\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration Summary\n",
    "\n",
    "*Fill in your current configuration:*\n",
    "\n",
    "**Loss function:** MSE + SSIM (weights: α=__, β=__)\n",
    "\n",
    "**Optimizer:** Adam (lr=__, weight_decay=__)\n",
    "\n",
    "**Scheduler:** ReduceLROnPlateau (patience=__, factor=__)\n",
    "\n",
    "**Gradient clipping:** max_norm=__\n",
    "\n",
    "**Early stopping:** patience=__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Issues\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
