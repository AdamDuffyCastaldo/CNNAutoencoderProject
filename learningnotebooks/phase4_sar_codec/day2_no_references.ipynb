{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Model Implementation (No References)\n",
    "\n",
    "**Time:** 4-5 hours\n",
    "\n",
    "This version has NO reference implementations. You'll need to figure it out from:\n",
    "- The docstrings and hints\n",
    "- The test cells (they tell you expected behavior)\n",
    "- Your Day 1 knowledge\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "ENCODER (compresses):\n",
    "256×256×1 → 128×128×64 → 64×64×128 → 32×32×256 → 16×16×64\n",
    "\n",
    "DECODER (reconstructs):\n",
    "16×16×64 → 32×32×256 → 64×64×128 → 128×128×64 → 256×256×1\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Encoder (45 minutes)\n",
    "\n",
    "## 1.1 ConvBlock\n",
    "\n",
    "Build a block that does: **Conv2d → BatchNorm → LeakyReLU**\n",
    "\n",
    "Hints:\n",
    "- `nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=...)`\n",
    "- `nn.BatchNorm2d(num_features)`\n",
    "- `nn.LeakyReLU(negative_slope)`\n",
    "- If using BatchNorm, set `bias=False` in Conv2d (BN has its own bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv2d → BatchNorm → LeakyReLU\n",
    "    \n",
    "    Default params create a block that halves spatial dimensions:\n",
    "    - kernel_size=5, stride=2, padding=2\n",
    "    - LeakyReLU slope = 0.2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
    "                 stride=2, padding=2, use_bn=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Create self.conv, self.bn, self.activation\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias = not use_bn)\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Apply conv → bn → activation\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ConvBlock: torch.Size([2, 1, 256, 256]) → torch.Size([2, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test_conv_block():\n",
    "    block = ConvBlock(1, 64)\n",
    "    x = torch.randn(2, 1, 256, 256)\n",
    "    y = block(x)\n",
    "    assert y.shape == (2, 64, 128, 128), f\"Expected (2,64,128,128), got {y.shape}\"\n",
    "    print(f\"✓ ConvBlock: {x.shape} → {y.shape}\")\n",
    "\n",
    "test_conv_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 SAREncoder\n",
    "\n",
    "Stack 4 ConvBlocks. The last one should be a plain Conv2d (no BN, no activation).\n",
    "\n",
    "```\n",
    "Layer 1: 1 → 64 channels\n",
    "Layer 2: 64 → 128 channels  \n",
    "Layer 3: 128 → 256 channels\n",
    "Layer 4: 256 → latent_channels (just Conv2d, no activation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAREncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: 256×256×1 → 16×16×latent_channels\n",
    "    \n",
    "    4 layers, each halves spatial dimensions.\n",
    "    Last layer has no activation (latent should be unbounded).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, latent_channels=64, base_channels=64, use_bn = True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.latent_channels = latent_channels\n",
    "        # TODO: Create 4 layers\n",
    "        # Hint: Use ConvBlock for layers 1-3, plain nn.Conv2d for layer 4\n",
    "        channels = [in_channels, base_channels, base_channels*2, base_channels*4]\n",
    "\n",
    "        self.layer1 = ConvBlock(\n",
    "            channels[0], channels[1],\n",
    "            kernel_size=5, stride=2, padding=2, use_bn=use_bn\n",
    "        )\n",
    "\n",
    "        self.layer2 = ConvBlock(\n",
    "            channels[1], channels[2],\n",
    "            kernel_size=5, stride=2, padding=2, use_bn=use_bn\n",
    "        )\n",
    "\n",
    "        self.layer3 = ConvBlock(\n",
    "            channels[2], channels[3],\n",
    "            kernel_size=5, stride=2, padding=2, use_bn=use_bn\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Conv2d(\n",
    "            channels[3], latent_channels,\n",
    "            kernel_size=5, stride=2, padding=2\n",
    "        )\n",
    "        \n",
    "        self._initialise_weights()\n",
    "\n",
    "    def _initialise_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"leaky_relu\")\n",
    "            \n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "    \n",
    "    def calcreceptivefield(self):\n",
    "        rf = 5  \n",
    "        stride_product = 2\n",
    "        for _ in range(3): \n",
    "            rf += (5 - 1) * stride_product\n",
    "            stride_product *= 2\n",
    "        return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Architecture:\n",
      "  Input shape:  (2, 1, 256, 256)\n",
      "  Output shape: (2, 64, 16, 16)\n",
      "  Receptive field: 61\n",
      "✓ Encoder: torch.Size([2, 1, 256, 256]) → torch.Size([2, 64, 16, 16])\n",
      "\n",
      "Gradient norms (should be non-zero, similar magnitude):\n",
      "  layer1.conv.weight: 0.371410\n",
      "  layer1.bn.weight: 0.011589\n",
      "  layer1.bn.bias: 0.008325\n",
      "  layer2.conv.weight: 0.446748\n",
      "✓ Gradients flow\n",
      "✓ Parameters: 1,436,160\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test_encoder():\n",
    "    encoder = SAREncoder(latent_channels=64)\n",
    "    x = torch.randn(2, 1, 256, 256)\n",
    "    z = encoder(x)\n",
    "    print(f\"\\nArchitecture:\")\n",
    "    print(f\"  Input shape:  {tuple(x.shape)}\")\n",
    "    print(f\"  Output shape: {tuple(z.shape)}\")\n",
    "    print(f\"  Receptive field: {encoder.calcreceptivefield()}\")\n",
    "\n",
    "    assert z.shape == (2, 64, 16, 16), f\"Expected (2,64,16,16), got {z.shape}\"\n",
    "    print(f\"✓ Encoder: {x.shape} → {z.shape}\")\n",
    "    \n",
    "    # Check gradients\n",
    "    z.mean().backward()\n",
    "    grad_norms = []\n",
    "    for name, param in encoder.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append((name, param.grad.norm().item()))\n",
    "    print(f\"\\nGradient norms (should be non-zero, similar magnitude):\")\n",
    "    for name, norm in grad_norms[:4]:  # First 4\n",
    "        print(f\"  {name}: {norm:.6f}\")\n",
    "    print(\"✓ Gradients flow\")\n",
    "\n",
    "\n",
    "    \n",
    "    params = sum(p.numel() for p in encoder.parameters())\n",
    "    print(f\"✓ Parameters: {params:,}\")\n",
    "\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Decoder (45 minutes)\n",
    "\n",
    "## 2.1 DeconvBlock\n",
    "\n",
    "Like ConvBlock but uses `nn.ConvTranspose2d` to upsample.\n",
    "\n",
    "Key difference: need `output_padding=1` to get exact 2× upsampling.\n",
    "\n",
    "Use ReLU (not LeakyReLU) for decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvTranspose2d → BatchNorm → ReLU\n",
    "    \n",
    "    Doubles spatial dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5,\n",
    "                 stride=2, padding=2, output_padding=1, use_bn=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Create self.deconv, self.bn, self.activation\n",
    "        self.deconv = nn.ConvTranspose2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            output_padding=output_padding,\n",
    "            bias=not use_bn\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DeconvBlock: torch.Size([2, 64, 16, 16]) → torch.Size([2, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test_deconv_block():\n",
    "    block = DeconvBlock(64, 32)\n",
    "    x = torch.randn(2, 64, 16, 16)\n",
    "    y = block(x)\n",
    "    assert y.shape == (2, 32, 32, 32), f\"Expected (2,32,32,32), got {y.shape}\"\n",
    "    print(f\"✓ DeconvBlock: {x.shape} → {y.shape}\")\n",
    "\n",
    "test_deconv_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SARDecoder\n",
    "\n",
    "Mirror of encoder. Last layer: just ConvTranspose2d + sigmoid for [0,1] output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder: 16×16×latent → 256×256×1\n",
    "    \n",
    "    Mirrors encoder. Output has sigmoid for [0,1] range.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, out_channels=1, latent_channels=64, base_channels=64, use_bn = True):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.latent_channels = latent_channels\n",
    "        # TODO: Create 4 layers (reverse of encoder channel progression)\n",
    "        # Layer 1: latent → 256\n",
    "        # Layer 2: 256 → 128\n",
    "        # Layer 3: 128 → 64\n",
    "        # Layer 4: 64 → 1 (no BN, use sigmoid)\n",
    "        channels = [latent_channels, base_channels*4, base_channels*2, \n",
    "                    base_channels, out_channels]\n",
    "            \n",
    "        self.layer1 = DeconvBlock(\n",
    "            channels[0], channels[1],\n",
    "            kernel_size=5, stride=2, padding=2, output_padding=1, use_bn=use_bn\n",
    "        )    \n",
    "\n",
    "        self.layer2 = DeconvBlock(\n",
    "            channels[1], channels[2],\n",
    "            kernel_size=5, stride=2, padding=2, output_padding=1, use_bn=use_bn\n",
    "        )\n",
    "\n",
    "        self.layer3 = DeconvBlock(\n",
    "            channels[2], channels[3],\n",
    "            kernel_size=5, stride=2, padding=2, output_padding=1, use_bn=use_bn\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.ConvTranspose2d(\n",
    "            channels[3], channels[4],\n",
    "            kernel_size=5, stride=2, padding=2, output_padding=1\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.layer1(z)  # (B, 256, 32, 32)\n",
    "        x = self.layer2(x)  # (B, 128, 64, 64)\n",
    "        x = self.layer3(x)  # (B, 64, 128, 128)\n",
    "        x = self.layer4(x)  # (B, 1, 256, 256)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Decoder: torch.Size([2, 64, 16, 16]) → torch.Size([2, 1, 256, 256])\n",
      "✓ Output range: [0.060, 0.953]\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test_decoder():\n",
    "    decoder = SARDecoder(latent_channels=64)\n",
    "    z = torch.randn(2, 64, 16, 16)\n",
    "    x_hat = decoder(z)\n",
    "    \n",
    "    assert x_hat.shape == (2, 1, 256, 256), f\"Expected (2,1,256,256), got {x_hat.shape}\"\n",
    "    print(f\"✓ Decoder: {z.shape} → {x_hat.shape}\")\n",
    "    \n",
    "    assert x_hat.min() >= 0 and x_hat.max() <= 1, \"Output should be [0,1]\"\n",
    "    print(f\"✓ Output range: [{x_hat.min():.3f}, {x_hat.max():.3f}]\")\n",
    "\n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Autoencoder (30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete autoencoder.\n",
    "    \n",
    "    forward() returns (x_hat, z)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_channels=64, base_channels=64, use_bn = True):\n",
    "        super().__init__()\n",
    "        self.latent_channels = latent_channels\n",
    "        \n",
    "        # TODO: Create encoder and decoder\n",
    "        \n",
    "        self.latent_channels = latent_channels\n",
    "        self.base_channels = base_channels\n",
    "\n",
    "        self.encoder = SAREncoder(\n",
    "            in_channels=1,\n",
    "            latent_channels=latent_channels,\n",
    "            base_channels=base_channels,\n",
    "            use_bn=use_bn\n",
    "        )\n",
    "        \n",
    "        self.decoder = SARDecoder(\n",
    "            out_channels=1,\n",
    "            latent_channels=latent_channels,\n",
    "            base_channels=base_channels,\n",
    "            use_bn=use_bn\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns (reconstruction, latent)\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def get_compression_ratio(self):\n",
    "        \"\"\"256×256×1 input, 16×16×latent_channels latent\"\"\"\n",
    "        return (256 * 256 * 1) / (16 * 16 * self.latent_channels)\n",
    "    \n",
    "    def get_latent_size(self, input_size: int = 256):\n",
    "        latent_spatial = input_size // 16\n",
    "        return (self.latent_channels, latent_spatial, latent_spatial)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count parameters in encoder and decoder.\"\"\"\n",
    "        encoder_params = sum(p.numel() for p in self.encoder.parameters())\n",
    "        decoder_params = sum(p.numel() for p in self.decoder.parameters())\n",
    "        return {\n",
    "            'encoder': encoder_params,\n",
    "            'decoder': decoder_params,\n",
    "            'total': encoder_params + decoder_params\n",
    "        }\n",
    "    \n",
    "    def analyze_latent(self, z: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Analyze latent representation statistics.\n",
    "        \n",
    "        Useful for monitoring training and diagnosing issues.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            z_np = z.cpu().numpy()\n",
    "            \n",
    "            return {\n",
    "                'mean': float(np.mean(z_np)),\n",
    "                'std': float(np.std(z_np)),\n",
    "                'min': float(np.min(z_np)),\n",
    "                'max': float(np.max(z_np)),\n",
    "                'sparsity': float(np.mean(np.abs(z_np) < 0.01)),  # Near-zero fraction\n",
    "                'channel_stds': [float(np.std(z_np[:, c, :, :])) \n",
    "                                for c in range(min(z_np.shape[1], 8))],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Standard (4x compression) ---\n",
      "  Compression ratio: 4.0x\n",
      "  Parameters: 2,872,257\n",
      "    Encoder: 1,436,160\n",
      "    Decoder: 1,436,097\n",
      "  Latent shape: (2, 64, 16, 16)\n",
      "  Latent mean: 0.1596\n",
      "  Latent std: 1.9508\n",
      "  Output range: [0.098, 0.918]\n",
      "\n",
      "--- High compression (8x) ---\n",
      "  Compression ratio: 8.0x\n",
      "  Parameters: 2,462,625\n",
      "    Encoder: 1,231,328\n",
      "    Decoder: 1,231,297\n",
      "  Latent shape: (2, 32, 16, 16)\n",
      "  Latent mean: -0.2214\n",
      "  Latent std: 2.8458\n",
      "  Output range: [0.153, 0.964]\n",
      "\n",
      "--- Low compression (2x) ---\n",
      "  Compression ratio: 2.0x\n",
      "  Parameters: 3,691,521\n",
      "    Encoder: 1,845,824\n",
      "    Decoder: 1,845,697\n",
      "  Latent shape: (2, 128, 16, 16)\n",
      "  Latent mean: -0.0645\n",
      "  Latent std: 1.3746\n",
      "  Output range: [0.096, 0.929]\n",
      "\n",
      "--- Gradient Flow Test ---\n",
      "\n",
      "✓ All autoencoder tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test_autoencoder():\n",
    "    configs = [\n",
    "        {'latent_channels': 64, 'name': 'Standard (4x compression)'},\n",
    "        {'latent_channels': 32, 'name': 'High compression (8x)'},\n",
    "        {'latent_channels': 128, 'name': 'Low compression (2x)'},\n",
    "    ]\n",
    "    for config in configs:\n",
    "        print(f\"\\n--- {config['name']} ---\")\n",
    "        \n",
    "        model = SARAutoencoder(latent_channels=config['latent_channels'])\n",
    "        x = torch.randn(2, 1, 256, 256)\n",
    "        x = torch.sigmoid(x)\n",
    "    \n",
    "        x_hat, z = model(x)\n",
    "    \n",
    "        assert x_hat.shape == x.shape, f\"Shape mismatch: {x_hat.shape} vs {x.shape}\"\n",
    "        expected_latent = model.get_latent_size(256)\n",
    "        actual_latent = tuple(z.shape[1:])\n",
    "        assert actual_latent == expected_latent, f\"Latent shape mismatch\"\n",
    "\n",
    "        params = model.count_parameters()\n",
    "        compression = model.get_compression_ratio()\n",
    "        latent_stats = model.analyze_latent(z)\n",
    "\n",
    "        print(f\"  Compression ratio: {compression:.1f}x\")\n",
    "        print(f\"  Parameters: {params['total']:,}\")\n",
    "        print(f\"    Encoder: {params['encoder']:,}\")\n",
    "        print(f\"    Decoder: {params['decoder']:,}\")\n",
    "        print(f\"  Latent shape: {tuple(z.shape)}\")\n",
    "        print(f\"  Latent mean: {latent_stats['mean']:.4f}\")\n",
    "        print(f\"  Latent std: {latent_stats['std']:.4f}\")\n",
    "        print(f\"  Output range: [{x_hat.min():.3f}, {x_hat.max():.3f}]\")\n",
    "\n",
    "    print(\"\\n--- Gradient Flow Test ---\")\n",
    "    model = SARAutoencoder(latent_channels=64)\n",
    "    x = torch.randn(2, 1, 256, 256, requires_grad=True)\n",
    "    x_hat, z = model(x)\n",
    "    loss = F.mse_loss(x_hat, torch.zeros_like(x_hat))\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            if grad_norm == 0:\n",
    "                print(f\"  WARNING: Zero gradient in {name}\")\n",
    "            elif grad_norm > 100:\n",
    "                print(f\"  WARNING: Large gradient in {name}: {grad_norm:.2f}\")\n",
    "    \n",
    "    print(\"\\n✓ All autoencoder tests passed!\")\n",
    "    \n",
    "    # print(f\"✓ Autoencoder: {x.shape} → {z.shape} → {x_hat.shape}\")\n",
    "    # print(f\"✓ Compression: {model.get_compression_ratio():.1f}x\")\n",
    "    # print(f\"✓ Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "test_autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Loss Function (30 minutes)\n",
    "\n",
    "## 4.1 SSIM Loss\n",
    "\n",
    "Steps:\n",
    "1. Create Gaussian window (for local averaging)\n",
    "2. Compute local means: μx, μy\n",
    "3. Compute local variances: σx², σy² = E[X²] - E[X]²\n",
    "4. Compute covariance: σxy = E[XY] - E[X]E[Y]\n",
    "5. Apply SSIM formula\n",
    "6. Return 1 - mean(SSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSIMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    SSIM Loss: returns 1 - SSIM\n",
    "    \n",
    "    Lower is better (0 = identical images).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=11, sigma=1.5, data_range = 1.0, channel = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.sigma = sigma\n",
    "        self.data_range = data_range\n",
    "        self.channel = channel\n",
    "        self.C1 = (0.01 * data_range) ** 2\n",
    "        self.C2 = (0.03 * data_range) ** 2\n",
    "        \n",
    "        # TODO: Create Gaussian window and register as buffer\n",
    "        # 1. coords = torch.arange(window_size) - window_size // 2\n",
    "        # 2. g = exp(-coords² / (2σ²)), normalize so sum = 1\n",
    "        # 3. window = outer product: g.unsqueeze(1) @ g.unsqueeze(0)\n",
    "        # 4. Reshape to (1, 1, window_size, window_size)\n",
    "        # 5. self.register_buffer('window', window)\n",
    "        \n",
    "        self.register_buffer('window', self._create_window(window_size, sigma, channel))\n",
    "\n",
    "    def _create_window(self, window_size, sigma, channel):\n",
    "        coords = torch.arange(window_size, dtype=torch.float32)\n",
    "        coords -= window_size // 2\n",
    "        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
    "        g /= g.sum()\n",
    "\n",
    "        window = g.unsqueeze(1) @ g.unsqueeze(0)\n",
    "        \n",
    "        # Expand to (channel, 1, window_size, window_size)\n",
    "        window = window.unsqueeze(0).unsqueeze(0)\n",
    "        window = window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "        \n",
    "        return window\n",
    "    \n",
    "    def forward(self, x_hat, x):\n",
    "        # TODO: Compute SSIM\n",
    "        mu_x = F.conv2d(x, self.window, padding=self.window_size//2, groups=self.channel)\n",
    "        mu_y = F.conv2d(x_hat, self.window, padding=self.window_size//2, groups=self.channel)\n",
    "        \n",
    "        mu_x_sq = mu_x ** 2\n",
    "        mu_y_sq = mu_y ** 2\n",
    "        mu_xy = mu_x * mu_y\n",
    "        \n",
    "        # Local variances\n",
    "        sigma_x_sq = F.conv2d(x ** 2, self.window, padding=self.window_size//2, \n",
    "                              groups=self.channel) - mu_x_sq\n",
    "        sigma_y_sq = F.conv2d(x_hat ** 2, self.window, padding=self.window_size//2,\n",
    "                              groups=self.channel) - mu_y_sq\n",
    "        sigma_xy = F.conv2d(x * x_hat, self.window, padding=self.window_size//2,\n",
    "                            groups=self.channel) - mu_xy\n",
    "        \n",
    "        # SSIM formula\n",
    "        numerator = (2 * mu_xy + self.C1) * (2 * sigma_xy + self.C2)\n",
    "        denominator = (mu_x_sq + mu_y_sq + self.C1) * (sigma_x_sq + sigma_y_sq + self.C2)\n",
    "        \n",
    "        ssim_map = numerator / (denominator + 1e-8)\n",
    "        \n",
    "        # Return 1 - mean SSIM as loss\n",
    "        return 1 - ssim_map.mean()\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SSIM(same): 0.000000\n",
      "✓ SSIM(diff): 0.9300\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test_ssim():\n",
    "    ssim_loss = SSIMLoss()\n",
    "    x = torch.rand(2, 1, 64, 64)\n",
    "    \n",
    "    loss_same = ssim_loss(x, x)\n",
    "    loss_diff = ssim_loss(torch.rand_like(x), x)\n",
    "    \n",
    "    assert loss_same < 0.01, f\"Same images should have ~0 loss, got {loss_same:.4f}\"\n",
    "    assert loss_diff > loss_same, \"Different images should have higher loss\"\n",
    "    print(f\"✓ SSIM(same): {loss_same:.6f}\")\n",
    "    print(f\"✓ SSIM(diff): {loss_diff:.4f}\")\n",
    "\n",
    "test_ssim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CNNAutoencoderProject\n"
     ]
    }
   ],
   "source": [
    "project_root = Path.cwd().parents[1] # Up from learningnotebooks/phase4_sar_codec/\n",
    "print(project_root)\n",
    "import sys\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "from losses.mse import MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Combined Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    loss = mse_weight * MSE + ssim_weight * (1 - SSIM)\n",
    "    \n",
    "    Returns: (loss_tensor, metrics_dict)\n",
    "    metrics_dict has: 'loss', 'mse', 'ssim', 'psnr'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mse_weight=1.0, ssim_weight=0.1, window_size = 11):\n",
    "        super().__init__()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.ssim_weight = ssim_weight\n",
    "        \n",
    "        self.mse_loss = MSELoss()\n",
    "        self.ssim_loss = SSIMLoss(window_size=window_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x_hat, x):\n",
    "        # TODO:\n",
    "        # 1. mse = F.mse_loss(x_hat, x)\n",
    "        # 2. ssim_l = self.ssim_loss(x_hat, x)\n",
    "        # 3. loss = weighted combination\n",
    "        # 4. psnr = 10 * log10(1 / mse)  (use torch.log10)\n",
    "        # 5. Return loss, {'loss':..., 'mse':..., 'ssim': 1-ssim_l, 'psnr':...}\n",
    "        \n",
    "        mse = self.mse_loss(x_hat, x)\n",
    "        ssim_l = self.ssim_loss(x_hat, x)\n",
    "        \n",
    "        loss = self.mse_weight * mse + self.ssim_weight * ssim_l\n",
    "        \n",
    "        # Compute PSNR (for logging)\n",
    "        with torch.no_grad():\n",
    "            psnr = 10 * torch.log10(1.0 / (mse + 1e-10))\n",
    "            ssim = 1 - ssim_l\n",
    "        \n",
    "        metrics = {\n",
    "            'loss': loss.item(),\n",
    "            'mse': mse.item(),\n",
    "            'ssim': ssim.item(),\n",
    "            'psnr': psnr.item(),\n",
    "        }\n",
    "        \n",
    "        return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loss: 0.0140\n",
      "✓ PSNR: 20.50 dB\n",
      "✓ SSIM: 0.9493\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test_combined_loss():\n",
    "    loss_fn = CombinedLoss()\n",
    "    x = torch.rand(2, 1, 64, 64)\n",
    "    x_noisy = (x + 0.1 * torch.randn_like(x)).clamp(0, 1)\n",
    "    \n",
    "    loss, metrics = loss_fn(x_noisy, x)\n",
    "    \n",
    "    assert all(k in metrics for k in ['loss', 'mse', 'ssim', 'psnr'])\n",
    "    print(f\"✓ Loss: {metrics['loss']:.4f}\")\n",
    "    print(f\"✓ PSNR: {metrics['psnr']:.2f} dB\")\n",
    "    print(f\"✓ SSIM: {metrics['ssim']:.4f}\")\n",
    "\n",
    "test_combined_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgePreservingLoss(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, mse_weight: float = 1.0, ssim_weight: float = 0.1,\n",
    "                 edge_weight: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mse_weight = mse_weight\n",
    "        self.ssim_weight = ssim_weight\n",
    "        self.edge_weight = edge_weight\n",
    "        \n",
    "        self.mse_loss = MSELoss()\n",
    "        self.ssim_loss = SSIMLoss()\n",
    "        \n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                               dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                               dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        \n",
    "        self.register_buffer('sobel_x', sobel_x)\n",
    "        self.register_buffer('sobel_y', sobel_y)\n",
    "        \n",
    "    def _compute_edges(self, x: torch.Tensor):\n",
    "        \"\"\"Compute edge magnitude using Sobel filters.\"\"\"\n",
    "        edge_x = F.conv2d(x, self.sobel_x, padding=1)\n",
    "        edge_y = F.conv2d(x, self.sobel_y, padding=1)\n",
    "        return torch.sqrt(edge_x ** 2 + edge_y ** 2 + 1e-8)\n",
    "    \n",
    "    def forward(self, x_hat: torch.Tensor, x: torch.Tensor):\n",
    "        \"\"\"Compute edge-preserving loss.\"\"\"\n",
    "        mse = self.mse_loss(x_hat, x)\n",
    "        ssim_l = self.ssim_loss(x_hat, x)\n",
    "        \n",
    "        # Edge loss\n",
    "        edges_x = self._compute_edges(x)\n",
    "        edges_x_hat = self._compute_edges(x_hat)\n",
    "        edge_loss = F.mse_loss(edges_x_hat, edges_x)\n",
    "        \n",
    "        loss = (self.mse_weight * mse + \n",
    "                self.ssim_weight * ssim_l + \n",
    "                self.edge_weight * edge_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            psnr = 10 * torch.log10(1.0 / (mse + 1e-10))\n",
    "        \n",
    "        metrics = {\n",
    "            'loss': loss.item(),\n",
    "            'mse': mse.item(),\n",
    "            'ssim': (1 - ssim_l).item(),\n",
    "            'edge': edge_loss.item(),\n",
    "            'psnr': psnr.item(),\n",
    "        }\n",
    "        \n",
    "        return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_losses():\n",
    "    \"\"\"Test loss functions.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LOSS FUNCTION TESTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create test images\n",
    "    x = torch.rand(4, 1, 64, 64)\n",
    "    \n",
    "    # Perfect reconstruction\n",
    "    x_hat_perfect = x.clone()\n",
    "    \n",
    "    # Noisy reconstruction\n",
    "    x_hat_noisy = x + 0.1 * torch.randn_like(x)\n",
    "    x_hat_noisy = x_hat_noisy.clamp(0, 1)\n",
    "    \n",
    "    # Blurry reconstruction\n",
    "    x_hat_blur = F.avg_pool2d(x, 3, stride=1, padding=1)\n",
    "    \n",
    "    # Test each loss\n",
    "    losses_to_test = [\n",
    "        ('MSE', MSELoss()),\n",
    "        ('SSIM', SSIMLoss()),\n",
    "        ('Combined', CombinedLoss()),\n",
    "        ('EdgePreserving', EdgePreservingLoss()),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nLoss values for different reconstruction types:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for loss_name, loss_fn in losses_to_test:\n",
    "        print(f\"\\n{loss_name}:\")\n",
    "        \n",
    "        returns_tuple = loss_name in ['Combined', 'EdgePreserving']\n",
    "        \n",
    "        for recon_name, x_hat in [('Perfect', x_hat_perfect), \n",
    "                                   ('Noisy', x_hat_noisy),\n",
    "                                   ('Blurry', x_hat_blur)]:\n",
    "            if returns_tuple:\n",
    "                loss, metrics = loss_fn(x_hat, x)\n",
    "                print(f\"  {recon_name}: loss={metrics['loss']:.4f}, psnr={metrics.get('psnr', 'N/A')}\")\n",
    "            else:\n",
    "                loss = loss_fn(x_hat, x)\n",
    "                print(f\"  {recon_name}: loss={loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\n✓ Loss function tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOSS FUNCTION TESTS\n",
      "============================================================\n",
      "\n",
      "Loss values for different reconstruction types:\n",
      "------------------------------------------------------------\n",
      "\n",
      "MSE:\n",
      "  Perfect: loss=0.0000\n",
      "  Noisy: loss=0.0090\n",
      "  Blurry: loss=0.0758\n",
      "\n",
      "SSIM:\n",
      "  Perfect: loss=0.0000\n",
      "  Noisy: loss=0.0513\n",
      "  Blurry: loss=0.7742\n",
      "\n",
      "Combined:\n",
      "  Perfect: loss=0.0000, psnr=100.0\n",
      "  Noisy: loss=0.0141, psnr=20.47527313232422\n",
      "  Blurry: loss=0.1532, psnr=11.203914642333984\n",
      "\n",
      "EdgePreserving:\n",
      "  Perfect: loss=0.0000, psnr=100.0\n",
      "  Noisy: loss=0.0237, psnr=20.47527313232422\n",
      "  Blurry: loss=0.2404, psnr=11.203914642333984\n",
      "\n",
      "✓ Loss function tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Training (60 minutes)\n",
    "\n",
    "## Dataset (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARPatchDataset(Dataset):\n",
    "    def __init__(self, patches, augment=True, normalisestats = None):\n",
    "        self.patches = patches.astype(np.float32)\n",
    "        self.augment = augment\n",
    "        self.normalisestats = normalisestats\n",
    "\n",
    "        assert len(patches.shape) == 3, f\"Expected (N, H, W), got {patches.shape}\"\n",
    "        assert patches.min() >= 0 and patches.max() <= 1, \\\n",
    "            f\"Expected [0,1] range, got [{patches.min()}, {patches.max()}]\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        patch = self.patches[idx].copy()\n",
    "        \n",
    "        if self.augment:\n",
    "            patch = self._augment(patch)\n",
    "        \n",
    "        # Add channel dimension: (H, W) → (1, H, W)\n",
    "        patch = torch.from_numpy(patch).unsqueeze(0)\n",
    "        \n",
    "        return patch\n",
    "    \n",
    "    def _augment(self, patch: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply random augmentations.\n",
    "        \n",
    "        For SAR, safe augmentations are:\n",
    "        - Horizontal flip\n",
    "        - Vertical flip\n",
    "        - 90° rotations\n",
    "        \"\"\"\n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            patch = np.fliplr(patch).copy()\n",
    "        \n",
    "        # Random vertical flip\n",
    "        if random.random() > 0.5:\n",
    "            patch = np.flipud(patch).copy()\n",
    "        \n",
    "        # Random 90° rotation (0, 90, 180, or 270 degrees)\n",
    "        k = random.randint(0, 3)\n",
    "        if k > 0:\n",
    "            patch = np.rot90(patch, k).copy()\n",
    "        \n",
    "        return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARDataModule:\n",
    "    \"\"\"\n",
    "    Data module for managing train/validation data.\n",
    "    \n",
    "    Handles:\n",
    "    - Loading patches from disk\n",
    "    - Splitting into train/validation\n",
    "    - Creating DataLoaders\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patches_path, val_fraction= 0.1,batch_size = 16, num_workers= 4, augment_train= True, seed = 42):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        print(f\"Loading patches from {patches_path}\")\n",
    "        all_patches = np.load(patches_path)\n",
    "        print(f\"Loaded {len(all_patches)} patches of shape {all_patches.shape[1:]}\")\n",
    "        \n",
    "        # Split into train/validation\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(len(all_patches))\n",
    "        val_size = int(len(all_patches) * val_fraction)\n",
    "        \n",
    "        val_indices = indices[:val_size]\n",
    "        train_indices = indices[val_size:]\n",
    "        \n",
    "        self.train_patches = all_patches[train_indices]\n",
    "        self.val_patches = all_patches[val_indices]\n",
    "        \n",
    "        print(f\"Train: {len(self.train_patches)}, Validation: {len(self.val_patches)}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        self.train_dataset = SARPatchDataset(self.train_patches, augment=augment_train)\n",
    "        self.val_dataset = SARPatchDataset(self.val_patches, augment=False)\n",
    "        \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get training DataLoader.\"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,  # Drop incomplete batches for stable BN\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get validation DataLoader.\"\"\"\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    \n",
    "    def get_sample_batch(self, split: str = 'train') -> torch.Tensor:\n",
    "        \"\"\"Get a sample batch for visualization.\"\"\"\n",
    "        loader = self.train_dataloader() if split == 'train' else self.val_dataloader()\n",
    "        return next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET TEST\n",
      "============================================================\n",
      "Loading patches from test_patches.npy\n",
      "Loaded 100 patches of shape (256, 256)\n",
      "Train: 80, Validation: 20\n",
      "\n",
      "Train batch shape: torch.Size([8, 1, 256, 256])\n",
      "Train batch range: [0.000, 1.000]\n",
      "Val batch shape: torch.Size([8, 1, 256, 256])\n",
      "Val batch range: [0.000, 1.000]\n",
      "\n",
      "Augmentation verified: same index gives different patches\n",
      "\n",
      "✓ Dataset test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_dataset():\n",
    "    \"\"\"Test dataset and dataloader.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET TEST\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create synthetic patches for testing\n",
    "    np.random.seed(42)\n",
    "    test_patches = np.random.rand(100, 256, 256).astype(np.float32)\n",
    "    np.save('test_patches.npy', test_patches)\n",
    "    \n",
    "    # Test DataModule\n",
    "    data_module = SARDataModule(\n",
    "        patches_path='test_patches.npy',\n",
    "        val_fraction=0.2,\n",
    "        batch_size=8,\n",
    "        num_workers=0,  # 0 for testing\n",
    "    )\n",
    "    \n",
    "    # Test train loader\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    train_batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"\\nTrain batch shape: {train_batch.shape}\")\n",
    "    print(f\"Train batch range: [{train_batch.min():.3f}, {train_batch.max():.3f}]\")\n",
    "    \n",
    "    # Test validation loader\n",
    "    val_loader = data_module.val_dataloader()\n",
    "    val_batch = next(iter(val_loader))\n",
    "    \n",
    "    print(f\"Val batch shape: {val_batch.shape}\")\n",
    "    print(f\"Val batch range: [{val_batch.min():.3f}, {val_batch.max():.3f}]\")\n",
    "    \n",
    "    # Verify augmentation creates variety\n",
    "    dataset = data_module.train_dataset\n",
    "    patch1 = dataset[0]\n",
    "    patch2 = dataset[0]  # Same index, should be different due to augmentation\n",
    "    \n",
    "    if torch.allclose(patch1, patch2):\n",
    "        print(\"\\nWARNING: Augmentation may not be working\")\n",
    "    else:\n",
    "        print(f\"\\nAugmentation verified: same index gives different patches\")\n",
    "    \n",
    "    # Cleanup\n",
    "    import os\n",
    "    os.remove('test_patches.npy')\n",
    "    \n",
    "    print(\"\\n✓ Dataset test passed!\")\n",
    "\n",
    "\n",
    "test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic test data...\n",
      "Loading patches from test_patches.npy\n",
      "Loaded 500 patches of shape (256, 256)\n",
      "Train: 450, Validation: 50\n",
      "Using device: cuda\n",
      "\n",
      "======================================================================\n",
      "TRAINING START\n",
      "======================================================================\n",
      "Epochs: 5\n",
      "Device: cuda\n",
      "Training samples: 450\n",
      "Validation samples: 50\n",
      "Batch size: 16\n",
      "Log directory: runs\\20260118_112047\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 28/28 [00:01<00:00, 19.66it/s, loss=0.1767, psnr=10.8]\n",
      "Epoch 1 [Val]: 100%|██████████| 4/4 [00:00<00:00, 69.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ New best model saved (val_loss: 0.1794)\n",
      "\n",
      "Epoch 1/5\n",
      "  Train: loss=0.1811, psnr=10.67, ssim=0.0478\n",
      "  Val:   loss=0.1794, psnr=10.79, ssim=0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 28/28 [00:01<00:00, 25.43it/s, loss=0.1681, psnr=10.9]\n",
      "Epoch 2 [Val]: 100%|██████████| 4/4 [00:00<00:00, 61.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ New best model saved (val_loss: 0.1710)\n",
      "\n",
      "Epoch 2/5\n",
      "  Train: loss=0.1726, psnr=10.89, ssim=0.0892\n",
      "  Val:   loss=0.1710, psnr=10.95, ssim=0.0945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 28/28 [00:01<00:00, 25.42it/s, loss=0.1585, psnr=11.1]\n",
      "Epoch 3 [Val]: 100%|██████████| 4/4 [00:00<00:00, 60.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ New best model saved (val_loss: 0.1584)\n",
      "\n",
      "Epoch 3/5\n",
      "  Train: loss=0.1629, psnr=11.02, ssim=0.1616\n",
      "  Val:   loss=0.1584, psnr=11.08, ssim=0.1954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 28/28 [00:01<00:00, 25.37it/s, loss=0.1511, psnr=11.2]\n",
      "Epoch 4 [Val]: 100%|██████████| 4/4 [00:00<00:00, 60.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ New best model saved (val_loss: 0.1510)\n",
      "\n",
      "Epoch 4/5\n",
      "  Train: loss=0.1543, psnr=11.15, ssim=0.2250\n",
      "  Val:   loss=0.1510, psnr=11.19, ssim=0.2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 28/28 [00:01<00:00, 25.57it/s, loss=0.1455, psnr=11.3]\n",
      "Epoch 5 [Val]: 100%|██████████| 4/4 [00:00<00:00, 60.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ New best model saved (val_loss: 0.1458)\n",
      "\n",
      "Epoch 5/5\n",
      "  Train: loss=0.1478, psnr=11.25, ssim=0.2715\n",
      "  Val:   loss=0.1458, psnr=11.28, ssim=0.2877\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE\n",
      "Best validation loss: 0.1458\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from typing import Dict, Optional, Callable\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "class Trainer:\n",
    "\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 loss_fn,\n",
    "                 config,\n",
    "                 device = None):\n",
    "\n",
    "        # Device setup\n",
    "        if device is None:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = torch.device(device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Model\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        # Data\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        \n",
    "        # Loss\n",
    "        self.loss_fn = loss_fn.to(self.device)\n",
    "        \n",
    "        # Config\n",
    "        self.config = config\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config.get('learning_rate', 1e-4),\n",
    "            betas=config.get('betas', (0.9, 0.999)),\n",
    "            weight_decay=config.get('weight_decay', 0),\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=config.get('lr_factor', 0.5),\n",
    "            patience=config.get('lr_patience', 10),\n",
    "        )\n",
    "        \n",
    "        # Logging\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.log_dir = Path(config.get('log_dir', 'runs')) / timestamp\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.writer = SummaryWriter(self.log_dir)\n",
    "        \n",
    "        # Save config\n",
    "        with open(self.log_dir / 'config.json', 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        # Checkpointing\n",
    "        self.checkpoint_dir = Path(config.get('checkpoint_dir', 'checkpoints'))\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Training state\n",
    "        self.epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.epochs_without_improvement = 0\n",
    "        \n",
    "    def train_epoch(self) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        epoch_metrics = {'loss': 0, 'mse': 0, 'ssim': 0, 'psnr': 0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {self.epoch+1} [Train]\")\n",
    "        \n",
    "        for batch in pbar:\n",
    "            x = batch.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            x_hat, z = self.model(x)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, metrics = self.loss_fn(x_hat, x)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            max_grad_norm = self.config.get('max_grad_norm', 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
    "            \n",
    "            # Update weights\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            for key in epoch_metrics:\n",
    "                if key in metrics:\n",
    "                    epoch_metrics[key] += metrics[key]\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{metrics['loss']:.4f}\",\n",
    "                'psnr': f\"{metrics.get('psnr', 0):.1f}\",\n",
    "            })\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            self.writer.add_scalar('train/loss_step', metrics['loss'], self.global_step)\n",
    "            self.global_step += 1\n",
    "        \n",
    "        # Average metrics\n",
    "        for key in epoch_metrics:\n",
    "            epoch_metrics[key] /= num_batches\n",
    "        \n",
    "        return epoch_metrics\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self) -> Dict[str, float]:\n",
    "        \"\"\"Validate on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        epoch_metrics = {'loss': 0, 'mse': 0, 'ssim': 0, 'psnr': 0}\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in tqdm(self.val_loader, desc=f\"Epoch {self.epoch+1} [Val]\"):\n",
    "            x = batch.to(self.device)\n",
    "            x_hat, z = self.model(x)\n",
    "            \n",
    "            loss, metrics = self.loss_fn(x_hat, x)\n",
    "            \n",
    "            for key in epoch_metrics:\n",
    "                if key in metrics:\n",
    "                    epoch_metrics[key] += metrics[key]\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average metrics\n",
    "        for key in epoch_metrics:\n",
    "            epoch_metrics[key] /= num_batches\n",
    "        \n",
    "        return epoch_metrics\n",
    "    \n",
    "    def save_checkpoint(self, is_best: bool = False):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': self.epoch,\n",
    "            'global_step': self.global_step,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': self.config,\n",
    "        }\n",
    "        \n",
    "        # Save latest\n",
    "        torch.save(checkpoint, self.checkpoint_dir / 'latest.pth')\n",
    "        \n",
    "        # Save best\n",
    "        if is_best:\n",
    "            torch.save(checkpoint, self.checkpoint_dir / 'best.pth')\n",
    "            print(f\"  ✓ New best model saved (val_loss: {self.best_val_loss:.4f})\")\n",
    "    \n",
    "    def load_checkpoint(self, path: str):\n",
    "        \"\"\"Load checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.epoch = checkpoint['epoch']\n",
    "        self.global_step = checkpoint['global_step']\n",
    "        self.best_val_loss = checkpoint['best_val_loss']\n",
    "        \n",
    "        print(f\"Loaded checkpoint from epoch {self.epoch}\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def log_images(self, num_images: int = 4):\n",
    "        \"\"\"Log sample reconstructions to TensorBoard.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get sample batch\n",
    "        batch = next(iter(self.val_loader))[:num_images].to(self.device)\n",
    "        x_hat, z = self.model(batch)\n",
    "        \n",
    "        # Create grid: original | reconstructed | difference\n",
    "        from torchvision.utils import make_grid\n",
    "        \n",
    "        # Original\n",
    "        self.writer.add_images('val/original', batch, self.epoch)\n",
    "        \n",
    "        # Reconstructed\n",
    "        self.writer.add_images('val/reconstructed', x_hat, self.epoch)\n",
    "        \n",
    "        # Difference (scaled for visibility)\n",
    "        diff = torch.abs(batch - x_hat)\n",
    "        diff = diff / diff.max() if diff.max() > 0 else diff\n",
    "        self.writer.add_images('val/difference', diff, self.epoch)\n",
    "    \n",
    "    def train(self, epochs: int, early_stopping_patience: int = 20):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of epochs to train\n",
    "            early_stopping_patience: Stop if no improvement for this many epochs\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TRAINING START\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Epochs: {epochs}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Training samples: {len(self.train_loader.dataset)}\")\n",
    "        print(f\"Validation samples: {len(self.val_loader.dataset)}\")\n",
    "        print(f\"Batch size: {self.train_loader.batch_size}\")\n",
    "        print(f\"Log directory: {self.log_dir}\")\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.epoch = epoch\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.validate()\n",
    "            \n",
    "            # Log metrics\n",
    "            for key, value in train_metrics.items():\n",
    "                self.writer.add_scalar(f'train/{key}', value, epoch)\n",
    "            for key, value in val_metrics.items():\n",
    "                self.writer.add_scalar(f'val/{key}', value, epoch)\n",
    "            \n",
    "            # Log learning rate\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            self.writer.add_scalar('train/learning_rate', lr, epoch)\n",
    "            \n",
    "            # Log images periodically\n",
    "            if epoch % 5 == 0:\n",
    "                self.log_images()\n",
    "            \n",
    "            # Update scheduler\n",
    "            self.scheduler.step(val_metrics['loss'])\n",
    "            \n",
    "            # Check for improvement\n",
    "            is_best = val_metrics['loss'] < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_metrics['loss']\n",
    "                self.epochs_without_improvement = 0\n",
    "            else:\n",
    "                self.epochs_without_improvement += 1\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(is_best=is_best)\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            print(f\"  Train: loss={train_metrics['loss']:.4f}, \"\n",
    "                  f\"psnr={train_metrics.get('psnr', 0):.2f}, \"\n",
    "                  f\"ssim={train_metrics.get('ssim', 0):.4f}\")\n",
    "            print(f\"  Val:   loss={val_metrics['loss']:.4f}, \"\n",
    "                  f\"psnr={val_metrics.get('psnr', 0):.2f}, \"\n",
    "                  f\"ssim={val_metrics.get('ssim', 0):.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.epochs_without_improvement >= early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping: no improvement for {early_stopping_patience} epochs\")\n",
    "                break\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TRAINING COMPLETE\")\n",
    "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        self.writer.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example training script.\"\"\"\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'latent_channels': 64,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 1e-4,\n",
    "        'mse_weight': 1.0,\n",
    "        'ssim_weight': 0.1,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'lr_factor': 0.5,\n",
    "        'lr_patience': 10,\n",
    "        'log_dir': 'runs',\n",
    "        'checkpoint_dir': 'checkpoints',\n",
    "    }\n",
    "    \n",
    "    # Create synthetic data for testing\n",
    "    print(\"Creating synthetic test data...\")\n",
    "    np.random.seed(42)\n",
    "    test_patches = np.random.rand(500, 256, 256).astype(np.float32)\n",
    "    np.save('test_patches.npy', test_patches)\n",
    "    \n",
    "    # Create data module\n",
    "    data_module = SARDataModule(\n",
    "        patches_path='test_patches.npy',\n",
    "        val_fraction=0.1,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = SARAutoencoder(latent_channels=config['latent_channels'])\n",
    "    \n",
    "    # Create loss function\n",
    "    loss_fn = CombinedLoss(\n",
    "        mse_weight=config['mse_weight'],\n",
    "        ssim_weight=config['ssim_weight']\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=data_module.train_dataloader(),\n",
    "        val_loader=data_module.val_dataloader(),\n",
    "        loss_fn=loss_fn,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train(epochs=5, early_stopping_patience=10)\n",
    "    \n",
    "    # Cleanup\n",
    "    import os\n",
    "    os.remove('test_patches.npy')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def get_npy_shape(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        f.read(8)\n",
    "        header_len = struct.unpack('<H', f.read(2))[0]\n",
    "        header = f.read(header_len).decode('latin1')\n",
    "        shape_start = header.find('(') + 1\n",
    "        shape_end = header.find(')')\n",
    "        shape_str = header[shape_start:shape_end]\n",
    "        shape = tuple(int(x.strip()) for x in shape_str.split(',') if x.strip())\n",
    "        return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LazyPatchDataset(Dataset):\n",
    "    def __init__(self, metadata, shuffle_idx, augment=True):\n",
    "        self.files = [Path(f) for f, _ in metadata['file_index']]\n",
    "        self.cumsum = [0]\n",
    "        for _, n in metadata['file_index']:\n",
    "            self.cumsum.append(self.cumsum[-1] + n)\n",
    "        self.shuffle_idx = shuffle_idx\n",
    "        self.augment = augment\n",
    "        self._cache_idx = None\n",
    "        self._cache_data = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.shuffle_idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.shuffle_idx[idx]\n",
    "        file_idx = next(i for i, (s, e) in enumerate(zip(self.cumsum[:-1], self.cumsum[1:])) if s <= real_idx < e)\n",
    "        local_idx = real_idx - self.cumsum[file_idx]\n",
    "        \n",
    "        if self._cache_idx != file_idx:\n",
    "            self._cache_idx = file_idx\n",
    "            self._cache_data = np.load(self.files[file_idx])\n",
    "        \n",
    "        patch = self._cache_data[local_idx].copy()\n",
    "        \n",
    "        if self.augment:\n",
    "            if random.random() > 0.5: patch = np.fliplr(patch).copy()\n",
    "            if random.random() > 0.5: patch = np.flipud(patch).copy()\n",
    "            k = random.randint(0, 3)\n",
    "            if k: patch = np.rot90(patch, k).copy()\n",
    "        \n",
    "        return torch.from_numpy(patch).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches: 696,277\n",
      "Global bounds: [14.77, 24.54] dB\n"
     ]
    }
   ],
   "source": [
    "outputdir = project_root / \"data\" / \"patches\"\n",
    "metadata = np.load(outputdir / 'metadata.npy', allow_pickle=True).item()\n",
    "shuffle_idx = np.load(outputdir / \"shuffle_idx.npy\")\n",
    "\n",
    "print(f\"Total patches: {metadata['num_patches']:,}\")\n",
    "print(f\"Global bounds: [{metadata['vmin']:.2f}, {metadata['vmax']:.2f}] dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Creating train dataset...\n",
      "Train: 1,000, Val: 200\n",
      "Creating dataloaders...\n",
      "Batch shape: torch.Size([16, 1, 256, 256]), range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "val_size = 200\n",
    "train_size = 1000\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = LazyPatchDataset(metadata, shuffle_idx[:train_size], augment=True)\n",
    "val_dataset = LazyPatchDataset(metadata, shuffle_idx[train_size:train_size + val_size], augment=False)\n",
    "\n",
    "print(\"Creating train dataset...\")\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,}, Val: {len(val_dataset):,}\")\n",
    "\n",
    "batch_size = 16\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "# Test batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {batch.shape}, range: [{batch.min():.3f}, {batch.max():.3f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 2,872,257\n",
      "Compression: 4.0x\n",
      "\n",
      "Baseline (untrained):\n",
      "  Loss: 0.1371\n",
      "  PSNR: 13.20 dB\n",
      "  SSIM: 0.1073\n",
      "  Latent shape: torch.Size([16, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "model = SARAutoencoder(latent_channels=64).to(device)\n",
    "loss_fn = CombinedLoss(mse_weight=1.0, ssim_weight=0.1).to(device)\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Compression: {model.get_compression_ratio():.1f}x\")\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    x = batch.to(device)\n",
    "    x_hat, z = model(x)\n",
    "    _, metrics = loss_fn(x_hat, x)\n",
    "\n",
    "print(f\"\\nBaseline (untrained):\")\n",
    "print(f\"  Loss: {metrics['loss']:.4f}\")\n",
    "print(f\"  PSNR: {metrics['psnr']:.2f} dB\")\n",
    "print(f\"  SSIM: {metrics['ssim']:.4f}\")\n",
    "print(f\"  Latent shape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'latent_channels': 64,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': 1e-4,\n",
    "    'mse_weight': 1.0,\n",
    "    'ssim_weight': 0.1,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'lr_factor': 0.5,\n",
    "    'lr_patience': 5,\n",
    "    'log_dir': str(project_root / 'runs'),\n",
    "    'checkpoint_dir': str(project_root / 'checkpoints'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "======================================================================\n",
      "TRAINING START\n",
      "======================================================================\n",
      "Epochs: 3\n",
      "Device: cuda\n",
      "Training samples: 1000\n",
      "Validation samples: 200\n",
      "Batch size: 16\n",
      "Log directory: d:\\Projects\\CNNAutoencoderProject\\runs\\20260118_105754\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/62 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(model, train_loader, val_loader, loss_fn, config)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 226\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, epochs, early_stopping_patience)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28mself\u001b[39m.epoch = epoch\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m train_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    229\u001b[39m val_metrics = \u001b[38;5;28mself\u001b[39m.validate()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m num_batches = \u001b[32m0\u001b[39m\n\u001b[32m     78\u001b[39m pbar = tqdm(\u001b[38;5;28mself\u001b[39m.train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.epoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Train]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# # Forward pass\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# self.optimizer.zero_grad()\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# x_hat, z = self.model(x)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# for key in epoch_metrics:\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m#     epoch_metrics[key] /= num_batches\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mLazyPatchDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache_idx != file_idx:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache_idx = file_idx\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache_data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfile_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m patch = \u001b[38;5;28mself\u001b[39m._cache_data[local_idx].copy()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.augment:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:483\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.open_memmap(file, mode=mmap_mode,\n\u001b[32m    481\u001b[39m                                   max_header_size=max_header_size)\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\numpy\\lib\\_format_impl.py:847\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[32m    846\u001b[39m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m         array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[32m    850\u001b[39m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    858\u001b[39m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[32m    859\u001b[39m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[32m    860\u001b[39m         array = numpy.ndarray(count, dtype=dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_loader, val_loader, loss_fn, config)\n",
    "trainer.train(epochs=3, early_stopping_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 626,650, Val: 69,627\n",
      "Using device: cuda\n",
      "\n",
      "======================================================================\n",
      "TRAINING START\n",
      "======================================================================\n",
      "Epochs: 50\n",
      "Device: cuda\n",
      "Training samples: 626650\n",
      "Validation samples: 69627\n",
      "Batch size: 16\n",
      "Log directory: d:\\Projects\\CNNAutoencoderProject\\runs\\20260118_112127\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 1/39165 [01:05<715:23:40, 65.76s/it, loss=0.1527, psnr=12.2]"
     ]
    }
   ],
   "source": [
    "val_size = metadata['num_patches'] // 10\n",
    "train_dataset = LazyPatchDataset(metadata, shuffle_idx[val_size:], augment=True)\n",
    "val_dataset = LazyPatchDataset(metadata, shuffle_idx[:val_size], augment=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,}, Val: {len(val_dataset):,}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False,\n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "model = SARAutoencoder(latent_channels=64)\n",
    "loss_fn = CombinedLoss(mse_weight=1.0, ssim_weight=0.1)\n",
    "\n",
    "config['lr_patience'] = 10\n",
    "trainer = Trainer(model, train_loader, val_loader, loss_fn, config)\n",
    "trainer.train(epochs=50, early_stopping_patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(project_root / 'checkpoints' / 'best.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Best val loss: {checkpoint['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample = next(iter(val_loader))[:8].to(device)\n",
    "    recon, latent = model(sample)\n",
    "\n",
    "loss_fn_eval = CombinedLoss(mse_weight=1.0, ssim_weight=0.1)\n",
    "_, metrics = loss_fn_eval(recon, sample)\n",
    "print(f\"Sample metrics: PSNR={metrics['psnr']:.2f} dB, SSIM={metrics['ssim']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patches: 696,277\n",
      "Train: 1,000, Val: 200\n",
      "Parameters: 2,872,257\n",
      "Using device: cpu\n",
      "\n",
      "======================================================================\n",
      "TRAINING START\n",
      "======================================================================\n",
      "Epochs: 3\n",
      "Device: cpu\n",
      "Training samples: 1000\n",
      "Validation samples: 200\n",
      "Batch size: 16\n",
      "Log directory: d:\\Projects\\CNNAutoencoderProject\\runs\\20260118_094427\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  15%|█▍        | 9/62 [11:41<1:08:51, 77.95s/it, loss=0.1186, psnr=15.4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     42\u001b[39m trainer = Trainer(model, train_loader, val_loader, loss_fn, config)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 226\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, epochs, early_stopping_patience)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28mself\u001b[39m.epoch = epoch\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m train_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    229\u001b[39m val_metrics = \u001b[38;5;28mself\u001b[39m.validate()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m num_batches = \u001b[32m0\u001b[39m\n\u001b[32m     78\u001b[39m pbar = tqdm(\u001b[38;5;28mself\u001b[39m.train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.epoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Train]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mLazyPatchDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache_idx != file_idx:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache_idx = file_idx\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache_data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfile_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m patch = \u001b[38;5;28mself\u001b[39m._cache_data[local_idx].copy()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.augment:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:483\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.open_memmap(file, mode=mmap_mode,\n\u001b[32m    481\u001b[39m                                   max_header_size=max_header_size)\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\numpy\\lib\\_format_impl.py:847\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[32m    846\u001b[39m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m         array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[32m    850\u001b[39m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    858\u001b[39m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[32m    859\u001b[39m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[32m    860\u001b[39m         array = numpy.ndarray(count, dtype=dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# outputdir = project_root / \"data\" / \"patches\"\n",
    "# metadata = np.load(outputdir / 'metadata.npy', allow_pickle=True).item()\n",
    "# shuffle_idx = np.load(outputdir / \"shuffle_idx.npy\")\n",
    "\n",
    "# print(f\"Total patches: {metadata['num_patches']:,}\")\n",
    "\n",
    "# # Split\n",
    "# val_size = metadata['num_patches'] // 10\n",
    "\n",
    "# train_dataset = LazyPatchDataset(metadata, shuffle_idx[:1000], augment=True)\n",
    "# val_dataset = LazyPatchDataset(metadata, shuffle_idx[1000:1200], augment=False)\n",
    "\n",
    "# print(f\"Train: {len(train_dataset):,}, Val: {len(val_dataset):,}\")\n",
    "\n",
    "# # Dataloaders\n",
    "# batch_size = 16\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "#                           num_workers=0, pin_memory=True, drop_last=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "#                         num_workers=0, pin_memory=True)\n",
    "\n",
    "# # Model\n",
    "# model = SARAutoencoder(latent_channels=64)\n",
    "# loss_fn = CombinedLoss(mse_weight=1.0, ssim_weight=0.1)\n",
    "# print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# # Config\n",
    "# config = {\n",
    "#     'latent_channels': 64,\n",
    "#     'batch_size': batch_size,\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'mse_weight': 1.0,\n",
    "#     'ssim_weight': 0.1,\n",
    "#     'max_grad_norm': 1.0,\n",
    "#     'lr_factor': 0.5,\n",
    "#     'lr_patience': 10,\n",
    "#     'log_dir': str(project_root / 'runs'),\n",
    "#     'checkpoint_dir': str(project_root / 'checkpoints'),\n",
    "# }\n",
    "\n",
    "# # Train\n",
    "# trainer = Trainer(model, train_loader, val_loader, loss_fn, config)\n",
    "# trainer.train(epochs=3, early_stopping_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0, i].imshow(sample[i, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0: axes[0, i].set_ylabel('Original', fontsize=12)\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(recon[i, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0: axes[1, i].set_ylabel('Reconstructed', fontsize=12)\n",
    "    \n",
    "    # Difference\n",
    "    diff = torch.abs(sample[i, 0] - recon[i, 0]).cpu()\n",
    "    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.2)\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0: axes[2, i].set_ylabel('Difference', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'Reconstruction Results (PSNR: {metrics[\"psnr\"]:.1f} dB, SSIM: {metrics[\"ssim\"]:.3f})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'checkpoints' / 'reconstruction_samples.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Latent shape: {latent.shape}\")\n",
    "print(f\"Latent stats: mean={latent.mean():.3f}, std={latent.std():.3f}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(latent[0, i].cpu(), cmap='viridis')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(f'Ch {i}')\n",
    "    \n",
    "    axes[1, i].imshow(latent[0, i+8].cpu(), cmap='viridis')\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title(f'Ch {i+8}')\n",
    "\n",
    "plt.suptitle('Latent Space Channels (first 16 of 64)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Done!\n",
    "\n",
    "If all tests pass and you can train, copy your implementations to:\n",
    "- `src/models/blocks.py` → ConvBlock, DeconvBlock\n",
    "- `src/models/encoder.py` → SAREncoder\n",
    "- `src/models/decoder.py` → SARDecoder\n",
    "- `src/models/autoencoder.py` → SARAutoencoder\n",
    "- `src/losses/ssim.py` → SSIMLoss\n",
    "- `src/losses/combined.py` → CombinedLoss\n",
    "- `src/training/trainer.py` → training functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
