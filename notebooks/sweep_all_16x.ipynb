{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweep: Baseline vs ResNet at 16x Compression\n",
    "\n",
    "Fair architecture comparison with correct hyperparameters.\n",
    "\n",
    "**Previous failures:** LR too high (7e-3), base_channels too small (32), OneCycleLR overshoot.  \n",
    "**This sweep:** LR=1e-4, AdamW, ReduceLROnPlateau, base_channels=64.\n",
    "\n",
    "**Monitor with TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\Projects\\CNNAutoencoderProject\n",
      "PyTorch: 2.5.1+cu121\n",
      "CUDA: True\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "VRAM: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data.datamodule import SARDataModule\n",
    "from src.models.autoencoder import SARAutoencoder\n",
    "from src.models.resnet_autoencoder import ResNetAutoencoder\n",
    "from src.losses.combined import CombinedLoss\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sweep Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression: 16x (LC=16)\n",
      "LR=0.0001, Epochs=35, Patience=12\n",
      "Data: 10% subset, batch_size=16\n",
      "\n",
      "Sweep Plan:\n",
      "  baseline_c16_b64_cr16x\n",
      "  resnet_c16_b64_cr16x\n",
      "\n",
      "Total runs: 2\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SWEEP CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Data\n",
    "DATA_PATH = \"D:/Projects/CNNAutoencoderProject/data/patches/metadata.npy\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "VAL_FRACTION = 0.1\n",
    "TRAIN_SUBSET = 0.10\n",
    "\n",
    "# Fixed training params (proven with baseline@16x -> 20.47 dB)\n",
    "LATENT_CHANNELS = 16  # 16x compression for all\n",
    "EPOCHS = 35\n",
    "LEARNING_RATE = 1e-4\n",
    "EARLY_STOPPING_PATIENCE = 12\n",
    "LR_PATIENCE = 10\n",
    "LR_FACTOR = 0.5\n",
    "MSE_WEIGHT = 0.5\n",
    "SSIM_WEIGHT = 0.5\n",
    "\n",
    "# Sweep variable: model architecture\n",
    "SWEEP_CONFIGS = [\n",
    "    {'name': 'baseline',  'cls': SARAutoencoder,       'base_channels': 64, 'extra_kwargs': {}},\n",
    "    {'name': 'resnet',    'cls': ResNetAutoencoder,     'base_channels': 64, 'extra_kwargs': {'in_channels': 1}},\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "compression_ratio = (256 * 256) / (16 * 16 * LATENT_CHANNELS)\n",
    "print(f\"Compression: {compression_ratio:.0f}x (LC={LATENT_CHANNELS})\")\n",
    "print(f\"LR={LEARNING_RATE}, Epochs={EPOCHS}, Patience={EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"Data: {TRAIN_SUBSET*100:.0f}% subset, batch_size={BATCH_SIZE}\")\n",
    "print()\n",
    "print(\"Sweep Plan:\")\n",
    "for cfg in SWEEP_CONFIGS:\n",
    "    name = f\"{cfg['name']}_c{LATENT_CHANNELS}_b{cfg['base_channels']}_cr{int(compression_ratio)}x\"\n",
    "    print(f\"  {name}\")\n",
    "print(f\"\\nTotal runs: {len(SWEEP_CONFIGS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading metadata from D:\\Projects\\CNNAutoencoderProject\\data\\patches\\metadata.npy\n",
      "Total patches: 696277\n",
      "Train: 626650, Val: 69627\n",
      "Using 10% subset:\n",
      "  Train: 62,665 of 626,650\n",
      "  Val: 6,962 of 69,627\n",
      "\n",
      "Preprocessing params: {'vmin': np.float32(14.768799), 'vmax': np.float32(24.54073)}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(\"Loading data...\")\n",
    "dm = SARDataModule(\n",
    "    patches_path=DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    val_fraction=VAL_FRACTION,\n",
    ")\n",
    "\n",
    "if TRAIN_SUBSET < 1.0:\n",
    "    full_train_size = len(dm.train_dataset)\n",
    "    train_subset_size = int(full_train_size * TRAIN_SUBSET)\n",
    "    train_indices = random.sample(range(full_train_size), train_subset_size)\n",
    "    dm.train_dataset = torch.utils.data.Subset(dm.train_dataset, train_indices)\n",
    "\n",
    "    full_val_size = len(dm.val_dataset)\n",
    "    val_subset_size = int(full_val_size * TRAIN_SUBSET)\n",
    "    val_indices = random.sample(range(full_val_size), val_subset_size)\n",
    "    dm.val_dataset = torch.utils.data.Subset(dm.val_dataset, val_indices)\n",
    "\n",
    "    print(f\"Using {TRAIN_SUBSET*100:.0f}% subset:\")\n",
    "    print(f\"  Train: {train_subset_size:,} of {full_train_size:,}\")\n",
    "    print(f\"  Val: {val_subset_size:,} of {full_val_size:,}\")\n",
    "\n",
    "print(f\"\\nPreprocessing params: {dm.preprocessing_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515adf4062c14281b80d2a1874dfb308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sweep Progress:   0%|          | 0/2 [00:00<?, ?run/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  SKIPPING baseline_c16_b64_cr16x — checkpoint exists\n",
      "  checkpoints\\baseline_c16_b64_cr16x_20260127_231730\\best.pth\n",
      "======================================================================\n",
      "  Validated: PSNR=19.09 dB, SSIM=0.5721\n",
      "\n",
      "======================================================================\n",
      "  resnet_c16_b64_cr16x\n",
      "======================================================================\n",
      "  Parameters: 22,395,873\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2026-01-28 00:39:26,730 - Log directory: runs\\resnet_c16_b64_cr16x_20260128_003926\n",
      "2026-01-28 00:39:26,730 - Checkpoint directory: checkpoints\\resnet_c16_b64_cr16x_20260128_003926\n",
      "2026-01-28 00:39:26,730 - Mixed Precision (AMP): enabled\n",
      "2026-01-28 00:39:26,732 - Starting training for 35 epochs\n",
      "2026-01-28 00:39:26,733 - Model: ResNetAutoencoder\n",
      "2026-01-28 00:39:26,733 - Config: {'learning_rate': 0.0001, 'optimizer': 'adamw', 'scheduler': 'plateau', 'lr_patience': 10, 'lr_factor': 0.5, 'max_grad_norm': 1.0, 'use_amp': True, 'notebook': True, 'run_name': 'resnet_c16_b64_cr16x', 'preprocessing_params': {'vmin': np.float32(14.768799), 'vmax': np.float32(24.54073)}, 'model_type': 'resnet', 'latent_channels': 16, 'base_channels': 64, 'mse_weight': 0.5, 'ssim_weight': 0.5, 'batch_size': 16, 'compression_ratio': 16.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 634MB / 8192MB (8% used, 7.4 GB free)\n",
      "Using AdamW optimizer with weight_decay=1e-05\n",
      "Using ReduceLROnPlateau: patience=10\n",
      "Mixed Precision (AMP) enabled - ~2x training speedup\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48a7bb64b214a8e8bce842c80606afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/3916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "results = []\n",
    "\n",
    "sweep_pbar = tqdm(SWEEP_CONFIGS, desc=\"Sweep Progress\", unit=\"run\")\n",
    "\n",
    "for cfg in sweep_pbar:\n",
    "    model_name = cfg['name']\n",
    "    base_channels = cfg['base_channels']\n",
    "    run_name = f\"{model_name}_c{LATENT_CHANNELS}_b{base_channels}_cr{int(compression_ratio)}x\"\n",
    "    sweep_pbar.set_postfix_str(run_name)\n",
    "\n",
    "    # Check for existing checkpoint — skip if already trained\n",
    "    existing = glob.glob(f'checkpoints/{run_name}_*/best.pth')\n",
    "    if existing:\n",
    "        checkpoint_path = existing[-1]  # most recent\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"  SKIPPING {run_name} — checkpoint exists\")\n",
    "        print(f\"  {checkpoint_path}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        # Load checkpoint to extract metrics\n",
    "        ckpt = torch.load(checkpoint_path, weights_only=False)\n",
    "        log_dir = checkpoint_path.replace('checkpoints', 'runs').replace('/best.pth', '').replace('\\\\best.pth', '')\n",
    "\n",
    "        # Run quick validation to get metrics\n",
    "        model = cfg['cls'](\n",
    "            latent_channels=LATENT_CHANNELS,\n",
    "            base_channels=base_channels,\n",
    "            **cfg['extra_kwargs'],\n",
    "        )\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        model.eval().cuda()\n",
    "        params = model.count_parameters()\n",
    "\n",
    "        loss_fn_tmp = CombinedLoss(mse_weight=MSE_WEIGHT, ssim_weight=SSIM_WEIGHT)\n",
    "        val_loader = dm.val_dataloader()\n",
    "\n",
    "        val_losses, val_psnrs, val_ssims = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.cuda()\n",
    "                output, _ = model(batch)\n",
    "                loss, metrics = loss_fn_tmp(output, batch)\n",
    "                val_losses.append(loss.item())\n",
    "                val_psnrs.append(metrics['psnr'])\n",
    "                val_ssims.append(metrics['ssim'])\n",
    "\n",
    "        best_psnr = sum(val_psnrs) / len(val_psnrs)\n",
    "        best_ssim = sum(val_ssims) / len(val_ssims)\n",
    "        best_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "        result = {\n",
    "            'run_name': Path(checkpoint_path).parent.name,\n",
    "            'model': model_name,\n",
    "            'base_channels': base_channels,\n",
    "            'parameters': params['total'],\n",
    "            'epochs_trained': ckpt.get('epoch', '?'),\n",
    "            'elapsed_min': 0,\n",
    "            'checkpoint': checkpoint_path,\n",
    "            'log_dir': log_dir,\n",
    "            'best_val_loss': best_loss,\n",
    "            'best_psnr': best_psnr,\n",
    "            'best_ssim': best_ssim,\n",
    "            'skipped': True,\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"  Validated: PSNR={best_psnr:.2f} dB, SSIM={best_ssim:.4f}\")\n",
    "\n",
    "        del model, loss_fn_tmp, val_loader\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"  {run_name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Create model\n",
    "    model = cfg['cls'](\n",
    "        latent_channels=LATENT_CHANNELS,\n",
    "        base_channels=base_channels,\n",
    "        **cfg['extra_kwargs'],\n",
    "    )\n",
    "    params = model.count_parameters()\n",
    "    print(f\"  Parameters: {params['total']:,}\")\n",
    "\n",
    "    # Loss\n",
    "    loss_fn = CombinedLoss(mse_weight=MSE_WEIGHT, ssim_weight=SSIM_WEIGHT)\n",
    "\n",
    "    # Trainer config\n",
    "    config = {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'optimizer': 'adamw',\n",
    "        'scheduler': 'plateau',\n",
    "        'lr_patience': LR_PATIENCE,\n",
    "        'lr_factor': LR_FACTOR,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'use_amp': True,\n",
    "        'notebook': True,\n",
    "        'run_name': run_name,\n",
    "        'preprocessing_params': dm.preprocessing_params,\n",
    "        'model_type': model_name,\n",
    "        'latent_channels': LATENT_CHANNELS,\n",
    "        'base_channels': base_channels,\n",
    "        'mse_weight': MSE_WEIGHT,\n",
    "        'ssim_weight': SSIM_WEIGHT,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'compression_ratio': compression_ratio,\n",
    "    }\n",
    "\n",
    "    train_loader = dm.train_dataloader()\n",
    "    val_loader = dm.val_dataloader()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        loss_fn=loss_fn,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    t0 = time.time()\n",
    "    history = trainer.train(\n",
    "        epochs=EPOCHS,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Collect results\n",
    "    result = {\n",
    "        'run_name': trainer.run_name,\n",
    "        'model': model_name,\n",
    "        'base_channels': base_channels,\n",
    "        'parameters': params['total'],\n",
    "        'epochs_trained': len(history),\n",
    "        'elapsed_min': elapsed / 60,\n",
    "        'checkpoint': str(trainer.checkpoint_dir / 'best.pth'),\n",
    "        'log_dir': str(trainer.log_dir),\n",
    "        'skipped': False,\n",
    "    }\n",
    "\n",
    "    if history:\n",
    "        best_epoch = min(history, key=lambda h: h.get('val_loss', float('inf')))\n",
    "        result['best_val_loss'] = best_epoch.get('val_loss')\n",
    "        result['best_psnr'] = best_epoch.get('val_psnr')\n",
    "        result['best_ssim'] = best_epoch.get('val_ssim')\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    psnr_str = f\"{result.get('best_psnr', 0):.2f} dB\" if result.get('best_psnr') else \"N/A\"\n",
    "    print(f\"\\n  Done: {psnr_str} | {elapsed/60:.1f} min | {trainer.checkpoint_dir / 'best.pth'}\")\n",
    "\n",
    "    # Cleanup GPU\n",
    "    del model, trainer, loss_fn, train_loader, val_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nSweep complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Run':<40} {'Params':>10} {'PSNR':>10} {'SSIM':>10} {'Epochs':>8} {'Time':>8}\")\n",
    "print(\"-\" * 88)\n",
    "\n",
    "for r in results:\n",
    "    name = r['run_name']\n",
    "    params = f\"{r['parameters']/1e6:.1f}M\"\n",
    "    psnr = f\"{r['best_psnr']:.2f} dB\" if r.get('best_psnr') else \"N/A\"\n",
    "    ssim = f\"{r['best_ssim']:.4f}\" if r.get('best_ssim') else \"N/A\"\n",
    "    epochs = str(r['epochs_trained'])\n",
    "    mins = f\"{r['elapsed_min']:.0f}m\"\n",
    "    print(f\"{name:<40} {params:>10} {psnr:>10} {ssim:>10} {epochs:>8} {mins:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Architecture Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "models = [r['model'] for r in results if r.get('best_psnr')]\n",
    "psnrs = [r['best_psnr'] for r in results if r.get('best_psnr')]\n",
    "ssims = [r['best_ssim'] for r in results if r.get('best_ssim')]\n",
    "param_counts = [r['parameters'] / 1e6 for r in results if r.get('best_psnr')]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "colors = ['#2196F3', '#4CAF50']\n",
    "\n",
    "# PSNR comparison\n",
    "ax = axes[0]\n",
    "bars = ax.bar(models, psnrs, color=colors[:len(models)])\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('PSNR by Architecture', fontsize=14)\n",
    "ax.axhline(y=25, color='r', linestyle='--', alpha=0.5, label='Target')\n",
    "for bar, val in zip(bars, psnrs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "            f'{val:.2f}', ha='center', fontsize=10)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# SSIM comparison\n",
    "ax = axes[1]\n",
    "bars = ax.bar(models, ssims, color=colors[:len(models)])\n",
    "ax.set_ylabel('SSIM', fontsize=12)\n",
    "ax.set_title('SSIM by Architecture', fontsize=14)\n",
    "ax.axhline(y=0.85, color='r', linestyle='--', alpha=0.5, label='Target')\n",
    "for bar, val in zip(bars, ssims):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{val:.4f}', ha='center', fontsize=10)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# PSNR vs Parameters (efficiency)\n",
    "ax = axes[2]\n",
    "ax.scatter(param_counts, psnrs, c=colors[:len(models)], s=150, zorder=5)\n",
    "for m, p, psnr in zip(models, param_counts, psnrs):\n",
    "    ax.annotate(m, (p, psnr), textcoords='offset points',\n",
    "                xytext=(5, 5), fontsize=10)\n",
    "ax.set_xlabel('Parameters (M)', fontsize=12)\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('Quality vs Model Size', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Architecture Comparison at 16x Compression', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "datestamp = datetime.now().strftime('%Y%m%d')\n",
    "save_path = f'runs/architecture_comparison_16x_{datestamp}.png'\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline ratios sweep and append 16x results for combined R-D curve\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Load previous baseline ratios sweep\n",
    "ratios_json = sorted(Path('runs').glob('sweep_baseline_ratios*.json'), key=lambda p: p.stat().st_mtime)\n",
    "if ratios_json:\n",
    "    with open(ratios_json[-1]) as f:\n",
    "        prev_sweep = json.load(f)\n",
    "    prev_results = prev_sweep['results']\n",
    "    print(f\"Loaded previous sweep: {ratios_json[-1].name}\")\n",
    "    print(f\"  Ratios: {[f'{r[\\\"compression_ratio\\\"]:.0f}x' for r in prev_results]}\")\n",
    "else:\n",
    "    prev_results = []\n",
    "    print(\"No previous sweep found — plotting 16x only\")\n",
    "\n",
    "# Get best baseline result from this sweep\n",
    "baseline_16x = next((r for r in results if r['model'] == 'baseline' and r.get('best_psnr')), None)\n",
    "resnet_16x = next((r for r in results if r['model'] == 'resnet' and r.get('best_psnr')), None)\n",
    "\n",
    "# Build combined baseline R-D data (4x, 8x, 12x from prev + 16x from this sweep)\n",
    "bl_ratios = [r['compression_ratio'] for r in prev_results if r.get('best_psnr')]\n",
    "bl_psnrs = [r['best_psnr'] for r in prev_results if r.get('best_psnr')]\n",
    "bl_ssims = [r['best_ssim'] for r in prev_results if r.get('best_ssim')]\n",
    "\n",
    "if baseline_16x:\n",
    "    bl_ratios.append(16.0)\n",
    "    bl_psnrs.append(baseline_16x['best_psnr'])\n",
    "    bl_ssims.append(baseline_16x['best_ssim'])\n",
    "\n",
    "# Sort by ratio\n",
    "sorted_idx = sorted(range(len(bl_ratios)), key=lambda i: bl_ratios[i])\n",
    "bl_ratios = [bl_ratios[i] for i in sorted_idx]\n",
    "bl_psnrs = [bl_psnrs[i] for i in sorted_idx]\n",
    "bl_ssims = [bl_ssims[i] for i in sorted_idx]\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PSNR\n",
    "ax1.plot(bl_ratios, bl_psnrs, 'bo-', markersize=8, linewidth=2, label='Baseline')\n",
    "for r, p in zip(bl_ratios, bl_psnrs):\n",
    "    ax1.annotate(f'{p:.1f}', (r, p), textcoords='offset points',\n",
    "                 xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "if resnet_16x:\n",
    "    ax1.plot(16.0, resnet_16x['best_psnr'], 'gs', markersize=10, label='ResNet @16x')\n",
    "    ax1.annotate(f\"{resnet_16x['best_psnr']:.1f}\", (16.0, resnet_16x['best_psnr']),\n",
    "                 textcoords='offset points', xytext=(10, -5), ha='left', fontsize=9, color='green')\n",
    "\n",
    "ax1.axhline(y=25, color='r', linestyle='--', alpha=0.5, label='Target (25 dB)')\n",
    "ax1.set_xlabel('Compression Ratio', fontsize=12)\n",
    "ax1.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax1.set_title('Rate-Distortion: PSNR', fontsize=14)\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_xticks(bl_ratios)\n",
    "ax1.set_xticklabels([f'{int(r)}x' for r in bl_ratios])\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.invert_xaxis()\n",
    "\n",
    "# SSIM\n",
    "ax2.plot(bl_ratios, bl_ssims, 'bo-', markersize=8, linewidth=2, label='Baseline')\n",
    "for r, s in zip(bl_ratios, bl_ssims):\n",
    "    ax2.annotate(f'{s:.3f}', (r, s), textcoords='offset points',\n",
    "                 xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "if resnet_16x:\n",
    "    ax2.plot(16.0, resnet_16x['best_ssim'], 'gs', markersize=10, label='ResNet @16x')\n",
    "    ax2.annotate(f\"{resnet_16x['best_ssim']:.3f}\", (16.0, resnet_16x['best_ssim']),\n",
    "                 textcoords='offset points', xytext=(10, -5), ha='left', fontsize=9, color='green')\n",
    "\n",
    "ax2.axhline(y=0.85, color='r', linestyle='--', alpha=0.5, label='Target (0.85)')\n",
    "ax2.set_xlabel('Compression Ratio', fontsize=12)\n",
    "ax2.set_ylabel('SSIM', fontsize=12)\n",
    "ax2.set_title('Rate-Distortion: SSIM', fontsize=14)\n",
    "ax2.set_xscale('log', base=2)\n",
    "ax2.set_xticks(bl_ratios)\n",
    "ax2.set_xticklabels([f'{int(r)}x' for r in bl_ratios])\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.invert_xaxis()\n",
    "\n",
    "plt.suptitle('Combined Rate-Distortion (4x → 16x)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "datestamp = datetime.now().strftime('%Y%m%d')\n",
    "save_path = f'runs/combined_rate_distortion_{datestamp}.png'\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print combined table\n",
    "print(f\"\\n{'Ratio':<8} {'Model':<12} {'PSNR':>10} {'SSIM':>10}\")\n",
    "print(\"-\" * 42)\n",
    "for r, p, s in zip(bl_ratios, bl_psnrs, bl_ssims):\n",
    "    print(f\"{int(r)}x{'':<5} {'baseline':<12} {p:>10.2f} {s:>10.4f}\")\n",
    "if resnet_16x:\n",
    "    print(f\"16x{'':<5} {'resnet':<12} {resnet_16x['best_psnr']:>10.2f} {resnet_16x['best_ssim']:>10.4f}\")\n",
    "\n",
    "print(f\"\\nSaved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "datestamp = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "output = {\n",
    "    'sweep_type': 'architecture_comparison_16x',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'latent_channels': LATENT_CHANNELS,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'train_subset': TRAIN_SUBSET,\n",
    "    },\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "output_path = Path(f'runs/sweep_all_16x_{datestamp}.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "**Next steps:**\n",
    "1. View TensorBoard: `tensorboard --logdir=runs`\n",
    "2. Compare with baseline rate-distortion curves from `sweep_baseline_ratios.ipynb`\n",
    "3. Pick best architecture and sweep its compression ratios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
