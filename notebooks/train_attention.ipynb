{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Autoencoder Training (Variant C)\n",
    "\n",
    "Training the AttentionAutoencoder (Pre-Activation Residual + CBAM) for SAR image compression at 16x compression ratio.\n",
    "\n",
    "**Variant C Architecture:**\n",
    "- Pre-activation residual blocks (BN->ReLU->Conv)\n",
    "- CBAM attention after every residual block (16 CBAM modules total)\n",
    "- 24M parameters (0.7% more than Variant B)\n",
    "- Target: +0.5 dB PSNR improvement over Variant B\n",
    "\n",
    "**Configuration (v2 - with stability improvements):**\n",
    "- Loss: 0.7 MSE + 0.3 SSIM (emphasize pixel accuracy for PSNR)\n",
    "- Learning rate: 5e-5 with 3-epoch linear warmup\n",
    "- Optimizer: AdamW with weight_decay=1e-5\n",
    "- Gradient clipping: 0.5\n",
    "- Batch size: 16 (CBAM memory overhead)\n",
    "- Epochs: 30 or early stopping\n",
    "- Data: 20% subset for fair comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Projects\\CNNAutoencoderProject\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "if project_root.name == 'notebooks':\n",
    "    project_root = project_root.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "VRAM: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Project imports\n",
    "from src.data.datamodule import SARDataModule\n",
    "from src.models import AttentionAutoencoder  # Variant C with CBAM!\n",
    "from src.losses.combined import CombinedLoss\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODE: QUICK SEARCH (5% data, 20 epochs)\n",
      "============================================================\n",
      "Model: AttentionAutoencoder (Variant C)\n",
      "Base channels: 48\n",
      "Compression ratio: 16x\n",
      "Training subset: 5% of data\n",
      "Epochs: 20\n",
      "Batch size: 16 (reduced for CBAM memory)\n",
      "Loss weights: 0.7 MSE + 0.3 SSIM\n",
      "Learning rate: 5e-05 with 3 epoch warmup\n",
      "Optimizer: ADAMW (weight_decay=1e-05)\n",
      "Gradient clipping: 0.5\n",
      "Run name: attention_v2_quick_c16\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Attention Autoencoder (Variant C)\n",
    "# ============================================================\n",
    "\n",
    "# =========================\n",
    "# TRAINING MODE\n",
    "# =========================\n",
    "QUICK_SEARCH = True  # True = 10% data, 20 epochs (hyperparameter search)\n",
    "                      # False = 20% data, 30 epochs (full training)\n",
    "\n",
    "# Data settings\n",
    "DATA_PATH = \"D:/Projects/CNNAutoencoderProject/data/patches/metadata.npy\"\n",
    "BATCH_SIZE = 16       # REDUCED: CBAM memory overhead, batch=32 causes OOM\n",
    "NUM_WORKERS = 0       # Set to 0 for Windows compatibility\n",
    "VAL_FRACTION = 0.1    # 10% validation split\n",
    "TRAIN_SUBSET = 0.05 if QUICK_SEARCH else 0.20\n",
    "\n",
    "# Model settings\n",
    "LATENT_CHANNELS = 16  # 16 = 16x compression\n",
    "BASE_CHANNELS = 48    # Full channels for Variant C\n",
    "\n",
    "# Loss settings - emphasize pixel accuracy for PSNR (per CONTEXT.md)\n",
    "MSE_WEIGHT = 0.7\n",
    "SSIM_WEIGHT = 0.3\n",
    "\n",
    "# Training settings - with stability improvements\n",
    "EPOCHS = 20 if QUICK_SEARCH else 30\n",
    "LEARNING_RATE = 5e-5      # With warmup for stability\n",
    "WARMUP_EPOCHS = 3         # Linear warmup\n",
    "OPTIMIZER = 'adamw'       # AdamW handles weight decay better\n",
    "WEIGHT_DECAY = 1e-5       # Regularization\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "LR_PATIENCE = 5\n",
    "LR_FACTOR = 0.5\n",
    "MAX_GRAD_NORM = 0.5       # More aggressive clipping\n",
    "USE_AMP = True            # Mixed precision\n",
    "\n",
    "# Output settings\n",
    "mode_suffix = \"quick\" if QUICK_SEARCH else \"full\"\n",
    "RUN_NAME = f\"attention_v2_{mode_suffix}_c{LATENT_CHANNELS}\"\n",
    "\n",
    "# Calculate compression ratio\n",
    "compression_ratio = (256 * 256) / (16 * 16 * LATENT_CHANNELS)\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"MODE: {'QUICK SEARCH (5% data, 20 epochs)' if QUICK_SEARCH else 'FULL TRAINING (20% data, 30 epochs)'}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: AttentionAutoencoder (Variant C)\")\n",
    "print(f\"Base channels: {BASE_CHANNELS}\")\n",
    "print(f\"Compression ratio: {compression_ratio:.0f}x\")\n",
    "print(f\"Training subset: {TRAIN_SUBSET*100:.0f}% of data\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (reduced for CBAM memory)\")\n",
    "print(f\"Loss weights: {MSE_WEIGHT} MSE + {SSIM_WEIGHT} SSIM\")\n",
    "print(f\"Learning rate: {LEARNING_RATE} with {WARMUP_EPOCHS} epoch warmup\")\n",
    "print(f\"Optimizer: {OPTIMIZER.upper()} (weight_decay={WEIGHT_DECAY})\")\n",
    "print(f\"Gradient clipping: {MAX_GRAD_NORM}\")\n",
    "print(f\"Run name: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading metadata from D:\\Projects\\CNNAutoencoderProject\\data\\patches\\metadata.npy\n",
      "Total patches: 696277\n",
      "Train: 626650, Val: 69627\n",
      "Using 5% subset:\n",
      "  Train: 31,332 of 626,650\n",
      "  Val: 3,481 of 69,627\n",
      "\n",
      "Dataset loaded:\n",
      "  Train patches: 31,332\n",
      "  Val patches: 3,481\n",
      "  Train batches: 1,958\n",
      "  Val batches: 218\n",
      "  Preprocessing params: {'vmin': np.float32(14.768799), 'vmax': np.float32(24.54073)}\n",
      "\n",
      "  Estimated time per epoch: ~36 minutes\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "dm = SARDataModule(\n",
    "    patches_path=DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    val_fraction=VAL_FRACTION,\n",
    ")\n",
    "\n",
    "# Apply subset to BOTH train and val for faster iteration\n",
    "if TRAIN_SUBSET < 1.0:\n",
    "    # Subset training data\n",
    "    full_train_size = len(dm.train_dataset)\n",
    "    train_subset_size = int(full_train_size * TRAIN_SUBSET)\n",
    "    train_indices = random.sample(range(full_train_size), train_subset_size)\n",
    "    dm.train_dataset = torch.utils.data.Subset(dm.train_dataset, train_indices)\n",
    "    \n",
    "    # Subset validation data (same proportion)\n",
    "    full_val_size = len(dm.val_dataset)\n",
    "    val_subset_size = int(full_val_size * TRAIN_SUBSET)\n",
    "    val_indices = random.sample(range(full_val_size), val_subset_size)\n",
    "    dm.val_dataset = torch.utils.data.Subset(dm.val_dataset, val_indices)\n",
    "    \n",
    "    print(f\"Using {TRAIN_SUBSET*100:.0f}% subset:\")\n",
    "    print(f\"  Train: {train_subset_size:,} of {full_train_size:,}\")\n",
    "    print(f\"  Val: {val_subset_size:,} of {full_val_size:,}\")\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Train patches: {len(dm.train_dataset):,}\")\n",
    "print(f\"  Val patches: {len(dm.val_dataset):,}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")\n",
    "print(f\"  Preprocessing params: {dm.preprocessing_params}\")\n",
    "\n",
    "# Estimate epoch time (batch_size=16 is slower than 32)\n",
    "est_batches = len(train_loader) + len(val_loader)\n",
    "est_time_min = est_batches / 1.0 / 60  # ~1.0 it/s with larger model + smaller batch\n",
    "print(f\"\\n  Estimated time per epoch: ~{est_time_min:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([16, 1, 256, 256])\n",
      "Sample batch dtype: torch.float32\n",
      "Sample batch range: [0.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "# Verify a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample batch dtype: {sample_batch.dtype}\")\n",
    "print(f\"Sample batch range: [{sample_batch.min():.4f}, {sample_batch.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating AttentionAutoencoder (Variant C)...\n",
      "\n",
      "Model created:\n",
      "  Architecture: AttentionAutoencoder (Pre-Act Residual + CBAM)\n",
      "  Total parameters: 13,529,761\n",
      "  Encoder params: 6,318,752\n",
      "  Decoder params: 7,211,009\n",
      "  Compression ratio: 16.0x\n",
      "  Latent size: (16, 16, 16)\n",
      "  CBAM modules: 16\n",
      "\n",
      "  vs Baseline: 6.0x more parameters\n",
      "  vs ResNet-Lite: 2.4x more parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating AttentionAutoencoder (Variant C)...\")\n",
    "model = AttentionAutoencoder(\n",
    "    latent_channels=LATENT_CHANNELS,\n",
    "    base_channels=BASE_CHANNELS,\n",
    ")\n",
    "\n",
    "params = model.count_parameters()\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Architecture: AttentionAutoencoder (Pre-Act Residual + CBAM)\")\n",
    "print(f\"  Total parameters: {params['total']:,}\")\n",
    "print(f\"  Encoder params: {params['encoder']:,}\")\n",
    "print(f\"  Decoder params: {params['decoder']:,}\")\n",
    "print(f\"  Compression ratio: {model.get_compression_ratio():.1f}x\")\n",
    "print(f\"  Latent size: {model.get_latent_size()}\")\n",
    "\n",
    "# Count CBAM modules\n",
    "cbam_count = sum(1 for m in model.modules() if m.__class__.__name__ == 'CBAM')\n",
    "print(f\"  CBAM modules: {cbam_count}\")\n",
    "\n",
    "# Compare to other models\n",
    "baseline_params = 2_257_809\n",
    "resnet_lite_params = 5_648_033\n",
    "print(f\"\\n  vs Baseline: {params['total'] / baseline_params:.1f}x more parameters\")\n",
    "print(f\"  vs ResNet-Lite: {params['total'] / resnet_lite_params:.1f}x more parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward pass...\n",
      "  Input: torch.Size([2, 1, 256, 256])\n",
      "  Latent: torch.Size([2, 16, 16, 16])\n",
      "  Output: torch.Size([2, 1, 256, 256])\n",
      "  Output range: [0.0000, 1.0000]\n",
      "  GPU memory allocated: 0.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass and GPU memory\n",
    "print(\"Testing forward pass...\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_test = model.to(device)\n",
    "x_test = sample_batch[:2].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat, z = model_test(x_test)\n",
    "\n",
    "print(f\"  Input: {x_test.shape}\")\n",
    "print(f\"  Latent: {z.shape}\")\n",
    "print(f\"  Output: {x_hat.shape}\")\n",
    "print(f\"  Output range: [{x_hat.min():.4f}, {x_hat.max():.4f}]\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Clean up test\n",
    "del model_test, x_test, x_hat, z\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: CombinedLoss\n",
      "  MSE weight: 0.7 (emphasize pixel accuracy)\n",
      "  SSIM weight: 0.3\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CombinedLoss(\n",
    "    mse_weight=MSE_WEIGHT,\n",
    "    ssim_weight=SSIM_WEIGHT,\n",
    ")\n",
    "\n",
    "print(f\"Loss function: CombinedLoss\")\n",
    "print(f\"  MSE weight: {MSE_WEIGHT} (emphasize pixel accuracy)\")\n",
    "print(f\"  SSIM weight: {SSIM_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2026-01-26 16:17:38,290 - Log directory: runs\\attention_v2_quick_c16\n",
      "2026-01-26 16:17:38,290 - Checkpoint directory: checkpoints\\attention_v2_quick_c16\n",
      "2026-01-26 16:17:38,291 - Mixed Precision (AMP): enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing trainer...\n",
      "Using device: cuda\n",
      "GPU memory: 2456MB / 8192MB (30% used, 5.6 GB free)\n",
      "Using AdamW optimizer with weight_decay=1e-05\n",
      "Learning rate warmup: 3 epochs (5.00e-06 -> 5.00e-05)\n",
      "Mixed Precision (AMP) enabled - ~2x training speedup\n",
      "\n",
      "Trainer ready:\n",
      "  Log dir: runs\\attention_v2_quick_c16\n",
      "  Checkpoint dir: checkpoints\\attention_v2_quick_c16\n",
      "  Device: cuda\n",
      "  AMP enabled: True\n",
      "  Warmup epochs: 3\n",
      "  Base LR: 5e-05\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'warmup_epochs': WARMUP_EPOCHS,      # Linear warmup\n",
    "    'optimizer': OPTIMIZER,               # 'adam' or 'adamw'\n",
    "    'weight_decay': WEIGHT_DECAY,         # For AdamW\n",
    "    'lr_patience': LR_PATIENCE,\n",
    "    'lr_factor': LR_FACTOR,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'run_name': RUN_NAME,\n",
    "    'preprocessing_params': dm.preprocessing_params,\n",
    "    'use_amp': USE_AMP,\n",
    "    # Store hyperparams for reproducibility\n",
    "    'model_type': 'AttentionAutoencoder-Variant-C',\n",
    "    'latent_channels': LATENT_CHANNELS,\n",
    "    'base_channels': BASE_CHANNELS,\n",
    "    'mse_weight': MSE_WEIGHT,\n",
    "    'ssim_weight': SSIM_WEIGHT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrainer ready:\")\n",
    "print(f\"  Log dir: {trainer.log_dir}\")\n",
    "print(f\"  Checkpoint dir: {trainer.checkpoint_dir}\")\n",
    "print(f\"  Device: {trainer.device}\")\n",
    "print(f\"  AMP enabled: {trainer.use_amp}\")\n",
    "print(f\"  Warmup epochs: {trainer.warmup_epochs}\")\n",
    "print(f\"  Base LR: {trainer.base_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "**Monitor with TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=D:/Projects/CNNAutoencoderProject/notebooks/runs\n",
    "```\n",
    "\n",
    "Compare `attention_v1_c16` with `baseline_c16_fast` and `resnet_lite_v2_c16` in TensorBoard.\n",
    "\n",
    "**Expected training time:** ~2 hours per epoch (7,833 train + 871 val batches at batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Starting Attention Autoencoder Training (Variant C)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: AttentionAutoencoder ({params['total']:,} params)\")\n",
    "print(f\"CBAM modules: {cbam_count}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nTensorBoard: tensorboard --logdir={trainer.log_dir.parent}\")\n",
    "print(f\"Checkpoints: {trainer.checkpoint_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = trainer.train(\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training was not run in this session. Loading from checkpoint...\n",
      "No checkpoint found at: checkpoints\\attention_v2_quick_c16\\best.pth\n",
      "Please run training first (Cell 10)\n"
     ]
    }
   ],
   "source": [
    "# Load results from checkpoint if training wasn't run in this session\n",
    "checkpoint_path = Path(f\"checkpoints/{RUN_NAME}/best.pth\")\n",
    "\n",
    "if 'history' not in dir() or history is None:\n",
    "    print(\"Training was not run in this session. Loading from checkpoint...\")\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "        \n",
    "        # Load model weights\n",
    "        model = AttentionAutoencoder(latent_channels=LATENT_CHANNELS, base_channels=BASE_CHANNELS)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"Loaded model from: {checkpoint_path}\")\n",
    "        \n",
    "        # Get history from checkpoint\n",
    "        history = checkpoint.get('history', None)\n",
    "        \n",
    "        if history:\n",
    "            print(f\"Loaded history with {len(history)} epochs\")\n",
    "        else:\n",
    "            print(\"No history in checkpoint, running quick validation...\")\n",
    "            # Run quick validation to get current metrics\n",
    "            model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            val_losses, val_psnrs, val_ssims = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(next(model.parameters()).device)\n",
    "                    output, _ = model(batch)\n",
    "                    loss, metrics = loss_fn(output, batch)\n",
    "                    val_losses.append(loss.item())\n",
    "                    val_psnrs.append(metrics['psnr'])\n",
    "                    val_ssims.append(metrics['ssim'])\n",
    "            \n",
    "            loaded_metrics = {\n",
    "                'val_loss': sum(val_losses) / len(val_losses),\n",
    "                'val_psnr': sum(val_psnrs) / len(val_psnrs),\n",
    "                'val_ssim': sum(val_ssims) / len(val_ssims),\n",
    "            }\n",
    "            print(f\"  Val Loss: {loaded_metrics['val_loss']:.4f}\")\n",
    "            print(f\"  Val PSNR: {loaded_metrics['val_psnr']:.2f} dB\")\n",
    "            print(f\"  Val SSIM: {loaded_metrics['val_ssim']:.4f}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at: {checkpoint_path}\")\n",
    "        print(\"Please run training first (Cell 10)\")\n",
    "else:\n",
    "    print(\"Training history available from this session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Summary\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Summary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhistory\u001b[49m:\n\u001b[32m      6\u001b[39m     final = history[-\u001b[32m1\u001b[39m]\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel: AttentionAutoencoder (Variant C)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    print(f\"Model: AttentionAutoencoder (Variant C)\")\n",
    "    print(f\"Final epoch: {final['epoch'] + 1}\")\n",
    "    print(f\"Best val loss: {trainer.best_val_loss:.4f}\")\n",
    "    print(f\"Final val PSNR: {final['val_psnr']:.2f} dB\")\n",
    "    print(f\"Final val SSIM: {final['val_ssim']:.4f}\")\n",
    "    \n",
    "    # Compare to baseline and ResNet-Lite\n",
    "    baseline_psnr = 20.47\n",
    "    resnet_lite_psnr = 21.20\n",
    "    \n",
    "    improvement_baseline = final['val_psnr'] - baseline_psnr\n",
    "    improvement_resnet = final['val_psnr'] - resnet_lite_psnr\n",
    "    \n",
    "    print(f\"\\nImprovement over baseline: {improvement_baseline:+.2f} dB\")\n",
    "    print(f\"Improvement over ResNet-Lite: {improvement_resnet:+.2f} dB\")\n",
    "    \n",
    "    if final['val_psnr'] >= 22.0:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"[SUCCESS] PSNR >= 22.0 dB achieved!\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\n[INFO] PSNR {final['val_psnr']:.2f} dB (target: 22.0 dB)\")\n",
    "else:\n",
    "    print(\"[ERROR] No training history available.\")\n",
    "    print(\"Please run training (Cell 10) first.\")\n",
    "\n",
    "print(f\"\\nCheckpoint: checkpoints/{RUN_NAME}/best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def format_improvement(current, baseline):\n",
    "    diff = current - baseline\n",
    "    return f\"+{diff:.2f}\" if diff >= 0 else f\"{diff:.2f}\"\n",
    "\n",
    "# Previous results\n",
    "baseline_results = {\n",
    "    'name': 'Baseline',\n",
    "    'params': 2_257_809,\n",
    "    'psnr': 20.47,\n",
    "    'ssim': 0.646,\n",
    "    'loss': 0.1813\n",
    "}\n",
    "\n",
    "resnet_lite_results = {\n",
    "    'name': 'ResNet-Lite v2',\n",
    "    'params': 5_648_033,\n",
    "    'psnr': 21.20,\n",
    "    'ssim': 0.726,\n",
    "    'loss': 0.1410\n",
    "}\n",
    "\n",
    "# Variant C results\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    attention_results = {\n",
    "        'name': 'Attention v2 (C)',\n",
    "        'params': params['total'],\n",
    "        'psnr': final['val_psnr'],\n",
    "        'ssim': final['val_ssim'],\n",
    "        'loss': final['val_loss']\n",
    "    }\n",
    "    \n",
    "    all_models = [baseline_results, resnet_lite_results, attention_results]\n",
    "    \n",
    "    print(f\"{'Model':<20} {'Params':>12}  {'PSNR (dB)':>10}  {'SSIM':>8}  {'vs Baseline':>12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for m in all_models:\n",
    "        params_str = f\"{m['params']:,}\"\n",
    "        improvement = format_improvement(m['psnr'], baseline_results['psnr'])\n",
    "        print(f\"{m['name']:<20} {params_str:>12}  {m['psnr']:>10.2f}  {m['ssim']:>8.4f}  {improvement:>12}\")\n",
    "else:\n",
    "    print(\"No Attention model results available. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if history:\n",
    "    epochs_list = [h['epoch'] + 1 for h in history]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs_list, [h['train_loss'] for h in history], label='Train')\n",
    "    ax.plot(epochs_list, [h['val_loss'] for h in history], label='Val')\n",
    "    ax.axhline(y=baseline_results['loss'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax.axhline(y=resnet_lite_results['loss'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss (Attention v1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # PSNR\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs_list, [h['train_psnr'] for h in history], label='Train')\n",
    "    ax.plot(epochs_list, [h['val_psnr'] for h in history], label='Val')\n",
    "    ax.axhline(y=22.0, color='r', linestyle='--', label='Target (22 dB)')\n",
    "    ax.axhline(y=baseline_results['psnr'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax.axhline(y=resnet_lite_results['psnr'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('PSNR (dB)')\n",
    "    ax.set_title('PSNR (Attention v1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # SSIM\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(epochs_list, [h['train_ssim'] for h in history], label='Train')\n",
    "    ax.plot(epochs_list, [h['val_ssim'] for h in history], label='Val')\n",
    "    ax.axhline(y=baseline_results['ssim'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax.axhline(y=resnet_lite_results['ssim'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_title('SSIM (Attention v1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(epochs_list, [h['learning_rate'] for h in history])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('Learning Rate Schedule')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.suptitle('Attention Autoencoder (Variant C) Training', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to log_dir\n",
    "    save_path = Path(f\"runs/{RUN_NAME}/training_curves.png\")\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved training curves to: {save_path}\")\n",
    "else:\n",
    "    print(\"Training curves not available (model loaded from checkpoint).\")\n",
    "    print(f\"\\nCheck TensorBoard for historical curves:\")\n",
    "    print(f\"  tensorboard --logdir=runs/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample reconstructions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model from checkpoint if needed\n",
    "checkpoint_path = Path(f\"checkpoints/{RUN_NAME}/best.pth\")\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    model = AttentionAutoencoder(latent_channels=LATENT_CHANNELS, base_channels=BASE_CHANNELS)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model from: {checkpoint_path}\")\n",
    "\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Get a batch from validation\n",
    "val_batch = next(iter(val_loader))[:4].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, latent = model(val_batch)\n",
    "\n",
    "# Move to CPU for plotting\n",
    "originals = val_batch.cpu().numpy()\n",
    "reconstructions = reconstructed.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(4):\n",
    "    # Original\n",
    "    axes[0, i].imshow(originals[i, 0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Original {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstruction\n",
    "    axes[1, i].imshow(reconstructions[i, 0], cmap='gray')\n",
    "    axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Difference\n",
    "    diff = abs(originals[i, 0] - reconstructions[i, 0])\n",
    "    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[2, i].set_title(f'Difference {i+1}')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Difference', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'Attention v1 Reconstructions (Compression: {model.get_compression_ratio():.0f}x, Params: {params[\"total\"]:,})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "save_path = Path(f\"runs/{RUN_NAME}/sample_reconstructions.png\")\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(save_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved reconstructions to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SAR-Specific Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.metrics import enl_ratio, edge_preservation_index\n",
    "                                                                                                                                                                                                                                                                                                    # Load real SAR patches - use Path properly\n",
    "patches_dir = project_root / \"data\" / \"patches\"                                                                                                                                                                                                                                                  \n",
    "patches_files = [f for f in patches_dir.glob(\"*.npy\") if 'metadata' not in f.name]\n",
    "patches_file = patches_files[0]\n",
    "patches = np.load(str(patches_file), mmap_mode='r')\n",
    "\n",
    "print(f\"Loaded patches from: {patches_file.name}\")\n",
    "print(f\"Shape: {patches.shape}\")\n",
    "\n",
    "# Evaluate on multiple patches\n",
    "n_eval = 10\n",
    "enl_ratios = []\n",
    "epis = []\n",
    "\n",
    "model.eval()\n",
    "for i in range(n_eval):\n",
    "    original = patches[i].copy()\n",
    "\n",
    "# Reconstruct\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.from_numpy(original).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    recon, _ = model(input_tensor)\n",
    "reconstructed = recon[0, 0].cpu().numpy()\n",
    "\n",
    "# ENL ratio\n",
    "result = enl_ratio(original, reconstructed)\n",
    "enl_ratios.append(result['enl_ratio'])\n",
    "\n",
    "# EPI\n",
    "epi = edge_preservation_index(original, reconstructed)\n",
    "epis.append(epi)\n",
    "\n",
    "print(f\"\\nSAR-Specific Metrics ({n_eval} samples):\")\n",
    "print(f\"  ENL Ratio: {np.mean(enl_ratios):.3f} +/- {np.std(enl_ratios):.3f}\")\n",
    "print(f\"  EPI: {np.mean(epis):.4f} +/- {np.std(epis):.4f}\")\n",
    "print(f\"\\n  ENL ratio target: [0.7, 1.3]\")\n",
    "print(f\"  ENL ratio in range: {0.7 <= np.mean(enl_ratios) <= 1.3}\")\n",
    "print(f\"  EPI target: > 0.8\")\n",
    "print(f\"  EPI target met: {np.mean(epis) > 0.8}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison with a single patch\n",
    "original = patches[0].copy()\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.from_numpy(original).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    recon, _ = model(input_tensor)\n",
    "reconstructed = recon[0, 0].cpu().numpy()\n",
    "\n",
    "# Compute metrics for this patch\n",
    "from src.evaluation.metrics import SARMetrics\n",
    "psnr = SARMetrics.psnr(original.flatten(), reconstructed.flatten())\n",
    "ssim = SARMetrics.ssim(original.flatten(), reconstructed.flatten())\n",
    "result = enl_ratio(original, reconstructed)\n",
    "epi = edge_preservation_index(original, reconstructed)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(original, cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(reconstructed, cmap='gray')\n",
    "axes[1].set_title(f'Reconstructed\\nPSNR: {psnr:.2f} dB, SSIM: {ssim:.4f}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(original - reconstructed, cmap='RdBu_r', vmin=-0.3, vmax=0.3)\n",
    "axes[2].set_title(f'Difference\\nENL ratio: {result[\"enl_ratio\"]:.3f}, EPI: {epi:.4f}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('Attention Autoencoder (Variant C) - SAR Reconstruction', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = Path(f\"runs/{RUN_NAME}/sar_evaluation.png\")\n",
    "plt.savefig(save_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved SAR evaluation to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Full Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def format_improvement(current, baseline):\n",
    "    diff = current - baseline\n",
    "    return f\"+{diff:.2f} dB\" if diff >= 0 else f\"{diff:.2f} dB\"\n",
    "\n",
    "# Prepare data\n",
    "models_data = [\n",
    "    {'name': 'Baseline', 'params': '2.3M', 'psnr': 20.47, 'ssim': 0.646, 'enl': '-'},\n",
    "    {'name': 'ResNet-Lite v2', 'params': '5.6M', 'psnr': 21.20, 'ssim': 0.726, 'enl': '0.851'},\n",
    "]\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    enl_val = np.mean(enl_ratios) if 'enl_ratios' in dir() and len(enl_ratios) > 0 else None\n",
    "    models_data.append({\n",
    "        'name': 'Attention v2 (Variant C)', \n",
    "        'params': f'{params[\"total\"]/1e6:.1f}M',\n",
    "        'psnr': final['val_psnr'],\n",
    "        'ssim': final['val_ssim'],\n",
    "        'enl': f'{enl_val:.3f}' if enl_val else 'N/A'\n",
    "    })\n",
    "\n",
    "print(f\"{'Model':<25} {'Params':>8}  {'PSNR':>10}  {'SSIM':>8}  {'ENL Ratio':>10}  {'vs Baseline':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for m in models_data:\n",
    "    improvement = format_improvement(m['psnr'], 20.47)\n",
    "    print(f\"{m['name']:<25} {m['params']:>8}  {m['psnr']:>10.2f}  {m['ssim']:>8.4f}  {str(m['enl']):>10}  {improvement:>12}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. CBAM Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of CBAM attention impact\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CBAM ATTENTION IMPACT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    \n",
    "    print(f\"\\nArchitecture:\")\n",
    "    print(f\"  Total parameters: {params['total']:,}\")\n",
    "    print(f\"  CBAM modules: {cbam_count}\")\n",
    "    print(f\"  CBAM overhead vs ResidualAutoencoder: ~0.7%\")\n",
    "    \n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Val PSNR: {final['val_psnr']:.2f} dB\")\n",
    "    print(f\"  Val SSIM: {final['val_ssim']:.4f}\")\n",
    "    print(f\"  ENL Ratio: {np.mean(enl_ratios):.3f}\")\n",
    "    print(f\"  EPI: {np.mean(epis):.4f}\")\n",
    "    \n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"  vs Baseline: {final['val_psnr'] - 20.47:+.2f} dB PSNR\")\n",
    "    print(f\"  vs ResNet-Lite: {final['val_psnr'] - 21.20:+.2f} dB PSNR\")\n",
    "    \n",
    "    # Conclusion based on results\n",
    "    print(f\"\\nConclusion:\")\n",
    "    if final['val_psnr'] > 21.70:  # More than 0.5 dB above ResNet-Lite\n",
    "        print(\"  [+] CBAM provides meaningful improvement over residual-only architecture\")\n",
    "        print(f\"  [+] +{final['val_psnr'] - 21.20:.2f} dB PSNR gain with 0.7% parameter overhead\")\n",
    "    else:\n",
    "        print(\"  [-] CBAM did NOT provide expected +0.5 dB improvement\")\n",
    "        print(\"  [-] Possible reasons:\")\n",
    "        print(\"      - 20% subset may not show attention benefits\")\n",
    "        print(\"      - CBAM may need longer training to converge\")\n",
    "        print(\"      - SAR data may not benefit from attention as much as natural images\")\n",
    "else:\n",
    "    print(\"No results available. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "**Compare in TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=D:/Projects/CNNAutoencoderProject/notebooks/runs\n",
    "```\n",
    "\n",
    "Select `baseline_c16_fast`, `resnet_lite_v2_c16`, and `attention_v1_c16` to compare metrics.\n",
    "\n",
    "**Next steps:**\n",
    "- Train Variant B (ResidualAutoencoder) for direct comparison\n",
    "- Consider full dataset training if subset results are promising\n",
    "- Proceed to Phase 5 (Full Inference Pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
