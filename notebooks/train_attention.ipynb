{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Autoencoder Training (Variant C)\n",
    "\n",
    "Training the AttentionAutoencoder (Pre-Activation Residual + CBAM) for SAR image compression at 16x compression ratio.\n",
    "\n",
    "**Variant C Architecture:**\n",
    "- Pre-activation residual blocks (BN->ReLU->Conv)\n",
    "- CBAM attention after every residual block (16 CBAM modules total)\n",
    "- 24M parameters (0.7% more than Variant B)\n",
    "- Target: +0.5 dB PSNR improvement over Variant B\n",
    "\n",
    "**Configuration:**\n",
    "- Loss: 0.7 MSE + 0.3 SSIM (emphasize pixel accuracy for PSNR)\n",
    "- Batch size: 16-24 (CBAM memory overhead)\n",
    "- Learning rate: 1e-4\n",
    "- Epochs: 30 or early stopping\n",
    "- Data: 20% subset for fair comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Project imports\n",
    "from src.data.datamodule import SARDataModule\n",
    "from src.models import AttentionAutoencoder  # Variant C with CBAM!\n",
    "from src.losses.combined import CombinedLoss\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Attention Autoencoder (Variant C)\n",
    "# ============================================================\n",
    "\n",
    "# Data settings\n",
    "DATA_PATH = \"D:/Projects/CNNAutoencoderProject/data/patches/metadata.npy\"\n",
    "BATCH_SIZE = 16       # REDUCED: CBAM memory overhead, batch=32 causes OOM\n",
    "NUM_WORKERS = 4       # Parallel data loading\n",
    "VAL_FRACTION = 0.1    # 10% validation split\n",
    "TRAIN_SUBSET = 0.20   # Use 20% of data for fair comparison\n",
    "\n",
    "# Model settings\n",
    "LATENT_CHANNELS = 16  # 16 = 16x compression\n",
    "BASE_CHANNELS = 64    # Full channels for Variant C\n",
    "\n",
    "# Loss settings - emphasize pixel accuracy for PSNR (per CONTEXT.md)\n",
    "MSE_WEIGHT = 0.7\n",
    "SSIM_WEIGHT = 0.3\n",
    "\n",
    "# Training settings\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4  # Same as Variant B for fair comparison\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "LR_PATIENCE = 5\n",
    "LR_FACTOR = 0.5\n",
    "MAX_GRAD_NORM = 1.0\n",
    "USE_AMP = True        # Mixed precision\n",
    "\n",
    "# Output settings\n",
    "RUN_NAME = f\"attention_v1_c{LATENT_CHANNELS}\"\n",
    "\n",
    "# Calculate compression ratio\n",
    "compression_ratio = (256 * 256) / (16 * 16 * LATENT_CHANNELS)\n",
    "print(f\"Model: AttentionAutoencoder (Variant C)\")\n",
    "print(f\"Base channels: {BASE_CHANNELS}\")\n",
    "print(f\"Compression ratio: {compression_ratio:.0f}x\")\n",
    "print(f\"Training subset: {TRAIN_SUBSET*100:.0f}% of data\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (reduced for CBAM memory)\")\n",
    "print(f\"Loss weights: {MSE_WEIGHT} MSE + {SSIM_WEIGHT} SSIM\")\n",
    "print(f\"Mixed Precision (AMP): {USE_AMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "dm = SARDataModule(\n",
    "    patches_path=DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    val_fraction=VAL_FRACTION,\n",
    ")\n",
    "\n",
    "# Apply subset to BOTH train and val for faster iteration\n",
    "if TRAIN_SUBSET < 1.0:\n",
    "    # Subset training data\n",
    "    full_train_size = len(dm.train_dataset)\n",
    "    train_subset_size = int(full_train_size * TRAIN_SUBSET)\n",
    "    train_indices = random.sample(range(full_train_size), train_subset_size)\n",
    "    dm.train_dataset = torch.utils.data.Subset(dm.train_dataset, train_indices)\n",
    "    \n",
    "    # Subset validation data (same proportion)\n",
    "    full_val_size = len(dm.val_dataset)\n",
    "    val_subset_size = int(full_val_size * TRAIN_SUBSET)\n",
    "    val_indices = random.sample(range(full_val_size), val_subset_size)\n",
    "    dm.val_dataset = torch.utils.data.Subset(dm.val_dataset, val_indices)\n",
    "    \n",
    "    print(f\"Using {TRAIN_SUBSET*100:.0f}% subset:\")\n",
    "    print(f\"  Train: {train_subset_size:,} of {full_train_size:,}\")\n",
    "    print(f\"  Val: {val_subset_size:,} of {full_val_size:,}\")\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Train patches: {len(dm.train_dataset):,}\")\n",
    "print(f\"  Val patches: {len(dm.val_dataset):,}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")\n",
    "print(f\"  Preprocessing params: {dm.preprocessing_params}\")\n",
    "\n",
    "# Estimate epoch time (batch_size=16 is slower than 32)\n",
    "est_batches = len(train_loader) + len(val_loader)\n",
    "est_time_min = est_batches / 1.0 / 60  # ~1.0 it/s with larger model + smaller batch\n",
    "print(f\"\\n  Estimated time per epoch: ~{est_time_min:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample batch dtype: {sample_batch.dtype}\")\n",
    "print(f\"Sample batch range: [{sample_batch.min():.4f}, {sample_batch.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating AttentionAutoencoder (Variant C)...\")\n",
    "model = AttentionAutoencoder(\n",
    "    latent_channels=LATENT_CHANNELS,\n",
    "    base_channels=BASE_CHANNELS,\n",
    ")\n",
    "\n",
    "params = model.count_parameters()\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Architecture: AttentionAutoencoder (Pre-Act Residual + CBAM)\")\n",
    "print(f\"  Total parameters: {params['total']:,}\")\n",
    "print(f\"  Encoder params: {params['encoder']:,}\")\n",
    "print(f\"  Decoder params: {params['decoder']:,}\")\n",
    "print(f\"  Compression ratio: {model.get_compression_ratio():.1f}x\")\n",
    "print(f\"  Latent size: {model.get_latent_size()}\")\n",
    "\n",
    "# Count CBAM modules\n",
    "cbam_count = sum(1 for m in model.modules() if m.__class__.__name__ == 'CBAM')\n",
    "print(f\"  CBAM modules: {cbam_count}\")\n",
    "\n",
    "# Compare to other models\n",
    "baseline_params = 2_257_809\n",
    "resnet_lite_params = 5_648_033\n",
    "print(f\"\\n  vs Baseline: {params['total'] / baseline_params:.1f}x more parameters\")\n",
    "print(f\"  vs ResNet-Lite: {params['total'] / resnet_lite_params:.1f}x more parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass and GPU memory\n",
    "print(\"Testing forward pass...\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_test = model.to(device)\n",
    "x_test = sample_batch[:2].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat, z = model_test(x_test)\n",
    "\n",
    "print(f\"  Input: {x_test.shape}\")\n",
    "print(f\"  Latent: {z.shape}\")\n",
    "print(f\"  Output: {x_hat.shape}\")\n",
    "print(f\"  Output range: [{x_hat.min():.4f}, {x_hat.max():.4f}]\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Clean up test\n",
    "del model_test, x_test, x_hat, z\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CombinedLoss(\n",
    "    mse_weight=MSE_WEIGHT,\n",
    "    ssim_weight=SSIM_WEIGHT,\n",
    ")\n",
    "\n",
    "print(f\"Loss function: CombinedLoss\")\n",
    "print(f\"  MSE weight: {MSE_WEIGHT} (emphasize pixel accuracy)\")\n",
    "print(f\"  SSIM weight: {SSIM_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'lr_patience': LR_PATIENCE,\n",
    "    'lr_factor': LR_FACTOR,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'run_name': RUN_NAME,\n",
    "    'preprocessing_params': dm.preprocessing_params,\n",
    "    'use_amp': USE_AMP,\n",
    "    # Store hyperparams for reproducibility\n",
    "    'model_type': 'AttentionAutoencoder-Variant-C',\n",
    "    'latent_channels': LATENT_CHANNELS,\n",
    "    'base_channels': BASE_CHANNELS,\n",
    "    'mse_weight': MSE_WEIGHT,\n",
    "    'ssim_weight': SSIM_WEIGHT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrainer ready:\")\n",
    "print(f\"  Log dir: {trainer.log_dir}\")\n",
    "print(f\"  Checkpoint dir: {trainer.checkpoint_dir}\")\n",
    "print(f\"  Device: {trainer.device}\")\n",
    "print(f\"  AMP enabled: {trainer.use_amp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "**Monitor with TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=D:/Projects/CNNAutoencoderProject/notebooks/runs\n",
    "```\n",
    "\n",
    "Compare `attention_v1_c16` with `baseline_c16_fast` and `resnet_lite_v2_c16` in TensorBoard.\n",
    "\n",
    "**Expected training time:** ~2 hours per epoch (7,833 train + 871 val batches at batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Starting Attention Autoencoder Training (Variant C)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: AttentionAutoencoder ({params['total']:,} params)\")\n",
    "print(f\"CBAM modules: {cbam_count}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nTensorBoard: tensorboard --logdir={trainer.log_dir.parent}\")\n",
    "print(f\"Checkpoints: {trainer.checkpoint_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = trainer.train(\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from checkpoint if training wasn't run in this session\n",
    "checkpoint_path = Path(f\"checkpoints/{RUN_NAME}/best.pth\")\n",
    "\n",
    "if 'history' not in dir() or history is None:\n",
    "    print(\"Training was not run in this session. Loading from checkpoint...\")\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "        \n",
    "        # Load model weights\n",
    "        model = AttentionAutoencoder(latent_channels=LATENT_CHANNELS, base_channels=BASE_CHANNELS)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"Loaded model from: {checkpoint_path}\")\n",
    "        \n",
    "        # Get history from checkpoint\n",
    "        history = checkpoint.get('history', None)\n",
    "        \n",
    "        if history:\n",
    "            print(f\"Loaded history with {len(history)} epochs\")\n",
    "        else:\n",
    "            print(\"No history in checkpoint, running quick validation...\")\n",
    "            # Run quick validation to get current metrics\n",
    "            model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            val_losses, val_psnrs, val_ssims = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(next(model.parameters()).device)\n",
    "                    output, _ = model(batch)\n",
    "                    loss, metrics = loss_fn(output, batch)\n",
    "                    val_losses.append(loss.item())\n",
    "                    val_psnrs.append(metrics['psnr'])\n",
    "                    val_ssims.append(metrics['ssim'])\n",
    "            \n",
    "            loaded_metrics = {\n",
    "                'val_loss': sum(val_losses) / len(val_losses),\n",
    "                'val_psnr': sum(val_psnrs) / len(val_psnrs),\n",
    "                'val_ssim': sum(val_ssims) / len(val_ssims),\n",
    "            }\n",
    "            print(f\"  Val Loss: {loaded_metrics['val_loss']:.4f}\")\n",
    "            print(f\"  Val PSNR: {loaded_metrics['val_psnr']:.2f} dB\")\n",
    "            print(f\"  Val SSIM: {loaded_metrics['val_ssim']:.4f}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at: {checkpoint_path}\")\n",
    "        print(\"Please run training first (Cell 10)\")\n",
    "else:\n",
    "    print(\"Training history available from this session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    print(f\"Model: AttentionAutoencoder (Variant C)\")\n",
    "    print(f\"Final epoch: {final['epoch'] + 1}\")\n",
    "    print(f\"Best val loss: {trainer.best_val_loss:.4f}\")\n",
    "    print(f\"Final val PSNR: {final['val_psnr']:.2f} dB\")\n",
    "    print(f\"Final val SSIM: {final['val_ssim']:.4f}\")\n",
    "    \n",
    "    # Compare to baseline and ResNet-Lite\n",
    "    baseline_psnr = 20.47\n",
    "    resnet_lite_psnr = 21.20\n",
    "    \n",
    "    improvement_baseline = final['val_psnr'] - baseline_psnr\n",
    "    improvement_resnet = final['val_psnr'] - resnet_lite_psnr\n",
    "    \n",
    "    print(f\"\\nImprovement over baseline: {improvement_baseline:+.2f} dB\")\n",
    "    print(f\"Improvement over ResNet-Lite: {improvement_resnet:+.2f} dB\")\n",
    "    \n",
    "    if final['val_psnr'] >= 22.0:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"[SUCCESS] PSNR >= 22.0 dB achieved!\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\n[INFO] PSNR {final['val_psnr']:.2f} dB (target: 22.0 dB)\")\n",
    "else:\n",
    "    print(\"[ERROR] No training history available.\")\n",
    "    print(\"Please run training (Cell 10) first.\")\n",
    "\n",
    "print(f\"\\nCheckpoint: checkpoints/{RUN_NAME}/best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Previous results\n",
    "baseline_results = {\n",
    "    'name': 'Baseline',\n",
    "    'params': 2_257_809,\n",
    "    'psnr': 20.47,\n",
    "    'ssim': 0.646,\n",
    "    'loss': 0.1813\n",
    "}\n",
    "\n",
    "resnet_lite_results = {\n",
    "    'name': 'ResNet-Lite v2',\n",
    "    'params': 5_648_033,\n",
    "    'psnr': 21.20,\n",
    "    'ssim': 0.726,\n",
    "    'loss': 0.1410\n",
    "}\n",
    "\n",
    "# Variant C results\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    attention_results = {\n",
    "        'name': 'Attention v1 (C)',\n",
    "        'params': params['total'],\n",
    "        'psnr': final['val_psnr'],\n",
    "        'ssim': final['val_ssim'],\n",
    "        'loss': final['val_loss']\n",
    "    }\n",
    "    \n",
    "    all_models = [baseline_results, resnet_lite_results, attention_results]\n",
    "    \n",
    "    print(f\"{'Model':<20} {'Params':<12} {'PSNR (dB)':<12} {'SSIM':<12} {'vs Baseline':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for m in all_models:\n",
    "        improvement = m['psnr'] - baseline_results['psnr']\n",
    "        print(f\"{m['name']:<20} {m['params']:>10,} {m['psnr']:>10.2f} {m['ssim']:>10.4f} {improvement:>+10.2f}\")\n",
    "else:\n",
    "    print(\"No Attention model results available. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if history:\n",
    "    epochs_list = [h['epoch'] + 1 for h in history]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs_list, [h['train_loss'] for h in history], label='Train')\n",
    "    ax.plot(epochs_list, [h['val_loss'] for h in history], label='Val')\n",
    "    ax.axhline(y=baseline_results['loss'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax.axhline(y=resnet_lite_results['loss'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss (Attention v1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # PSNR\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs_list, [h['train_psnr'] for h in history], label='Train')\n",
    "    ax.plot(epochs_list, [h['val_psnr'] for h in history], label='Val')\n",
    "    ax.axhline(y=22.0, color='r', linestyle='--', label='Target (22 dB)')\n",
    "    ax.axhline(y=baseline_results['psnr'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax.axhline(y=resnet_lite_results['psnr'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('PSNR (dB)')\n",
    "    ax.set_title('PSNR (Attention v1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # SSIM\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(epochs_list, [h['train_ssim'] for h in history], label='Train')\n",
    "    ax.plot(epochs_list, [h['val_ssim'] for h in history], label='Val')\n",
    "    ax.axhline(y=baseline_results['ssim'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax.axhline(y=resnet_lite_results['ssim'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_title('SSIM (Attention v1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(epochs_list, [h['learning_rate'] for h in history])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('Learning Rate Schedule')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.suptitle('Attention Autoencoder (Variant C) Training', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to log_dir\n",
    "    save_path = Path(f\"runs/{RUN_NAME}/training_curves.png\")\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved training curves to: {save_path}\")\n",
    "else:\n",
    "    print(\"Training curves not available (model loaded from checkpoint).\")\n",
    "    print(f\"\\nCheck TensorBoard for historical curves:\")\n",
    "    print(f\"  tensorboard --logdir=runs/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample reconstructions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model from checkpoint if needed\n",
    "checkpoint_path = Path(f\"checkpoints/{RUN_NAME}/best.pth\")\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    model = AttentionAutoencoder(latent_channels=LATENT_CHANNELS, base_channels=BASE_CHANNELS)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model from: {checkpoint_path}\")\n",
    "\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Get a batch from validation\n",
    "val_batch = next(iter(val_loader))[:4].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, latent = model(val_batch)\n",
    "\n",
    "# Move to CPU for plotting\n",
    "originals = val_batch.cpu().numpy()\n",
    "reconstructions = reconstructed.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(4):\n",
    "    # Original\n",
    "    axes[0, i].imshow(originals[i, 0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Original {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstruction\n",
    "    axes[1, i].imshow(reconstructions[i, 0], cmap='gray')\n",
    "    axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Difference\n",
    "    diff = abs(originals[i, 0] - reconstructions[i, 0])\n",
    "    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[2, i].set_title(f'Difference {i+1}')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Difference', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'Attention v1 Reconstructions (Compression: {model.get_compression_ratio():.0f}x, Params: {params[\"total\"]:,})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "save_path = Path(f\"runs/{RUN_NAME}/sample_reconstructions.png\")\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(save_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved reconstructions to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SAR-Specific Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SAR-specific metrics (ENL ratio, EPI)\n",
    "from src.evaluation.metrics import enl_ratio, edge_preservation_index\n",
    "\n",
    "# Load real SAR patches\n",
    "patches_file = list(Path(f\"{project_root}/data/patches\").glob(\"*.npy\"))\n",
    "patches_file = [f for f in patches_file if 'metadata' not in f.name][0]\n",
    "patches = np.load(patches_file, mmap_mode='r')\n",
    "\n",
    "print(f\"Loaded patches from: {patches_file.name}\")\n",
    "print(f\"Shape: {patches.shape}\")\n",
    "\n",
    "# Evaluate on multiple patches\n",
    "n_eval = 10\n",
    "enl_ratios = []\n",
    "epis = []\n",
    "\n",
    "model.eval()\n",
    "for i in range(n_eval):\n",
    "    original = patches[i].copy()\n",
    "    \n",
    "    # Reconstruct\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.from_numpy(original).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "        recon, _ = model(input_tensor)\n",
    "    reconstructed = recon[0, 0].cpu().numpy()\n",
    "    \n",
    "    # ENL ratio\n",
    "    result = enl_ratio(original, reconstructed)\n",
    "    enl_ratios.append(result['enl_ratio'])\n",
    "    \n",
    "    # EPI\n",
    "    epi = edge_preservation_index(original, reconstructed)\n",
    "    epis.append(epi)\n",
    "\n",
    "print(f\"\\nSAR-Specific Metrics ({n_eval} samples):\")\n",
    "print(f\"  ENL Ratio: {np.mean(enl_ratios):.3f} +/- {np.std(enl_ratios):.3f}\")\n",
    "print(f\"  EPI: {np.mean(epis):.4f} +/- {np.std(epis):.4f}\")\n",
    "print(f\"\\n  ENL ratio target: [0.7, 1.3]\")\n",
    "print(f\"  ENL ratio in range: {0.7 <= np.mean(enl_ratios) <= 1.3}\")\n",
    "print(f\"  EPI target: > 0.8\")\n",
    "print(f\"  EPI target met: {np.mean(epis) > 0.8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison with a single patch\n",
    "original = patches[0].copy()\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.from_numpy(original).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    recon, _ = model(input_tensor)\n",
    "reconstructed = recon[0, 0].cpu().numpy()\n",
    "\n",
    "# Compute metrics for this patch\n",
    "from src.evaluation.metrics import SARMetrics\n",
    "psnr = SARMetrics.psnr(original.flatten(), reconstructed.flatten())\n",
    "ssim = SARMetrics.ssim(original.flatten(), reconstructed.flatten())\n",
    "result = enl_ratio(original, reconstructed)\n",
    "epi = edge_preservation_index(original, reconstructed)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(original, cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(reconstructed, cmap='gray')\n",
    "axes[1].set_title(f'Reconstructed\\nPSNR: {psnr:.2f} dB, SSIM: {ssim:.4f}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(original - reconstructed, cmap='RdBu_r', vmin=-0.3, vmax=0.3)\n",
    "axes[2].set_title(f'Difference\\nENL ratio: {result[\"enl_ratio\"]:.3f}, EPI: {epi:.4f}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('Attention Autoencoder (Variant C) - SAR Reconstruction', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = Path(f\"runs/{RUN_NAME}/sar_evaluation.png\")\n",
    "plt.savefig(save_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved SAR evaluation to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Full Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare data\n",
    "models_data = [\n",
    "    {'name': 'Baseline', 'params': '2.3M', 'psnr': 20.47, 'ssim': 0.646, 'enl': '-'},\n",
    "    {'name': 'ResNet-Lite v2', 'params': '5.6M', 'psnr': 21.20, 'ssim': 0.726, 'enl': '0.851'},\n",
    "]\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    models_data.append({\n",
    "        'name': 'Attention v1 (Variant C)', \n",
    "        'params': f'{params[\"total\"]/1e6:.1f}M',\n",
    "        'psnr': final['val_psnr'],\n",
    "        'ssim': final['val_ssim'],\n",
    "        'enl': f'{np.mean(enl_ratios):.3f}'\n",
    "    })\n",
    "\n",
    "print(f\"{'Model':<25} {'Params':<10} {'PSNR (dB)':<12} {'SSIM':<10} {'ENL Ratio':<12} {'vs Baseline':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for m in models_data:\n",
    "    improvement = m['psnr'] - 20.47  # vs baseline\n",
    "    print(f\"{m['name']:<25} {m['params']:<10} {m['psnr']:>10.2f} {m['ssim']:>10.4f} {str(m['enl']):>10} {improvement:>+10.2f}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. CBAM Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of CBAM attention impact\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CBAM ATTENTION IMPACT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    \n",
    "    print(f\"\\nArchitecture:\")\n",
    "    print(f\"  Total parameters: {params['total']:,}\")\n",
    "    print(f\"  CBAM modules: {cbam_count}\")\n",
    "    print(f\"  CBAM overhead vs ResidualAutoencoder: ~0.7%\")\n",
    "    \n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Val PSNR: {final['val_psnr']:.2f} dB\")\n",
    "    print(f\"  Val SSIM: {final['val_ssim']:.4f}\")\n",
    "    print(f\"  ENL Ratio: {np.mean(enl_ratios):.3f}\")\n",
    "    print(f\"  EPI: {np.mean(epis):.4f}\")\n",
    "    \n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"  vs Baseline: {final['val_psnr'] - 20.47:+.2f} dB PSNR\")\n",
    "    print(f\"  vs ResNet-Lite: {final['val_psnr'] - 21.20:+.2f} dB PSNR\")\n",
    "    \n",
    "    # Conclusion based on results\n",
    "    print(f\"\\nConclusion:\")\n",
    "    if final['val_psnr'] > 21.70:  # More than 0.5 dB above ResNet-Lite\n",
    "        print(\"  [+] CBAM provides meaningful improvement over residual-only architecture\")\n",
    "        print(f\"  [+] +{final['val_psnr'] - 21.20:.2f} dB PSNR gain with 0.7% parameter overhead\")\n",
    "    else:\n",
    "        print(\"  [-] CBAM did NOT provide expected +0.5 dB improvement\")\n",
    "        print(\"  [-] Possible reasons:\")\n",
    "        print(\"      - 20% subset may not show attention benefits\")\n",
    "        print(\"      - CBAM may need longer training to converge\")\n",
    "        print(\"      - SAR data may not benefit from attention as much as natural images\")\n",
    "else:\n",
    "    print(\"No results available. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "**Compare in TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=D:/Projects/CNNAutoencoderProject/notebooks/runs\n",
    "```\n",
    "\n",
    "Select `baseline_c16_fast`, `resnet_lite_v2_c16`, and `attention_v1_c16` to compare metrics.\n",
    "\n",
    "**Next steps:**\n",
    "- Train Variant B (ResidualAutoencoder) for direct comparison\n",
    "- Consider full dataset training if subset results are promising\n",
    "- Proceed to Phase 5 (Full Inference Pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
