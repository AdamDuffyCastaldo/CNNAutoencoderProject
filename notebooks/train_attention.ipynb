{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Autoencoder Training (Variant C)\n",
    "\n",
    "Training the AttentionAutoencoder (Pre-Activation Residual + CBAM) for SAR image compression at 16x compression ratio.\n",
    "\n",
    "**Variant C Architecture:**\n",
    "- Pre-activation residual blocks (BN->ReLU->Conv)\n",
    "- CBAM attention after every residual block (16 CBAM modules total)\n",
    "- 24M parameters (0.7% more than Variant B)\n",
    "- Target: +0.5 dB PSNR improvement over Variant B\n",
    "\n",
    "**Configuration (v2 - with stability improvements):**\n",
    "- Loss: 0.7 MSE + 0.3 SSIM (emphasize pixel accuracy for PSNR)\n",
    "- Learning rate: 5e-5 with 3-epoch linear warmup\n",
    "- Optimizer: AdamW with weight_decay=1e-5\n",
    "- Gradient clipping: 0.5\n",
    "- Batch size: 16 (CBAM memory overhead)\n",
    "- Epochs: 30 or early stopping\n",
    "- Data: 20% subset for fair comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Projects\\CNNAutoencoderProject\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "if project_root.name == 'notebooks':\n",
    "    project_root = project_root.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "VRAM: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Project imports\n",
    "from src.data.datamodule import SARDataModule\n",
    "from src.models import AttentionAutoencoder  # Variant C with CBAM!\n",
    "from src.losses.combined import CombinedLoss\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONFIGURATION - Attention Autoencoder (Variant C)\n# ============================================================\n\n# =========================\n# TRAINING MODE\n# =========================\nQUICK_SEARCH = True  # True = 10% data, 20 epochs (hyperparameter search)\n                      # False = 20% data, 30 epochs (full training)\n\n# Data settings\nDATA_PATH = \"D:/Projects/CNNAutoencoderProject/data/patches/metadata.npy\"\nBATCH_SIZE = 16       # REDUCED: CBAM memory overhead, batch=32 causes OOM\nNUM_WORKERS = 0       # Set to 0 for Windows compatibility\nVAL_FRACTION = 0.1    # 10% validation split\nTRAIN_SUBSET = 0.05 if QUICK_SEARCH else 0.20\n\n# Model settings\nLATENT_CHANNELS = 16  # 16 = 16x compression\nBASE_CHANNELS = 48    # Full channels for Variant C\n\n# Loss settings - emphasize pixel accuracy for PSNR (per CONTEXT.md)\nMSE_WEIGHT = 0.7\nSSIM_WEIGHT = 0.3\n\n# Training settings - with stability improvements\nEPOCHS = 20 if QUICK_SEARCH else 30\nLEARNING_RATE = 5e-5      # With warmup for stability\nWARMUP_EPOCHS = 3         # Linear warmup\nOPTIMIZER = 'adamw'       # AdamW handles weight decay better\nWEIGHT_DECAY = 1e-5       # Regularization\nEARLY_STOPPING_PATIENCE = 10\nLR_PATIENCE = 5\nLR_FACTOR = 0.5\nMAX_GRAD_NORM = 0.5       # More aggressive clipping\nUSE_AMP = True            # Mixed precision\n\n# Output settings - Standard naming: {model}_c{latent}_b{base}_cr{ratio}x\n# Trainer auto-appends timestamp for uniqueness\ncompression_ratio = (256 * 256) / (16 * 16 * LATENT_CHANNELS)\nmode_suffix = \"quick\" if QUICK_SEARCH else \"full\"\nRUN_NAME = f\"attention_{mode_suffix}_c{LATENT_CHANNELS}_b{BASE_CHANNELS}_cr{int(compression_ratio)}x\"\n\n# Results folder - clean model name for external users\nRESULTS_DIR = Path('results')\nRESULTS_DIR.mkdir(exist_ok=True)\nMODEL_RESULTS_DIR = RESULTS_DIR / f\"attention_c{LATENT_CHANNELS}_b{BASE_CHANNELS}_cr{int(compression_ratio)}x\"\nMODEL_RESULTS_DIR.mkdir(exist_ok=True)\n\nprint(f\"{'='*60}\")\nprint(f\"MODE: {'QUICK SEARCH (5% data, 20 epochs)' if QUICK_SEARCH else 'FULL TRAINING (20% data, 30 epochs)'}\")\nprint(f\"{'='*60}\")\nprint(f\"Model: AttentionAutoencoder (Variant C)\")\nprint(f\"Base channels: {BASE_CHANNELS}\")\nprint(f\"Compression ratio: {compression_ratio:.0f}x\")\nprint(f\"Training subset: {TRAIN_SUBSET*100:.0f}% of data\")\nprint(f\"Epochs: {EPOCHS}\")\nprint(f\"Batch size: {BATCH_SIZE} (reduced for CBAM memory)\")\nprint(f\"Loss weights: {MSE_WEIGHT} MSE + {SSIM_WEIGHT} SSIM\")\nprint(f\"Learning rate: {LEARNING_RATE} with {WARMUP_EPOCHS} epoch warmup\")\nprint(f\"Optimizer: {OPTIMIZER.upper()} (weight_decay={WEIGHT_DECAY})\")\nprint(f\"Gradient clipping: {MAX_GRAD_NORM}\")\nprint(f\"Run name: {RUN_NAME}\")\nprint(f\"Results dir: {MODEL_RESULTS_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading metadata from D:\\Projects\\CNNAutoencoderProject\\data\\patches\\metadata.npy\n",
      "Total patches: 696277\n",
      "Train: 626650, Val: 69627\n",
      "Using 5% subset:\n",
      "  Train: 31,332 of 626,650\n",
      "  Val: 3,481 of 69,627\n",
      "\n",
      "Dataset loaded:\n",
      "  Train patches: 31,332\n",
      "  Val patches: 3,481\n",
      "  Train batches: 1,958\n",
      "  Val batches: 218\n",
      "  Preprocessing params: {'vmin': np.float32(14.768799), 'vmax': np.float32(24.54073)}\n",
      "\n",
      "  Estimated time per epoch: ~36 minutes\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "dm = SARDataModule(\n",
    "    patches_path=DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    val_fraction=VAL_FRACTION,\n",
    ")\n",
    "\n",
    "# Apply subset to BOTH train and val for faster iteration\n",
    "if TRAIN_SUBSET < 1.0:\n",
    "    # Subset training data\n",
    "    full_train_size = len(dm.train_dataset)\n",
    "    train_subset_size = int(full_train_size * TRAIN_SUBSET)\n",
    "    train_indices = random.sample(range(full_train_size), train_subset_size)\n",
    "    dm.train_dataset = torch.utils.data.Subset(dm.train_dataset, train_indices)\n",
    "    \n",
    "    # Subset validation data (same proportion)\n",
    "    full_val_size = len(dm.val_dataset)\n",
    "    val_subset_size = int(full_val_size * TRAIN_SUBSET)\n",
    "    val_indices = random.sample(range(full_val_size), val_subset_size)\n",
    "    dm.val_dataset = torch.utils.data.Subset(dm.val_dataset, val_indices)\n",
    "    \n",
    "    print(f\"Using {TRAIN_SUBSET*100:.0f}% subset:\")\n",
    "    print(f\"  Train: {train_subset_size:,} of {full_train_size:,}\")\n",
    "    print(f\"  Val: {val_subset_size:,} of {full_val_size:,}\")\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Train patches: {len(dm.train_dataset):,}\")\n",
    "print(f\"  Val patches: {len(dm.val_dataset):,}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")\n",
    "print(f\"  Preprocessing params: {dm.preprocessing_params}\")\n",
    "\n",
    "# Estimate epoch time (batch_size=16 is slower than 32)\n",
    "est_batches = len(train_loader) + len(val_loader)\n",
    "est_time_min = est_batches / 1.0 / 60  # ~1.0 it/s with larger model + smaller batch\n",
    "print(f\"\\n  Estimated time per epoch: ~{est_time_min:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([16, 1, 256, 256])\n",
      "Sample batch dtype: torch.float32\n",
      "Sample batch range: [0.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "# Verify a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample batch dtype: {sample_batch.dtype}\")\n",
    "print(f\"Sample batch range: [{sample_batch.min():.4f}, {sample_batch.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating AttentionAutoencoder (Variant C)...\n",
      "\n",
      "Model created:\n",
      "  Architecture: AttentionAutoencoder (Pre-Act Residual + CBAM)\n",
      "  Total parameters: 13,529,761\n",
      "  Encoder params: 6,318,752\n",
      "  Decoder params: 7,211,009\n",
      "  Compression ratio: 16.0x\n",
      "  Latent size: (16, 16, 16)\n",
      "  CBAM modules: 16\n",
      "\n",
      "  vs Baseline: 6.0x more parameters\n",
      "  vs ResNet-Lite: 2.4x more parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating AttentionAutoencoder (Variant C)...\")\n",
    "model = AttentionAutoencoder(\n",
    "    latent_channels=LATENT_CHANNELS,\n",
    "    base_channels=BASE_CHANNELS,\n",
    ")\n",
    "\n",
    "params = model.count_parameters()\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Architecture: AttentionAutoencoder (Pre-Act Residual + CBAM)\")\n",
    "print(f\"  Total parameters: {params['total']:,}\")\n",
    "print(f\"  Encoder params: {params['encoder']:,}\")\n",
    "print(f\"  Decoder params: {params['decoder']:,}\")\n",
    "print(f\"  Compression ratio: {model.get_compression_ratio():.1f}x\")\n",
    "print(f\"  Latent size: {model.get_latent_size()}\")\n",
    "\n",
    "# Count CBAM modules\n",
    "cbam_count = sum(1 for m in model.modules() if m.__class__.__name__ == 'CBAM')\n",
    "print(f\"  CBAM modules: {cbam_count}\")\n",
    "\n",
    "# Compare to other models\n",
    "baseline_params = 2_257_809\n",
    "resnet_lite_params = 5_648_033\n",
    "print(f\"\\n  vs Baseline: {params['total'] / baseline_params:.1f}x more parameters\")\n",
    "print(f\"  vs ResNet-Lite: {params['total'] / resnet_lite_params:.1f}x more parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward pass...\n",
      "  Input: torch.Size([2, 1, 256, 256])\n",
      "  Latent: torch.Size([2, 16, 16, 16])\n",
      "  Output: torch.Size([2, 1, 256, 256])\n",
      "  Output range: [0.0000, 1.0000]\n",
      "  GPU memory allocated: 0.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass and GPU memory\n",
    "print(\"Testing forward pass...\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_test = model.to(device)\n",
    "x_test = sample_batch[:2].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat, z = model_test(x_test)\n",
    "\n",
    "print(f\"  Input: {x_test.shape}\")\n",
    "print(f\"  Latent: {z.shape}\")\n",
    "print(f\"  Output: {x_hat.shape}\")\n",
    "print(f\"  Output range: [{x_hat.min():.4f}, {x_hat.max():.4f}]\")\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Clean up test\n",
    "del model_test, x_test, x_hat, z\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: CombinedLoss\n",
      "  MSE weight: 0.7 (emphasize pixel accuracy)\n",
      "  SSIM weight: 0.3\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CombinedLoss(\n",
    "    mse_weight=MSE_WEIGHT,\n",
    "    ssim_weight=SSIM_WEIGHT,\n",
    ")\n",
    "\n",
    "print(f\"Loss function: CombinedLoss\")\n",
    "print(f\"  MSE weight: {MSE_WEIGHT} (emphasize pixel accuracy)\")\n",
    "print(f\"  SSIM weight: {SSIM_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2026-01-26 16:17:38,290 - Log directory: runs\\attention_v2_quick_c16\n",
      "2026-01-26 16:17:38,290 - Checkpoint directory: checkpoints\\attention_v2_quick_c16\n",
      "2026-01-26 16:17:38,291 - Mixed Precision (AMP): enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing trainer...\n",
      "Using device: cuda\n",
      "GPU memory: 2456MB / 8192MB (30% used, 5.6 GB free)\n",
      "Using AdamW optimizer with weight_decay=1e-05\n",
      "Learning rate warmup: 3 epochs (5.00e-06 -> 5.00e-05)\n",
      "Mixed Precision (AMP) enabled - ~2x training speedup\n",
      "\n",
      "Trainer ready:\n",
      "  Log dir: runs\\attention_v2_quick_c16\n",
      "  Checkpoint dir: checkpoints\\attention_v2_quick_c16\n",
      "  Device: cuda\n",
      "  AMP enabled: True\n",
      "  Warmup epochs: 3\n",
      "  Base LR: 5e-05\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'warmup_epochs': WARMUP_EPOCHS,      # Linear warmup\n",
    "    'optimizer': OPTIMIZER,               # 'adam' or 'adamw'\n",
    "    'weight_decay': WEIGHT_DECAY,         # For AdamW\n",
    "    'lr_patience': LR_PATIENCE,\n",
    "    'lr_factor': LR_FACTOR,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'run_name': RUN_NAME,\n",
    "    'preprocessing_params': dm.preprocessing_params,\n",
    "    'use_amp': USE_AMP,\n",
    "    # Store hyperparams for reproducibility\n",
    "    'model_type': 'AttentionAutoencoder-Variant-C',\n",
    "    'latent_channels': LATENT_CHANNELS,\n",
    "    'base_channels': BASE_CHANNELS,\n",
    "    'mse_weight': MSE_WEIGHT,\n",
    "    'ssim_weight': SSIM_WEIGHT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrainer ready:\")\n",
    "print(f\"  Log dir: {trainer.log_dir}\")\n",
    "print(f\"  Checkpoint dir: {trainer.checkpoint_dir}\")\n",
    "print(f\"  Device: {trainer.device}\")\n",
    "print(f\"  AMP enabled: {trainer.use_amp}\")\n",
    "print(f\"  Warmup epochs: {trainer.warmup_epochs}\")\n",
    "print(f\"  Base LR: {trainer.base_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "**Monitor with TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=D:/Projects/CNNAutoencoderProject/notebooks/runs\n",
    "```\n",
    "\n",
    "Compare `attention_v1_c16` with `baseline_c16_fast` and `resnet_lite_v2_c16` in TensorBoard.\n",
    "\n",
    "**Expected training time:** ~2 hours per epoch (7,833 train + 871 val batches at batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Starting Attention Autoencoder Training (Variant C)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: AttentionAutoencoder ({params['total']:,} params)\")\n",
    "print(f\"CBAM modules: {cbam_count}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nTensorBoard: tensorboard --logdir={trainer.log_dir.parent}\")\n",
    "print(f\"Checkpoints: {trainer.checkpoint_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = trainer.train(\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load results from checkpoint if training wasn't run in this session\ncheckpoint_path = trainer.checkpoint_dir / 'best.pth'\n\nif 'history' not in dir() or history is None:\n    print(\"Training was not run in this session. Loading from checkpoint...\")\n    \n    if checkpoint_path.exists():\n        checkpoint = torch.load(checkpoint_path, weights_only=False)\n        \n        # Load model weights\n        model = AttentionAutoencoder(latent_channels=LATENT_CHANNELS, base_channels=BASE_CHANNELS)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model.eval()\n        print(f\"Loaded model from: {checkpoint_path}\")\n        \n        # Get history from checkpoint\n        history = checkpoint.get('history', None)\n        \n        if history:\n            print(f\"Loaded history with {len(history)} epochs\")\n        else:\n            print(\"No history in checkpoint, running quick validation...\")\n            # Run quick validation to get current metrics\n            model.to('cuda' if torch.cuda.is_available() else 'cpu')\n            \n            val_losses, val_psnrs, val_ssims = [], [], []\n            with torch.no_grad():\n                for batch in val_loader:\n                    batch = batch.to(next(model.parameters()).device)\n                    output, _ = model(batch)\n                    loss, metrics = loss_fn(output, batch)\n                    val_losses.append(loss.item())\n                    val_psnrs.append(metrics['psnr'])\n                    val_ssims.append(metrics['ssim'])\n            \n            loaded_metrics = {\n                'val_loss': sum(val_losses) / len(val_losses),\n                'val_psnr': sum(val_psnrs) / len(val_psnrs),\n                'val_ssim': sum(val_ssims) / len(val_ssims),\n            }\n            print(f\"  Val Loss: {loaded_metrics['val_loss']:.4f}\")\n            print(f\"  Val PSNR: {loaded_metrics['val_psnr']:.2f} dB\")\n            print(f\"  Val SSIM: {loaded_metrics['val_ssim']:.4f}\")\n    else:\n        print(f\"No checkpoint found at: {checkpoint_path}\")\n        print(\"Please run training first\")\nelse:\n    print(\"Training history available from this session.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Training Summary\")\nprint(\"=\" * 60)\n\nif history:\n    final = history[-1]\n    print(f\"Model: AttentionAutoencoder (Variant C)\")\n    print(f\"Final epoch: {final['epoch'] + 1}\")\n    print(f\"Best val loss: {trainer.best_val_loss:.4f}\")\n    print(f\"Final val PSNR: {final['val_psnr']:.2f} dB\")\n    print(f\"Final val SSIM: {final['val_ssim']:.4f}\")\n    \n    # Compare to baseline and ResNet-Lite\n    baseline_psnr = 20.47\n    resnet_lite_psnr = 21.20\n    \n    improvement_baseline = final['val_psnr'] - baseline_psnr\n    improvement_resnet = final['val_psnr'] - resnet_lite_psnr\n    \n    print(f\"\\nImprovement over baseline: {improvement_baseline:+.2f} dB\")\n    print(f\"Improvement over ResNet-Lite: {improvement_resnet:+.2f} dB\")\n    \n    if final['val_psnr'] >= 22.0:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[SUCCESS] PSNR >= 22.0 dB achieved!\")\n        print(\"=\" * 60)\n    else:\n        print(f\"\\n[INFO] PSNR {final['val_psnr']:.2f} dB (target: 22.0 dB)\")\nelse:\n    print(\"[ERROR] No training history available.\")\n    print(\"Please run training first.\")\n\nprint(f\"\\nCheckpoint: {trainer.checkpoint_dir / 'best.pth'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Model Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def format_improvement(current, baseline):\n",
    "    diff = current - baseline\n",
    "    return f\"+{diff:.2f}\" if diff >= 0 else f\"{diff:.2f}\"\n",
    "\n",
    "# Previous results\n",
    "baseline_results = {\n",
    "    'name': 'Baseline',\n",
    "    'params': 2_257_809,\n",
    "    'psnr': 20.47,\n",
    "    'ssim': 0.646,\n",
    "    'loss': 0.1813\n",
    "}\n",
    "\n",
    "resnet_lite_results = {\n",
    "    'name': 'ResNet-Lite v2',\n",
    "    'params': 5_648_033,\n",
    "    'psnr': 21.20,\n",
    "    'ssim': 0.726,\n",
    "    'loss': 0.1410\n",
    "}\n",
    "\n",
    "# Variant C results\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    attention_results = {\n",
    "        'name': 'Attention v2 (C)',\n",
    "        'params': params['total'],\n",
    "        'psnr': final['val_psnr'],\n",
    "        'ssim': final['val_ssim'],\n",
    "        'loss': final['val_loss']\n",
    "    }\n",
    "    \n",
    "    all_models = [baseline_results, resnet_lite_results, attention_results]\n",
    "    \n",
    "    print(f\"{'Model':<20} {'Params':>12}  {'PSNR (dB)':>10}  {'SSIM':>8}  {'vs Baseline':>12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for m in all_models:\n",
    "        params_str = f\"{m['params']:,}\"\n",
    "        improvement = format_improvement(m['psnr'], baseline_results['psnr'])\n",
    "        print(f\"{m['name']:<20} {params_str:>12}  {m['psnr']:>10.2f}  {m['ssim']:>8.4f}  {improvement:>12}\")\n",
    "else:\n",
    "    print(\"No Attention model results available. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nif history:\n    epochs_list = [h['epoch'] + 1 for h in history]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # Loss\n    ax = axes[0, 0]\n    ax.plot(epochs_list, [h['train_loss'] for h in history], label='Train')\n    ax.plot(epochs_list, [h['val_loss'] for h in history], label='Val')\n    ax.axhline(y=baseline_results['loss'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n    ax.axhline(y=resnet_lite_results['loss'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.set_title('Training Loss (Attention)')\n    ax.legend()\n    ax.grid(True)\n    \n    # PSNR\n    ax = axes[0, 1]\n    ax.plot(epochs_list, [h['train_psnr'] for h in history], label='Train')\n    ax.plot(epochs_list, [h['val_psnr'] for h in history], label='Val')\n    ax.axhline(y=22.0, color='r', linestyle='--', label='Target (22 dB)')\n    ax.axhline(y=baseline_results['psnr'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n    ax.axhline(y=resnet_lite_results['psnr'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('PSNR (dB)')\n    ax.set_title('PSNR (Attention)')\n    ax.legend()\n    ax.grid(True)\n    \n    # SSIM\n    ax = axes[1, 0]\n    ax.plot(epochs_list, [h['train_ssim'] for h in history], label='Train')\n    ax.plot(epochs_list, [h['val_ssim'] for h in history], label='Val')\n    ax.axhline(y=baseline_results['ssim'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n    ax.axhline(y=resnet_lite_results['ssim'], color='orange', linestyle='--', alpha=0.5, label='ResNet-Lite')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('SSIM')\n    ax.set_title('SSIM (Attention)')\n    ax.legend()\n    ax.grid(True)\n    \n    # Learning Rate\n    ax = axes[1, 1]\n    ax.plot(epochs_list, [h['learning_rate'] for h in history])\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Learning Rate')\n    ax.set_title('Learning Rate Schedule')\n    ax.set_yscale('log')\n    ax.grid(True)\n    \n    plt.suptitle('Attention Autoencoder (Variant C) Training', fontsize=14)\n    plt.tight_layout()\n    \n    # Save to results folder\n    save_path = MODEL_RESULTS_DIR / 'training_curves.png'\n    plt.savefig(save_path, dpi=150)\n    plt.savefig(trainer.log_dir / 'training_curves.png', dpi=150)\n    plt.show()\n    \n    print(f\"\\nSaved training curves to: {save_path}\")\nelse:\n    print(\"Training curves not available (model loaded from checkpoint).\")\n    print(f\"\\nCheck TensorBoard for historical curves:\")\n    print(f\"  tensorboard --logdir=runs/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize some sample reconstructions\nimport matplotlib.pyplot as plt\n\n# Load model from checkpoint if needed\ncheckpoint_path = trainer.checkpoint_dir / 'best.pth'\nif checkpoint_path.exists():\n    checkpoint = torch.load(checkpoint_path, weights_only=False)\n    model = AttentionAutoencoder(latent_channels=LATENT_CHANNELS, base_channels=BASE_CHANNELS)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"Loaded model from: {checkpoint_path}\")\n\nmodel.eval()\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n\n# Get a batch from validation\nval_batch = next(iter(val_loader))[:4].to(device)\n\nwith torch.no_grad():\n    reconstructed, latent = model(val_batch)\n\n# Move to CPU for plotting\noriginals = val_batch.cpu().numpy()\nreconstructions = reconstructed.cpu().numpy()\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\nfor i in range(4):\n    # Original\n    axes[0, i].imshow(originals[i, 0], cmap='gray')\n    axes[0, i].set_title(f'Original {i+1}')\n    axes[0, i].axis('off')\n    \n    # Reconstruction\n    axes[1, i].imshow(reconstructions[i, 0], cmap='gray')\n    axes[1, i].set_title(f'Reconstructed {i+1}')\n    axes[1, i].axis('off')\n    \n    # Difference\n    diff = abs(originals[i, 0] - reconstructions[i, 0])\n    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n    axes[2, i].set_title(f'Difference {i+1}')\n    axes[2, i].axis('off')\n\naxes[0, 0].set_ylabel('Original', fontsize=12)\naxes[1, 0].set_ylabel('Reconstructed', fontsize=12)\naxes[2, 0].set_ylabel('Difference', fontsize=12)\n\nplt.suptitle(f'Attention Reconstructions (Compression: {model.get_compression_ratio():.0f}x, Params: {params[\"total\"]:,})', fontsize=14)\nplt.tight_layout()\n\n# Save to results folder\nsave_path = MODEL_RESULTS_DIR / 'sample_reconstructions.png'\nplt.savefig(save_path, dpi=150)\nplt.savefig(trainer.log_dir / 'sample_reconstructions.png', dpi=150)\nplt.show()\n\nprint(f\"Saved reconstructions to: {save_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SAR-Specific Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.metrics import enl_ratio, edge_preservation_index\n",
    "                                                                                                                                                                                                                                                                                                    # Load real SAR patches - use Path properly\n",
    "patches_dir = project_root / \"data\" / \"patches\"                                                                                                                                                                                                                                                  \n",
    "patches_files = [f for f in patches_dir.glob(\"*.npy\") if 'metadata' not in f.name]\n",
    "patches_file = patches_files[0]\n",
    "patches = np.load(str(patches_file), mmap_mode='r')\n",
    "\n",
    "print(f\"Loaded patches from: {patches_file.name}\")\n",
    "print(f\"Shape: {patches.shape}\")\n",
    "\n",
    "# Evaluate on multiple patches\n",
    "n_eval = 10\n",
    "enl_ratios = []\n",
    "epis = []\n",
    "\n",
    "model.eval()\n",
    "for i in range(n_eval):\n",
    "    original = patches[i].copy()\n",
    "\n",
    "# Reconstruct\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.from_numpy(original).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    recon, _ = model(input_tensor)\n",
    "reconstructed = recon[0, 0].cpu().numpy()\n",
    "\n",
    "# ENL ratio\n",
    "result = enl_ratio(original, reconstructed)\n",
    "enl_ratios.append(result['enl_ratio'])\n",
    "\n",
    "# EPI\n",
    "epi = edge_preservation_index(original, reconstructed)\n",
    "epis.append(epi)\n",
    "\n",
    "print(f\"\\nSAR-Specific Metrics ({n_eval} samples):\")\n",
    "print(f\"  ENL Ratio: {np.mean(enl_ratios):.3f} +/- {np.std(enl_ratios):.3f}\")\n",
    "print(f\"  EPI: {np.mean(epis):.4f} +/- {np.std(epis):.4f}\")\n",
    "print(f\"\\n  ENL ratio target: [0.7, 1.3]\")\n",
    "print(f\"  ENL ratio in range: {0.7 <= np.mean(enl_ratios) <= 1.3}\")\n",
    "print(f\"  EPI target: > 0.8\")\n",
    "print(f\"  EPI target met: {np.mean(epis) > 0.8}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visual comparison with a single patch\noriginal = patches[0].copy()\nwith torch.no_grad():\n    input_tensor = torch.from_numpy(original).unsqueeze(0).unsqueeze(0).float().to(device)\n    recon, _ = model(input_tensor)\nreconstructed = recon[0, 0].cpu().numpy()\n\n# Compute metrics for this patch\nfrom src.evaluation.metrics import SARMetrics\npsnr = SARMetrics.psnr(original.flatten(), reconstructed.flatten())\nssim = SARMetrics.ssim(original.flatten(), reconstructed.flatten())\nresult = enl_ratio(original, reconstructed)\nepi = edge_preservation_index(original, reconstructed)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].imshow(original, cmap='gray')\naxes[0].set_title('Original')\naxes[0].axis('off')\n\naxes[1].imshow(reconstructed, cmap='gray')\naxes[1].set_title(f'Reconstructed\\nPSNR: {psnr:.2f} dB, SSIM: {ssim:.4f}')\naxes[1].axis('off')\n\naxes[2].imshow(original - reconstructed, cmap='RdBu_r', vmin=-0.3, vmax=0.3)\naxes[2].set_title(f'Difference\\nENL ratio: {result[\"enl_ratio\"]:.3f}, EPI: {epi:.4f}')\naxes[2].axis('off')\n\nplt.suptitle('Attention Autoencoder (Variant C) - SAR Reconstruction', fontsize=14)\nplt.tight_layout()\n\n# Save to results folder\nsave_path = MODEL_RESULTS_DIR / 'sar_evaluation.png'\nplt.savefig(save_path, dpi=150)\nplt.savefig(trainer.log_dir / 'sar_evaluation.png', dpi=150)\nplt.show()\n\nprint(f\"Saved SAR evaluation to: {save_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Full Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def format_improvement(current, baseline):\n",
    "    diff = current - baseline\n",
    "    return f\"+{diff:.2f} dB\" if diff >= 0 else f\"{diff:.2f} dB\"\n",
    "\n",
    "# Prepare data\n",
    "models_data = [\n",
    "    {'name': 'Baseline', 'params': '2.3M', 'psnr': 20.47, 'ssim': 0.646, 'enl': '-'},\n",
    "    {'name': 'ResNet-Lite v2', 'params': '5.6M', 'psnr': 21.20, 'ssim': 0.726, 'enl': '0.851'},\n",
    "]\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    enl_val = np.mean(enl_ratios) if 'enl_ratios' in dir() and len(enl_ratios) > 0 else None\n",
    "    models_data.append({\n",
    "        'name': 'Attention v2 (Variant C)', \n",
    "        'params': f'{params[\"total\"]/1e6:.1f}M',\n",
    "        'psnr': final['val_psnr'],\n",
    "        'ssim': final['val_ssim'],\n",
    "        'enl': f'{enl_val:.3f}' if enl_val else 'N/A'\n",
    "    })\n",
    "\n",
    "print(f\"{'Model':<25} {'Params':>8}  {'PSNR':>10}  {'SSIM':>8}  {'ENL Ratio':>10}  {'vs Baseline':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for m in models_data:\n",
    "    improvement = format_improvement(m['psnr'], 20.47)\n",
    "    print(f\"{m['name']:<25} {m['params']:>8}  {m['psnr']:>10.2f}  {m['ssim']:>8.4f}  {str(m['enl']):>10}  {improvement:>12}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. CBAM Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of CBAM attention impact\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CBAM ATTENTION IMPACT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    \n",
    "    print(f\"\\nArchitecture:\")\n",
    "    print(f\"  Total parameters: {params['total']:,}\")\n",
    "    print(f\"  CBAM modules: {cbam_count}\")\n",
    "    print(f\"  CBAM overhead vs ResidualAutoencoder: ~0.7%\")\n",
    "    \n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Val PSNR: {final['val_psnr']:.2f} dB\")\n",
    "    print(f\"  Val SSIM: {final['val_ssim']:.4f}\")\n",
    "    print(f\"  ENL Ratio: {np.mean(enl_ratios):.3f}\")\n",
    "    print(f\"  EPI: {np.mean(epis):.4f}\")\n",
    "    \n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"  vs Baseline: {final['val_psnr'] - 20.47:+.2f} dB PSNR\")\n",
    "    print(f\"  vs ResNet-Lite: {final['val_psnr'] - 21.20:+.2f} dB PSNR\")\n",
    "    \n",
    "    # Conclusion based on results\n",
    "    print(f\"\\nConclusion:\")\n",
    "    if final['val_psnr'] > 21.70:  # More than 0.5 dB above ResNet-Lite\n",
    "        print(\"  [+] CBAM provides meaningful improvement over residual-only architecture\")\n",
    "        print(f\"  [+] +{final['val_psnr'] - 21.20:.2f} dB PSNR gain with 0.7% parameter overhead\")\n",
    "    else:\n",
    "        print(\"  [-] CBAM did NOT provide expected +0.5 dB improvement\")\n",
    "        print(\"  [-] Possible reasons:\")\n",
    "        print(\"      - 20% subset may not show attention benefits\")\n",
    "        print(\"      - CBAM may need longer training to converge\")\n",
    "        print(\"      - SAR data may not benefit from attention as much as natural images\")\n",
    "else:\n",
    "    print(\"No results available. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 15. Best & Worst Reconstructions\n\nFind the best and worst reconstruction cases to understand model strengths and failure modes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Find best and worst reconstruction cases\nfrom src.evaluation.evaluator import Evaluator\n\nprint(\"Finding best and worst reconstructions...\")\n\n# Create evaluator\nevaluator = Evaluator(model, device=device)\n\n# Find best and worst cases\nn_cases = 8\nworst_cases = evaluator.find_failure_cases(val_loader, n_worst=n_cases)\nbest_cases = evaluator.find_best_cases(val_loader, n_best=n_cases)\n\nprint(f\"\\nWorst reconstructions (highest MSE):\")\nfor i, case in enumerate(worst_cases[:5]):\n    print(f\"  {i+1}. MSE = {case['mse']:.6f}\")\n\nprint(f\"\\nBest reconstructions (lowest MSE):\")\nfor i, case in enumerate(best_cases[:5]):\n    print(f\"  {i+1}. MSE = {case['mse']:.6f}\")\n\n# Plot best reconstructions\nfig, axes = plt.subplots(3, n_cases, figsize=(2.5 * n_cases, 8))\nfig.suptitle('Best Reconstructions (Top: Original, Middle: Reconstructed, Bottom: Difference)', fontsize=14)\n\nfor i in range(min(n_cases, len(best_cases))):\n    orig = best_cases[i]['original'].numpy().squeeze()\n    recon = best_cases[i]['reconstructed'].numpy().squeeze()\n    diff = np.abs(orig - recon)\n    \n    axes[0, i].imshow(orig, cmap='gray', vmin=0, vmax=1)\n    axes[0, i].axis('off')\n    axes[0, i].set_title(f\"MSE: {best_cases[i]['mse']:.4f}\", fontsize=8)\n    \n    axes[1, i].imshow(recon, cmap='gray', vmin=0, vmax=1)\n    axes[1, i].axis('off')\n    \n    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.2)\n    axes[2, i].axis('off')\n\nplt.tight_layout()\nplt.savefig(MODEL_RESULTS_DIR / 'best_reconstructions.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Plot worst reconstructions\nfig, axes = plt.subplots(3, n_cases, figsize=(2.5 * n_cases, 8))\nfig.suptitle('Worst Reconstructions (Failure Cases)', fontsize=14)\n\nfor i in range(min(n_cases, len(worst_cases))):\n    orig = worst_cases[i]['original'].numpy().squeeze()\n    recon = worst_cases[i]['reconstructed'].numpy().squeeze()\n    diff = np.abs(orig - recon)\n    \n    axes[0, i].imshow(orig, cmap='gray', vmin=0, vmax=1)\n    axes[0, i].axis('off')\n    axes[0, i].set_title(f\"MSE: {worst_cases[i]['mse']:.4f}\", fontsize=8)\n    \n    axes[1, i].imshow(recon, cmap='gray', vmin=0, vmax=1)\n    axes[1, i].axis('off')\n    \n    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.3)\n    axes[2, i].axis('off')\n\nplt.tight_layout()\nplt.savefig(MODEL_RESULTS_DIR / 'worst_reconstructions.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nSaved: {MODEL_RESULTS_DIR / 'best_reconstructions.png'}\")\nprint(f\"Saved: {MODEL_RESULTS_DIR / 'worst_reconstructions.png'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize latent channel activations\nprint(\"Latent space visualization (first sample):\")\n\nwith torch.no_grad():\n    sample = next(iter(val_loader))[:1].to(device)\n    _, latent = model(sample)\n    latent_np = latent[0].cpu().numpy()\n\nn_channels = min(16, latent_np.shape[0])\nfig, axes = plt.subplots(4, 4, figsize=(12, 12))\nfig.suptitle(f'Latent Channel Activations ({latent_np.shape[1]}x{latent_np.shape[2]} spatial, {latent_np.shape[0]} channels)', fontsize=14)\n\nfor i, ax in enumerate(axes.flatten()):\n    if i < n_channels:\n        channel = latent_np[i]\n        vmax = max(abs(channel.min()), abs(channel.max()), 0.1)\n        im = ax.imshow(channel, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n        ax.set_title(f'Ch {i}: std={channel.std():.2f}', fontsize=8)\n        ax.axis('off')\n    else:\n        ax.axis('off')\n\nplt.tight_layout()\nplt.savefig(MODEL_RESULTS_DIR / 'latent_channels.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Compute latent statistics\nactive_channels = sum(1 for i in range(latent_np.shape[0]) if latent_np[i].std() > 0.01)\nprint(f\"\\nLatent statistics:\")\nprint(f\"  Shape: {latent_np.shape}\")\nprint(f\"  Active channels (std > 0.01): {active_channels}/{latent_np.shape[0]}\")\nprint(f\"  Mean activation: {latent_np.mean():.4f}\")\nprint(f\"  Std activation: {latent_np.std():.4f}\")\nprint(f\"\\nSaved: {MODEL_RESULTS_DIR / 'latent_channels.png'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 16. Latent Channel Visualization\n\nVisualize activations in the latent space to understand what the model encodes.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "**Compare in TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=D:/Projects/CNNAutoencoderProject/notebooks/runs\n",
    "```\n",
    "\n",
    "Select `baseline_c16_fast`, `resnet_lite_v2_c16`, and `attention_v1_c16` to compare metrics.\n",
    "\n",
    "**Next steps:**\n",
    "- Train Variant B (ResidualAutoencoder) for direct comparison\n",
    "- Consider full dataset training if subset results are promising\n",
    "- Proceed to Phase 5 (Full Inference Pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}