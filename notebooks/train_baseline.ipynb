{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline SAR Autoencoder Training\n",
    "\n",
    "Phase 2, Plan 04: Train the baseline autoencoder model.\n",
    "\n",
    "**Targets:**\n",
    "- Validation PSNR > 25 dB\n",
    "- 16x compression with latent_channels=16\n",
    "- Save best checkpoint with preprocessing_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\Projects\\CNNAutoencoderProject\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "VRAM: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Project imports\n",
    "from src.data.datamodule import SARDataModule\n",
    "from src.models.autoencoder import SARAutoencoder\n",
    "from src.losses.combined import CombinedLoss\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Adjust these parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression ratio: 16x\n",
      "Training subset: 30% of data\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Modify these as needed\n",
    "# ============================================================\n",
    "\n",
    "# Data settings\n",
    "DATA_PATH = \"D:/Projects/CNNAutoencoderProject/data/patches/metadata.npy\"\n",
    "BATCH_SIZE = 32       # 8 for RTX 3070 8GB VRAM (reduce to 4 if OOM)\n",
    "NUM_WORKERS = 0      # 0 for Windows compatibility\n",
    "VAL_FRACTION = 0.1   # 10% validation split\n",
    "TRAIN_SUBSET = 0.30\n",
    "\n",
    "# Model settings\n",
    "LATENT_CHANNELS = 16  # 16 = 16x compression\n",
    "BASE_CHANNELS = 64    # Base channel count\n",
    "\n",
    "# Loss settings\n",
    "MSE_WEIGHT = 0.5\n",
    "SSIM_WEIGHT = 0.5\n",
    "\n",
    "# Training settings\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "LR_PATIENCE = 10\n",
    "LR_FACTOR = 0.5\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Output settings\n",
    "RUN_NAME = f\"baseline_c{LATENT_CHANNELS}_fast\"\n",
    "\n",
    "# Calculate compression ratio\n",
    "compression_ratio = (256 * 256) / (16 * 16 * LATENT_CHANNELS)\n",
    "print(f\"Compression ratio: {compression_ratio:.0f}x\")\n",
    "print(f\"Training subset: {TRAIN_SUBSET*100:.0f}% of data\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading metadata from D:\\Projects\\CNNAutoencoderProject\\data\\patches\\metadata.npy\n",
      "Total patches: 696277\n",
      "Train: 626650, Val: 69627\n",
      "Using 30% subset:\n",
      "  Train: 187,995 of 626,650\n",
      "  Val: 20,888 of 69,627\n",
      "\n",
      "Dataset loaded:\n",
      "  Train patches: 187,995\n",
      "  Val patches: 20,888\n",
      "  Train batches: 5,874\n",
      "  Val batches: 653\n",
      "  Preprocessing params: {'vmin': np.float32(14.768799), 'vmax': np.float32(24.54073)}\n",
      "\n",
      "  Estimated time per epoch: ~44 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "dm = SARDataModule(\n",
    "    patches_path=DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    val_fraction=VAL_FRACTION,\n",
    ")\n",
    "\n",
    "# Apply subset to BOTH train and val for faster iteration\n",
    "if TRAIN_SUBSET < 1.0:\n",
    "    import random\n",
    "    \n",
    "    # Subset training data\n",
    "    full_train_size = len(dm.train_dataset)\n",
    "    train_subset_size = int(full_train_size * TRAIN_SUBSET)\n",
    "    train_indices = random.sample(range(full_train_size), train_subset_size)\n",
    "    dm.train_dataset = torch.utils.data.Subset(dm.train_dataset, train_indices)\n",
    "    \n",
    "    # Subset validation data (same proportion)\n",
    "    full_val_size = len(dm.val_dataset)\n",
    "    val_subset_size = int(full_val_size * TRAIN_SUBSET)\n",
    "    val_indices = random.sample(range(full_val_size), val_subset_size)\n",
    "    dm.val_dataset = torch.utils.data.Subset(dm.val_dataset, val_indices)\n",
    "    \n",
    "    print(f\"Using {TRAIN_SUBSET*100:.0f}% subset:\")\n",
    "    print(f\"  Train: {train_subset_size:,} of {full_train_size:,}\")\n",
    "    print(f\"  Val: {val_subset_size:,} of {full_val_size:,}\")\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Train patches: {len(dm.train_dataset):,}\")\n",
    "print(f\"  Val patches: {len(dm.val_dataset):,}\")\n",
    "print(f\"  Train batches: {len(train_loader):,}\")\n",
    "print(f\"  Val batches: {len(val_loader):,}\")\n",
    "print(f\"  Preprocessing params: {dm.preprocessing_params}\")\n",
    "\n",
    "# Estimate epoch time\n",
    "est_batches = len(train_loader) + len(val_loader)\n",
    "est_time_min = est_batches / 2.5 / 60  # ~2.5 it/s with batch_size=32\n",
    "print(f\"\\n  Estimated time per epoch: ~{est_time_min:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([32, 1, 256, 256])\n",
      "Sample batch dtype: torch.float32\n",
      "Sample batch range: [0.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "# Verify a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample batch dtype: {sample_batch.dtype}\")\n",
    "print(f\"Sample batch range: [{sample_batch.min():.4f}, {sample_batch.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "\n",
      "Model created:\n",
      "  Total parameters: 2,257,809\n",
      "  Encoder params: 1,128,912\n",
      "  Decoder params: 1,128,897\n",
      "  Compression ratio: 16.0x\n",
      "  Latent size: (16, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\")\n",
    "model = SARAutoencoder(\n",
    "    latent_channels=LATENT_CHANNELS,\n",
    "    base_channels=BASE_CHANNELS,\n",
    ")\n",
    "\n",
    "params = model.count_parameters()\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Total parameters: {params['total']:,}\")\n",
    "print(f\"  Encoder params: {params['encoder']:,}\")\n",
    "print(f\"  Decoder params: {params['decoder']:,}\")\n",
    "print(f\"  Compression ratio: {model.get_compression_ratio():.1f}x\")\n",
    "print(f\"  Latent size: {model.get_latent_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward pass...\n",
      "  Input: torch.Size([2, 1, 256, 256])\n",
      "  Latent: torch.Size([2, 16, 16, 16])\n",
      "  Output: torch.Size([2, 1, 256, 256])\n",
      "  Output range: [0.0946, 0.8949]\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "print(\"Testing forward pass...\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_test = model.to(device)\n",
    "x_test = sample_batch[:2].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat, z = model_test(x_test)\n",
    "\n",
    "print(f\"  Input: {x_test.shape}\")\n",
    "print(f\"  Latent: {z.shape}\")\n",
    "print(f\"  Output: {x_hat.shape}\")\n",
    "print(f\"  Output range: [{x_hat.min():.4f}, {x_hat.max():.4f}]\")\n",
    "\n",
    "# Clean up test\n",
    "del model_test, x_test, x_hat, z\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: CombinedLoss\n",
      "  MSE weight: 0.5\n",
      "  SSIM weight: 0.5\n"
     ]
    }
   ],
   "source": [
    "loss_fn = CombinedLoss(\n",
    "    mse_weight=MSE_WEIGHT,\n",
    "    ssim_weight=SSIM_WEIGHT,\n",
    ")\n",
    "\n",
    "print(f\"Loss function: CombinedLoss\")\n",
    "print(f\"  MSE weight: {MSE_WEIGHT}\")\n",
    "print(f\"  SSIM weight: {SSIM_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2026-01-22 05:08:40,694 - Log directory: runs\\baseline_c16_fast\n",
      "2026-01-22 05:08:40,695 - Checkpoint directory: checkpoints\\baseline_c16_fast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing trainer...\n",
      "Using device: cuda\n",
      "\n",
      "Trainer ready:\n",
      "  Log dir: runs\\baseline_c16_fast\n",
      "  Checkpoint dir: checkpoints\\baseline_c16_fast\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'lr_patience': LR_PATIENCE,\n",
    "    'lr_factor': LR_FACTOR,\n",
    "    'max_grad_norm': MAX_GRAD_NORM,\n",
    "    'run_name': RUN_NAME,\n",
    "    'preprocessing_params': dm.preprocessing_params,\n",
    "    # Store hyperparams for reproducibility\n",
    "    'latent_channels': LATENT_CHANNELS,\n",
    "    'base_channels': BASE_CHANNELS,\n",
    "    'mse_weight': MSE_WEIGHT,\n",
    "    'ssim_weight': SSIM_WEIGHT,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "}\n",
    "\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"\\nTrainer ready:\")\n",
    "print(f\"  Log dir: {trainer.log_dir}\")\n",
    "print(f\"  Checkpoint dir: {trainer.checkpoint_dir}\")\n",
    "print(f\"  Device: {trainer.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "**Expected duration:** ~2-4 hours for 50 epochs on RTX 3070\n",
    "\n",
    "**Monitor with TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 05:08:44,768 - Starting training for 50 epochs\n",
      "2026-01-22 05:08:44,768 - Model: SARAutoencoder\n",
      "2026-01-22 05:08:44,770 - Config: {'learning_rate': 0.0001, 'lr_patience': 10, 'lr_factor': 0.5, 'max_grad_norm': 1.0, 'run_name': 'baseline_c16_fast', 'preprocessing_params': {'vmin': np.float32(14.768799), 'vmax': np.float32(24.54073)}, 'latent_channels': 16, 'base_channels': 64, 'mse_weight': 0.5, 'ssim_weight': 0.5, 'batch_size': 32, 'epochs': 50}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Training\n",
      "============================================================\n",
      "Epochs: 50\n",
      "Early stopping patience: 10\n",
      "Learning rate: 0.0001\n",
      "\n",
      "TensorBoard: tensorboard --logdir=runs\n",
      "Checkpoints: checkpoints\\baseline_c16_fast\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  29%|██▊       | 1679/5874 [03:05<18:57,  3.69it/s, loss=0.2699, psnr=17.64, ssim=0.4773]  "
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nTensorBoard: tensorboard --logdir={trainer.log_dir.parent}\")\n",
    "print(f\"Checkpoints: {trainer.checkpoint_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = trainer.train(\n",
    "    epochs=EPOCHS,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if history:\n",
    "    final = history[-1]\n",
    "    print(f\"Final epoch: {final['epoch'] + 1}\")\n",
    "    print(f\"Best val loss: {trainer.best_val_loss:.4f}\")\n",
    "    print(f\"Final val PSNR: {final['val_psnr']:.2f} dB\")\n",
    "    print(f\"Final val SSIM: {final['val_ssim']:.4f}\")\n",
    "    \n",
    "    # Check success criterion\n",
    "    if final['val_psnr'] >= 25:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"[SUCCESS] PSNR > 25 dB achieved!\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(f\"\\n[WARNING] PSNR {final['val_psnr']:.2f} < 25 dB target\")\n",
    "\n",
    "print(f\"\\nBest checkpoint: {trainer.checkpoint_dir / 'best.pth'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if history:\n",
    "    epochs = [h['epoch'] + 1 for h in history]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs, [h['train_loss'] for h in history], label='Train')\n",
    "    ax.plot(epochs, [h['val_loss'] for h in history], label='Val')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # PSNR\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs, [h['train_psnr'] for h in history], label='Train')\n",
    "    ax.plot(epochs, [h['val_psnr'] for h in history], label='Val')\n",
    "    ax.axhline(y=25, color='r', linestyle='--', label='Target (25 dB)')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('PSNR (dB)')\n",
    "    ax.set_title('PSNR')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # SSIM\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(epochs, [h['train_ssim'] for h in history], label='Train')\n",
    "    ax.plot(epochs, [h['val_ssim'] for h in history], label='Val')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_title('SSIM')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(epochs, [h['learning_rate'] for h in history])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('Learning Rate Schedule')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(trainer.log_dir / 'training_curves.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSaved training curves to: {trainer.log_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify checkpoint can be loaded\n",
    "checkpoint_path = trainer.checkpoint_dir / 'best.pth'\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "    print(\"Checkpoint contents:\")\n",
    "    print(f\"  Keys: {list(ckpt.keys())}\")\n",
    "    print(f\"  Epoch: {ckpt['epoch']}\")\n",
    "    print(f\"  Best val loss: {ckpt['best_val_loss']:.4f}\")\n",
    "    print(f\"  Preprocessing params: {ckpt.get('preprocessing_params', 'MISSING')}\")\n",
    "    \n",
    "    # Test loading into fresh model\n",
    "    print(\"\\nLoading weights into fresh model...\")\n",
    "    test_model = SARAutoencoder(latent_channels=LATENT_CHANNELS)\n",
    "    test_model.load_state_dict(ckpt['model_state_dict'])\n",
    "    test_model.eval()\n",
    "    \n",
    "    # Test inference\n",
    "    x = torch.rand(1, 1, 256, 256)\n",
    "    with torch.no_grad():\n",
    "        x_hat, z = test_model(x)\n",
    "    \n",
    "    print(f\"  Inference test: {x.shape} -> {z.shape} -> {x_hat.shape}\")\n",
    "    print(\"\\nCheckpoint verification: PASS\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample reconstructions from validation set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# Get a batch from validation\n",
    "val_batch = next(iter(val_loader))[:4].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, latent = model(val_batch)\n",
    "\n",
    "# Move to CPU for plotting\n",
    "originals = val_batch.cpu().numpy()\n",
    "reconstructions = reconstructed.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(4):\n",
    "    # Original\n",
    "    axes[0, i].imshow(originals[i, 0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Original {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstruction\n",
    "    axes[1, i].imshow(reconstructions[i, 0], cmap='gray')\n",
    "    axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Difference\n",
    "    diff = abs(originals[i, 0] - reconstructions[i, 0])\n",
    "    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[2, i].set_title(f'Difference {i+1}')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=12)\n",
    "axes[2, 0].set_ylabel('Difference', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'Sample Reconstructions (Compression: {model.get_compression_ratio():.0f}x)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(trainer.log_dir / 'sample_reconstructions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "**Next steps:**\n",
    "1. View TensorBoard: `tensorboard --logdir=runs`\n",
    "2. Check the best checkpoint at the path shown above\n",
    "3. Proceed to Phase 3: SAR Evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
