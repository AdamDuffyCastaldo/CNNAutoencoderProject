{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Inference Pipeline Validation\n",
    "\n",
    "This notebook validates the complete SAR compression/decompression pipeline against Phase 5 success criteria:\n",
    "\n",
    "1. **Memory Test**: Large image (4096x4096) processes without OOM\n",
    "2. **Seamless Blending**: No visible tile boundaries in reconstructed image\n",
    "3. **PSNR Consistency**: Round-trip PSNR within 0.5 dB of patch-level metrics\n",
    "4. **Preprocessing Round-Trip**: Inverse preprocessing restores linear values\n",
    "5. **CLI Smoke Test**: GeoTIFF compress/decompress via CLI with metadata preservation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.inference import SARCompressor\n",
    "from src.inference.geotiff import read_geotiff, write_geotiff, GeoMetadata\n",
    "from src.inference.tiling import create_cosine_ramp_weights, visualize_blend_weights\n",
    "from src.evaluation.metrics import SARMetrics\n",
    "\n",
    "# Paths\n",
    "MODEL_PATH = \"checkpoints/resnet_lite_v2_c16/best.pth\"\n",
    "RESULTS_DIR = Path(\"evaluations\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Check model exists\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Results collector\n",
    "results = {\n",
    "    \"test_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_path\": MODEL_PATH,\n",
    "    \"tests\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Synthetic Large Image (Memory Test)\n",
    "\n",
    "Test that a 4096x4096 image can be processed without running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Test 1: Memory Test (4096x4096 Synthetic Image)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset CUDA memory stats\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Create synthetic normalized image with cosine patterns (4096x4096)\n",
    "# This simulates a large SAR scene\n",
    "size = 4096\n",
    "y, x = np.mgrid[0:size, 0:size].astype(np.float32)\n",
    "\n",
    "# Create smooth patterns that look SAR-like (already normalized [0, 1])\n",
    "image_4k = (\n",
    "    0.3 + \n",
    "    0.25 * np.cos(2 * np.pi * x / 200) * np.cos(2 * np.pi * y / 200) +\n",
    "    0.15 * np.cos(2 * np.pi * x / 50) * np.cos(2 * np.pi * y / 50) +\n",
    "    0.1 * np.sin(2 * np.pi * (x + y) / 100)\n",
    ")\n",
    "image_4k = np.clip(image_4k, 0, 1).astype(np.float32)\n",
    "\n",
    "print(f\"Created synthetic image: {image_4k.shape}\")\n",
    "print(f\"Value range: [{image_4k.min():.3f}, {image_4k.max():.3f}]\")\n",
    "print(f\"Memory: {image_4k.nbytes / 1024**2:.1f} MB\")\n",
    "\n",
    "# Initialize compressor\n",
    "compressor = SARCompressor(\n",
    "    model_path=MODEL_PATH,\n",
    "    patch_size=256,\n",
    "    overlap=64\n",
    ")\n",
    "print(f\"\\nCompressor initialized:\")\n",
    "print(f\"  Device: {compressor.device}\")\n",
    "print(f\"  Batch size: {compressor.batch_size}\")\n",
    "\n",
    "# Time the compression + decompression\n",
    "print(\"\\nRunning compression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # We use preprocess + compress since image is already normalized\n",
    "    # To test the full pipeline, we'll simulate linear SAR data\n",
    "    # Convert from normalized back to linear for full pipeline test\n",
    "    vmin, vmax = compressor.preprocess_params['vmin'], compressor.preprocess_params['vmax']\n",
    "    image_db = image_4k * (vmax - vmin) + vmin\n",
    "    image_linear = np.power(10.0, image_db / 10.0).astype(np.float32)\n",
    "    \n",
    "    # Now do full compress (which includes preprocess)\n",
    "    latent, metadata = compressor.compress(image_linear)\n",
    "    compress_time = time.time() - start_time\n",
    "    print(f\"Compression complete: {compress_time:.2f}s\")\n",
    "    print(f\"  Latent shape: {latent.shape}\")\n",
    "    print(f\"  Tiles: {latent.shape[0]}\")\n",
    "    \n",
    "    # Decompress\n",
    "    print(\"\\nRunning decompression...\")\n",
    "    decompress_start = time.time()\n",
    "    reconstructed = compressor.decompress(latent, metadata)\n",
    "    decompress_time = time.time() - decompress_start\n",
    "    print(f\"Decompression complete: {decompress_time:.2f}s\")\n",
    "    \n",
    "    total_time = compress_time + decompress_time\n",
    "    \n",
    "    # Memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory_mb = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        print(f\"\\nPeak GPU memory: {peak_memory_mb:.1f} MB\")\n",
    "    else:\n",
    "        peak_memory_mb = 0\n",
    "    \n",
    "    # Verify output shape\n",
    "    assert reconstructed.shape == image_linear.shape, f\"Shape mismatch: {reconstructed.shape} vs {image_linear.shape}\"\n",
    "    print(f\"Output shape matches input: {reconstructed.shape}\")\n",
    "    \n",
    "    # Quality check (on normalized data for fair comparison)\n",
    "    recon_normalized = compressor.preprocess(reconstructed)\n",
    "    psnr = SARMetrics.psnr(image_4k, recon_normalized)\n",
    "    print(f\"\\nQuality: PSNR = {psnr:.2f} dB\")\n",
    "    \n",
    "    test1_passed = True\n",
    "    test1_result = {\n",
    "        \"passed\": True,\n",
    "        \"image_size\": list(image_4k.shape),\n",
    "        \"compress_time_sec\": compress_time,\n",
    "        \"decompress_time_sec\": decompress_time,\n",
    "        \"total_time_sec\": total_time,\n",
    "        \"peak_gpu_memory_mb\": peak_memory_mb,\n",
    "        \"n_tiles\": int(latent.shape[0]),\n",
    "        \"psnr_db\": psnr\n",
    "    }\n",
    "    print(\"\\n[PASSED] Test 1: Memory test passed - no OOM\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    test1_passed = False\n",
    "    test1_result = {\n",
    "        \"passed\": False,\n",
    "        \"error\": \"OutOfMemoryError\",\n",
    "        \"message\": str(e)\n",
    "    }\n",
    "    print(f\"\\n[FAILED] Test 1: Out of memory - {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    test1_passed = False\n",
    "    test1_result = {\n",
    "        \"passed\": False,\n",
    "        \"error\": type(e).__name__,\n",
    "        \"message\": str(e)\n",
    "    }\n",
    "    print(f\"\\n[FAILED] Test 1: {type(e).__name__} - {e}\")\n",
    "\n",
    "results[\"tests\"][\"memory_test\"] = test1_result\n",
    "\n",
    "# Cleanup\n",
    "del image_4k, image_linear\n",
    "if 'reconstructed' in dir():\n",
    "    del reconstructed, latent\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Seamless Blending Verification\n",
    "\n",
    "Check that tile boundaries are not visible in the reconstructed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Test 2: Seamless Blending Verification\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create high-frequency pattern that would reveal tile boundaries\n",
    "size = 1024\n",
    "y, x = np.mgrid[0:size, 0:size].astype(np.float32)\n",
    "\n",
    "# Checkerboard + grid pattern (would show artifacts at tile boundaries)\n",
    "checkerboard = ((x // 8) % 2) ^ ((y // 8) % 2)\n",
    "grid = (np.sin(2 * np.pi * x / 16) * np.sin(2 * np.pi * y / 16) + 1) / 2\n",
    "test_pattern = 0.3 + 0.4 * (0.5 * checkerboard + 0.5 * grid)\n",
    "test_pattern = np.clip(test_pattern, 0, 1).astype(np.float32)\n",
    "\n",
    "print(f\"Created test pattern: {test_pattern.shape}\")\n",
    "print(f\"Value range: [{test_pattern.min():.3f}, {test_pattern.max():.3f}]\")\n",
    "\n",
    "# Convert to linear SAR for full pipeline\n",
    "vmin, vmax = compressor.preprocess_params['vmin'], compressor.preprocess_params['vmax']\n",
    "pattern_db = test_pattern * (vmax - vmin) + vmin\n",
    "pattern_linear = np.power(10.0, pattern_db / 10.0).astype(np.float32)\n",
    "\n",
    "# Compress and decompress\n",
    "latent, metadata = compressor.compress(pattern_linear)\n",
    "reconstructed_linear = compressor.decompress(latent, metadata)\n",
    "\n",
    "# Convert back to normalized for comparison\n",
    "reconstructed = compressor.preprocess(reconstructed_linear)\n",
    "\n",
    "# Compute difference\n",
    "difference = reconstructed - test_pattern\n",
    "\n",
    "print(f\"\\nReconstruction stats:\")\n",
    "print(f\"  PSNR: {SARMetrics.psnr(test_pattern, reconstructed):.2f} dB\")\n",
    "print(f\"  SSIM: {SARMetrics.ssim(test_pattern, reconstructed):.4f}\")\n",
    "print(f\"  Max diff: {np.abs(difference).max():.4f}\")\n",
    "print(f\"  Mean diff: {np.abs(difference).mean():.6f}\")\n",
    "\n",
    "# Analyze difference at tile boundaries\n",
    "# Tiles are at 192-pixel intervals (256 - 64 overlap)\n",
    "stride = 256 - 64\n",
    "\n",
    "# Extract rows/cols at tile boundaries\n",
    "boundary_positions = [stride * i for i in range(1, size // stride)]\n",
    "boundary_positions = [p for p in boundary_positions if p < size]\n",
    "\n",
    "# Average error at boundaries vs non-boundaries\n",
    "boundary_mask = np.zeros_like(difference, dtype=bool)\n",
    "boundary_width = 8  # Check 8 pixels around boundaries\n",
    "for pos in boundary_positions:\n",
    "    # Vertical boundaries\n",
    "    boundary_mask[:, max(0, pos-boundary_width):min(size, pos+boundary_width)] = True\n",
    "    # Horizontal boundaries\n",
    "    boundary_mask[max(0, pos-boundary_width):min(size, pos+boundary_width), :] = True\n",
    "\n",
    "error_at_boundaries = np.abs(difference[boundary_mask]).mean()\n",
    "error_elsewhere = np.abs(difference[~boundary_mask]).mean()\n",
    "\n",
    "print(f\"\\nBoundary analysis:\")\n",
    "print(f\"  Mean error at boundaries: {error_at_boundaries:.6f}\")\n",
    "print(f\"  Mean error elsewhere: {error_elsewhere:.6f}\")\n",
    "print(f\"  Ratio (boundary/elsewhere): {error_at_boundaries/error_elsewhere:.3f}\")\n",
    "\n",
    "# If ratio is close to 1.0, boundaries are seamless\n",
    "# A ratio > 1.5 would indicate visible seams\n",
    "boundary_ratio = error_at_boundaries / error_elsewhere\n",
    "seamless = boundary_ratio < 1.5\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(test_pattern, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0, 0].set_title('Original Pattern')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Reconstructed\n",
    "axes[0, 1].imshow(reconstructed, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0, 1].set_title('Reconstructed')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Difference (amplified)\n",
    "diff_display = np.clip(difference * 10 + 0.5, 0, 1)  # Amplified difference\n",
    "axes[0, 2].imshow(diff_display, cmap='RdBu', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title('Difference (10x amplified)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Zoomed region showing tile boundary\n",
    "zoom_center = stride  # First tile boundary\n",
    "zoom_size = 128\n",
    "zoom_slice = slice(zoom_center - zoom_size//2, zoom_center + zoom_size//2)\n",
    "\n",
    "axes[1, 0].imshow(test_pattern[zoom_slice, zoom_slice], cmap='gray', vmin=0, vmax=1)\n",
    "axes[1, 0].axvline(x=zoom_size//2, color='r', linestyle='--', alpha=0.5, label='Tile boundary')\n",
    "axes[1, 0].set_title('Zoomed Original (boundary marked)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].imshow(reconstructed[zoom_slice, zoom_slice], cmap='gray', vmin=0, vmax=1)\n",
    "axes[1, 1].axvline(x=zoom_size//2, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('Zoomed Reconstructed')\n",
    "\n",
    "# Difference in zoom region\n",
    "axes[1, 2].imshow(diff_display[zoom_slice, zoom_slice], cmap='RdBu', vmin=0, vmax=1)\n",
    "axes[1, 2].axvline(x=zoom_size//2, color='k', linestyle='--', alpha=0.5)\n",
    "axes[1, 2].set_title('Zoomed Difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'seamless_blending_test.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "if seamless:\n",
    "    print(\"\\n[PASSED] Test 2: Seamless blending - no visible tile boundaries\")\n",
    "else:\n",
    "    print(f\"\\n[FAILED] Test 2: Potential tile boundary artifacts (ratio={boundary_ratio:.2f})\")\n",
    "\n",
    "test2_result = {\n",
    "    \"passed\": seamless,\n",
    "    \"boundary_error_ratio\": boundary_ratio,\n",
    "    \"error_at_boundaries\": error_at_boundaries,\n",
    "    \"error_elsewhere\": error_elsewhere,\n",
    "    \"psnr_db\": SARMetrics.psnr(test_pattern, reconstructed),\n",
    "    \"ssim\": SARMetrics.ssim(test_pattern, reconstructed)\n",
    "}\n",
    "results[\"tests\"][\"seamless_blending\"] = test2_result\n",
    "\n",
    "# Cleanup\n",
    "del test_pattern, pattern_linear, reconstructed_linear, reconstructed, difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: PSNR Consistency\n",
    "\n",
    "Verify that the round-trip PSNR through tiled pipeline is within 0.5 dB of patch-level metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Test 3: PSNR Consistency (Patch vs Full Pipeline)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load some validation patches from training data\n",
    "# We'll use the same preprocessing to ensure fair comparison\n",
    "\n",
    "np.random.seed(42)\n",
    "n_patches = 10\n",
    "patch_size = 256\n",
    "\n",
    "# Try to load real validation data\n",
    "data_dir = Path(\"../data/processed\")\n",
    "validation_files = list(data_dir.glob(\"val_*.npy\")) if data_dir.exists() else []\n",
    "\n",
    "if validation_files:\n",
    "    print(f\"Found {len(validation_files)} validation files\")\n",
    "    # Load first file and sample patches\n",
    "    val_data = np.load(validation_files[0], mmap_mode='r')\n",
    "    indices = np.random.choice(len(val_data), size=min(n_patches, len(val_data)), replace=False)\n",
    "    patches = val_data[indices].squeeze()  # Remove channel dim if present\n",
    "    print(f\"Loaded {len(patches)} validation patches\")\n",
    "else:\n",
    "    print(\"No validation data found, creating synthetic SAR-like patches\")\n",
    "    # Create synthetic SAR-like patches (already normalized)\n",
    "    patches = []\n",
    "    for i in range(n_patches):\n",
    "        y, x = np.mgrid[0:patch_size, 0:patch_size].astype(np.float32)\n",
    "        # Create SAR-like pattern with smooth regions and edges\n",
    "        base = 0.3 + 0.4 * np.sin(2 * np.pi * (x + np.random.rand() * 100) / (50 + np.random.rand() * 50))\n",
    "        base *= np.sin(2 * np.pi * (y + np.random.rand() * 100) / (50 + np.random.rand() * 50))\n",
    "        base = (base - base.min()) / (base.max() - base.min() + 1e-8)  # Normalize to [0, 1]\n",
    "        patches.append(base.astype(np.float32))\n",
    "    patches = np.stack(patches)\n",
    "\n",
    "print(f\"Patch shape: {patches[0].shape}\")\n",
    "print(f\"Value range: [{patches.min():.3f}, {patches.max():.3f}]\")\n",
    "\n",
    "# For each patch:\n",
    "# 1. Direct model inference (single patch) -> patch_psnr\n",
    "# 2. Embed patch in 512x512 image, full pipeline -> full_psnr\n",
    "\n",
    "patch_psnrs = []\n",
    "full_psnrs = []\n",
    "differences = []\n",
    "\n",
    "# Get model for direct inference\n",
    "model = compressor.model\n",
    "model.eval()\n",
    "\n",
    "for i, patch in enumerate(patches):\n",
    "    # Ensure patch is 2D\n",
    "    if patch.ndim == 3:\n",
    "        patch = patch.squeeze()\n",
    "    \n",
    "    # === Method 1: Direct model inference ===\n",
    "    with torch.inference_mode():\n",
    "        patch_tensor = torch.from_numpy(patch).unsqueeze(0).unsqueeze(0).to(compressor.device)\n",
    "        if compressor.device.type == 'cuda':\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                latent = model.encode(patch_tensor)\n",
    "                reconstructed_tensor = model.decode(latent)\n",
    "        else:\n",
    "            latent = model.encode(patch_tensor)\n",
    "            reconstructed_tensor = model.decode(latent)\n",
    "        direct_recon = reconstructed_tensor.squeeze().cpu().numpy()\n",
    "    \n",
    "    patch_psnr = SARMetrics.psnr(patch, direct_recon)\n",
    "    patch_psnrs.append(patch_psnr)\n",
    "    \n",
    "    # === Method 2: Full pipeline (embed in larger image) ===\n",
    "    # Create 512x512 image with patch in center\n",
    "    full_size = 512\n",
    "    full_image = np.zeros((full_size, full_size), dtype=np.float32)\n",
    "    offset = (full_size - patch_size) // 2\n",
    "    full_image[offset:offset+patch_size, offset:offset+patch_size] = patch\n",
    "    \n",
    "    # Convert to linear SAR for full pipeline\n",
    "    vmin, vmax = compressor.preprocess_params['vmin'], compressor.preprocess_params['vmax']\n",
    "    full_db = full_image * (vmax - vmin) + vmin\n",
    "    full_linear = np.power(10.0, full_db / 10.0).astype(np.float32)\n",
    "    \n",
    "    # Full pipeline\n",
    "    latent_full, metadata_full = compressor.compress(full_linear)\n",
    "    recon_linear = compressor.decompress(latent_full, metadata_full)\n",
    "    \n",
    "    # Convert back to normalized\n",
    "    recon_normalized = compressor.preprocess(recon_linear)\n",
    "    \n",
    "    # Extract the patch region\n",
    "    full_patch_recon = recon_normalized[offset:offset+patch_size, offset:offset+patch_size]\n",
    "    \n",
    "    full_psnr = SARMetrics.psnr(patch, full_patch_recon)\n",
    "    full_psnrs.append(full_psnr)\n",
    "    \n",
    "    diff = abs(patch_psnr - full_psnr)\n",
    "    differences.append(diff)\n",
    "    \n",
    "    print(f\"Patch {i+1}: Direct={patch_psnr:.2f} dB, Full={full_psnr:.2f} dB, Diff={diff:.3f} dB\")\n",
    "\n",
    "# Statistics\n",
    "mean_diff = np.mean(differences)\n",
    "max_diff = np.max(differences)\n",
    "mean_patch_psnr = np.mean(patch_psnrs)\n",
    "mean_full_psnr = np.mean(full_psnrs)\n",
    "\n",
    "print(f\"\\n--- Statistics ---\")\n",
    "print(f\"Mean PSNR (direct): {mean_patch_psnr:.2f} dB\")\n",
    "print(f\"Mean PSNR (full pipeline): {mean_full_psnr:.2f} dB\")\n",
    "print(f\"Mean difference: {mean_diff:.3f} dB\")\n",
    "print(f\"Max difference: {max_diff:.3f} dB\")\n",
    "\n",
    "# Success criterion: difference < 0.5 dB\n",
    "psnr_consistent = max_diff < 0.5\n",
    "\n",
    "if psnr_consistent:\n",
    "    print(f\"\\n[PASSED] Test 3: PSNR consistent (max diff {max_diff:.3f} dB < 0.5 dB)\")\n",
    "else:\n",
    "    print(f\"\\n[FAILED] Test 3: PSNR inconsistent (max diff {max_diff:.3f} dB >= 0.5 dB)\")\n",
    "\n",
    "test3_result = {\n",
    "    \"passed\": psnr_consistent,\n",
    "    \"mean_patch_psnr_db\": mean_patch_psnr,\n",
    "    \"mean_full_psnr_db\": mean_full_psnr,\n",
    "    \"mean_difference_db\": mean_diff,\n",
    "    \"max_difference_db\": max_diff,\n",
    "    \"n_patches\": n_patches,\n",
    "    \"patch_psnrs\": patch_psnrs,\n",
    "    \"full_psnrs\": full_psnrs\n",
    "}\n",
    "results[\"tests\"][\"psnr_consistency\"] = test3_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Preprocessing Round-Trip\n",
    "\n",
    "Verify that inverse preprocessing correctly restores linear SAR values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Test 4: Preprocessing Round-Trip\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create synthetic SAR-like data with exponential distribution\n",
    "# Real SAR intensity follows approximately exponential/Rayleigh distribution\n",
    "np.random.seed(123)\n",
    "size = 512\n",
    "\n",
    "# SAR intensity ~ exponential, but we want values in typical range\n",
    "# Based on preprocess params: vmin=14.77 dB, vmax=24.54 dB\n",
    "# Linear range: 10^1.477 = 30 to 10^2.454 = 284\n",
    "vmin, vmax = compressor.preprocess_params['vmin'], compressor.preprocess_params['vmax']\n",
    "linear_min = np.power(10.0, vmin / 10.0)\n",
    "linear_max = np.power(10.0, vmax / 10.0)\n",
    "\n",
    "print(f\"Expected linear SAR range: [{linear_min:.1f}, {linear_max:.1f}]\")\n",
    "\n",
    "# Create realistic SAR-like data\n",
    "# Base structure + multiplicative speckle noise\n",
    "y, x = np.mgrid[0:size, 0:size].astype(np.float32)\n",
    "base_signal = linear_min + (linear_max - linear_min) * (\n",
    "    0.3 + 0.4 * np.sin(2 * np.pi * x / 100) * np.sin(2 * np.pi * y / 100)\n",
    ")\n",
    "# Add multiplicative noise (simulates speckle)\n",
    "speckle = np.random.exponential(1.0, (size, size)).astype(np.float32)\n",
    "sar_linear = base_signal * (0.5 + 0.5 * speckle)\n",
    "sar_linear = np.clip(sar_linear, 1e-6, None).astype(np.float32)  # Ensure positive\n",
    "\n",
    "print(f\"\\nInput SAR data:\")\n",
    "print(f\"  Shape: {sar_linear.shape}\")\n",
    "print(f\"  Range: [{sar_linear.min():.1f}, {sar_linear.max():.1f}]\")\n",
    "print(f\"  Mean: {sar_linear.mean():.1f}\")\n",
    "\n",
    "# Full pipeline: raw -> preprocess -> compress -> decompress -> inverse_preprocess\n",
    "print(\"\\nRunning full pipeline...\")\n",
    "latent, metadata = compressor.compress(sar_linear)\n",
    "reconstructed_linear = compressor.decompress(latent, metadata)\n",
    "\n",
    "print(f\"\\nOutput SAR data:\")\n",
    "print(f\"  Shape: {reconstructed_linear.shape}\")\n",
    "print(f\"  Range: [{reconstructed_linear.min():.1f}, {reconstructed_linear.max():.1f}]\")\n",
    "print(f\"  Mean: {reconstructed_linear.mean():.1f}\")\n",
    "\n",
    "# Check that output is in similar range as input (not normalized [0,1])\n",
    "output_in_linear_range = reconstructed_linear.max() > 10 and reconstructed_linear.min() > 1\n",
    "print(f\"\\nOutput in linear SAR range (not [0,1]): {output_in_linear_range}\")\n",
    "\n",
    "# Compute correlation between original and reconstructed\n",
    "correlation = np.corrcoef(sar_linear.flatten(), reconstructed_linear.flatten())[0, 1]\n",
    "print(f\"Correlation (original vs reconstructed): {correlation:.4f}\")\n",
    "\n",
    "# Compare in dB domain for fair PSNR\n",
    "orig_db = 10 * np.log10(np.clip(sar_linear, 1e-10, None))\n",
    "recon_db = 10 * np.log10(np.clip(reconstructed_linear, 1e-10, None))\n",
    "\n",
    "# Normalize both to [0, 1] for PSNR calculation\n",
    "orig_norm = np.clip((orig_db - vmin) / (vmax - vmin), 0, 1)\n",
    "recon_norm = np.clip((recon_db - vmin) / (vmax - vmin), 0, 1)\n",
    "\n",
    "psnr_db = SARMetrics.psnr(orig_norm, recon_norm)\n",
    "print(f\"PSNR (normalized dB domain): {psnr_db:.2f} dB\")\n",
    "\n",
    "# Success criteria:\n",
    "# 1. Output is in linear range (not [0,1])\n",
    "# 2. Correlation > 0.9\n",
    "test4_passed = output_in_linear_range and correlation > 0.9\n",
    "\n",
    "if test4_passed:\n",
    "    print(f\"\\n[PASSED] Test 4: Preprocessing round-trip correct\")\n",
    "else:\n",
    "    print(f\"\\n[FAILED] Test 4: Preprocessing round-trip issues\")\n",
    "\n",
    "test4_result = {\n",
    "    \"passed\": test4_passed,\n",
    "    \"output_in_linear_range\": output_in_linear_range,\n",
    "    \"correlation\": correlation,\n",
    "    \"psnr_db\": psnr_db,\n",
    "    \"input_range\": [float(sar_linear.min()), float(sar_linear.max())],\n",
    "    \"output_range\": [float(reconstructed_linear.min()), float(reconstructed_linear.max())],\n",
    "    \"expected_linear_range\": [linear_min, linear_max]\n",
    "}\n",
    "results[\"tests\"][\"preprocessing_roundtrip\"] = test4_result\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original (log scale for visualization)\n",
    "im0 = axes[0].imshow(10*np.log10(np.clip(sar_linear, 1e-10, None)), cmap='gray')\n",
    "axes[0].set_title('Original SAR (dB)')\n",
    "plt.colorbar(im0, ax=axes[0], label='dB')\n",
    "\n",
    "# Reconstructed (log scale)\n",
    "im1 = axes[1].imshow(10*np.log10(np.clip(reconstructed_linear, 1e-10, None)), cmap='gray')\n",
    "axes[1].set_title('Reconstructed SAR (dB)')\n",
    "plt.colorbar(im1, ax=axes[1], label='dB')\n",
    "\n",
    "# Scatter plot\n",
    "sample_idx = np.random.choice(sar_linear.size, 10000, replace=False)\n",
    "axes[2].scatter(sar_linear.flatten()[sample_idx], reconstructed_linear.flatten()[sample_idx], \n",
    "                alpha=0.3, s=1)\n",
    "axes[2].plot([0, 500], [0, 500], 'r--', label='y=x')\n",
    "axes[2].set_xlabel('Original (linear)')\n",
    "axes[2].set_ylabel('Reconstructed (linear)')\n",
    "axes[2].set_title(f'Correlation: {correlation:.4f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'preprocessing_roundtrip_test.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: CLI Smoke Test\n",
    "\n",
    "Test the command-line interface for compress/decompress with metadata preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Test 5: CLI Smoke Test\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.transform import Affine\n",
    "\n",
    "# Create temp directory for test files\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmpdir = Path(tmpdir)\n",
    "    \n",
    "    # Create synthetic GeoTIFF with mock CRS\n",
    "    size = 512\n",
    "    np.random.seed(456)\n",
    "    \n",
    "    # Create SAR-like data in linear range\n",
    "    y, x = np.mgrid[0:size, 0:size].astype(np.float32)\n",
    "    test_data = 50 + 150 * (0.5 + 0.3 * np.sin(2*np.pi*x/100) * np.cos(2*np.pi*y/100))\n",
    "    test_data = test_data.astype(np.float32)\n",
    "    \n",
    "    # Create mock metadata with WGS84 CRS\n",
    "    test_metadata = GeoMetadata(\n",
    "        crs=CRS.from_epsg(4326),\n",
    "        transform=Affine(0.0001, 0, 10.0, 0, -0.0001, 50.0),\n",
    "        nodata=None,\n",
    "        dtype='float32',\n",
    "        count=1,\n",
    "        width=size,\n",
    "        height=size,\n",
    "        tags={'source': 'test', 'description': 'CLI smoke test'},\n",
    "        descriptions=('SAR Intensity',)\n",
    "    )\n",
    "    \n",
    "    # Write test GeoTIFF\n",
    "    input_tif = tmpdir / 'test_input.tif'\n",
    "    compressed_npz = tmpdir / 'test_compressed.npz'\n",
    "    output_tif = tmpdir / 'test_output.tif'\n",
    "    \n",
    "    write_geotiff(test_data, test_metadata, input_tif)\n",
    "    print(f\"Created test GeoTIFF: {input_tif}\")\n",
    "    print(f\"  Size: {test_data.shape}\")\n",
    "    print(f\"  CRS: {test_metadata.crs}\")\n",
    "    print(f\"  Transform: {test_metadata.transform}\")\n",
    "    \n",
    "    # Run CLI compress\n",
    "    print(\"\\nRunning CLI compress...\")\n",
    "    cli_path = Path.cwd().parent / 'scripts' / 'sarcodec.py'\n",
    "    compress_cmd = [\n",
    "        sys.executable, str(cli_path), 'compress',\n",
    "        str(input_tif), '-o', str(compressed_npz)\n",
    "    ]\n",
    "    print(f\"  Command: {' '.join(compress_cmd)}\")\n",
    "    \n",
    "    result = subprocess.run(compress_cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"  STDERR: {result.stderr}\")\n",
    "        compress_success = False\n",
    "    else:\n",
    "        print(f\"  Compress exit code: {result.returncode}\")\n",
    "        print(f\"  Output exists: {compressed_npz.exists()}\")\n",
    "        compress_success = compressed_npz.exists()\n",
    "    \n",
    "    # Run CLI decompress\n",
    "    if compress_success:\n",
    "        print(\"\\nRunning CLI decompress...\")\n",
    "        decompress_cmd = [\n",
    "            sys.executable, str(cli_path), 'decompress',\n",
    "            str(compressed_npz), '-o', str(output_tif)\n",
    "        ]\n",
    "        print(f\"  Command: {' '.join(decompress_cmd)}\")\n",
    "        \n",
    "        result = subprocess.run(decompress_cmd, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"  STDERR: {result.stderr}\")\n",
    "            decompress_success = False\n",
    "        else:\n",
    "            print(f\"  Decompress exit code: {result.returncode}\")\n",
    "            print(f\"  Output exists: {output_tif.exists()}\")\n",
    "            decompress_success = output_tif.exists()\n",
    "    else:\n",
    "        decompress_success = False\n",
    "    \n",
    "    # Verify metadata preservation\n",
    "    metadata_preserved = False\n",
    "    if decompress_success:\n",
    "        print(\"\\nVerifying metadata preservation...\")\n",
    "        output_data, output_metadata = read_geotiff(output_tif)\n",
    "        \n",
    "        # Check CRS\n",
    "        crs_match = output_metadata.crs == test_metadata.crs\n",
    "        print(f\"  CRS preserved: {crs_match}\")\n",
    "        if not crs_match:\n",
    "            print(f\"    Expected: {test_metadata.crs}\")\n",
    "            print(f\"    Got: {output_metadata.crs}\")\n",
    "        \n",
    "        # Check transform\n",
    "        transform_match = output_metadata.transform == test_metadata.transform\n",
    "        print(f\"  Transform preserved: {transform_match}\")\n",
    "        \n",
    "        # Check shape\n",
    "        shape_match = output_data.shape == test_data.shape\n",
    "        print(f\"  Shape preserved: {shape_match} ({output_data.shape})\")\n",
    "        \n",
    "        # Check tags\n",
    "        tags_match = 'source' in output_metadata.tags\n",
    "        print(f\"  Tags preserved: {tags_match}\")\n",
    "        \n",
    "        metadata_preserved = crs_match and transform_match and shape_match\n",
    "        \n",
    "        # Calculate quality\n",
    "        correlation = np.corrcoef(test_data.flatten(), output_data.flatten())[0, 1]\n",
    "        print(f\"\\n  Data correlation: {correlation:.4f}\")\n",
    "    \n",
    "    test5_passed = compress_success and decompress_success and metadata_preserved\n",
    "    \n",
    "    if test5_passed:\n",
    "        print(f\"\\n[PASSED] Test 5: CLI round-trip works, metadata preserved\")\n",
    "    else:\n",
    "        print(f\"\\n[FAILED] Test 5: CLI round-trip issues\")\n",
    "        print(f\"  Compress success: {compress_success}\")\n",
    "        print(f\"  Decompress success: {decompress_success}\")\n",
    "        print(f\"  Metadata preserved: {metadata_preserved}\")\n",
    "\n",
    "test5_result = {\n",
    "    \"passed\": test5_passed,\n",
    "    \"compress_success\": compress_success,\n",
    "    \"decompress_success\": decompress_success,\n",
    "    \"metadata_preserved\": metadata_preserved,\n",
    "    \"crs_preserved\": crs_match if decompress_success else False,\n",
    "    \"transform_preserved\": transform_match if decompress_success else False,\n",
    "    \"data_correlation\": float(correlation) if decompress_success else None\n",
    "}\n",
    "results[\"tests\"][\"cli_smoke_test\"] = test5_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FULL INFERENCE VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compile results\n",
    "all_passed = all(t.get(\"passed\", False) for t in results[\"tests\"].values())\n",
    "results[\"all_passed\"] = all_passed\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n{:<40} {:<10}\".format(\"Test\", \"Status\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "test_names = {\n",
    "    \"memory_test\": \"1. Memory Test (4096x4096)\",\n",
    "    \"seamless_blending\": \"2. Seamless Blending\",\n",
    "    \"psnr_consistency\": \"3. PSNR Consistency (<0.5 dB diff)\",\n",
    "    \"preprocessing_roundtrip\": \"4. Preprocessing Round-Trip\",\n",
    "    \"cli_smoke_test\": \"5. CLI Smoke Test\"\n",
    "}\n",
    "\n",
    "for test_key, test_name in test_names.items():\n",
    "    if test_key in results[\"tests\"]:\n",
    "        status = \"PASSED\" if results[\"tests\"][test_key][\"passed\"] else \"FAILED\"\n",
    "        print(f\"{test_name:<40} {status:<10}\")\n",
    "    else:\n",
    "        print(f\"{test_name:<40} {'NOT RUN':<10}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "overall = \"ALL PASSED\" if all_passed else \"SOME FAILED\"\n",
    "print(f\"{'OVERALL':<40} {overall:<10}\")\n",
    "\n",
    "# Key metrics summary\n",
    "print(\"\\n--- Key Metrics ---\")\n",
    "if \"memory_test\" in results[\"tests\"] and results[\"tests\"][\"memory_test\"][\"passed\"]:\n",
    "    mt = results[\"tests\"][\"memory_test\"]\n",
    "    print(f\"4096x4096 processing time: {mt['total_time_sec']:.1f}s\")\n",
    "    print(f\"Peak GPU memory: {mt['peak_gpu_memory_mb']:.0f} MB\")\n",
    "    print(f\"Tiles processed: {mt['n_tiles']}\")\n",
    "\n",
    "if \"seamless_blending\" in results[\"tests\"]:\n",
    "    sb = results[\"tests\"][\"seamless_blending\"]\n",
    "    print(f\"Boundary error ratio: {sb['boundary_error_ratio']:.3f}\")\n",
    "\n",
    "if \"psnr_consistency\" in results[\"tests\"]:\n",
    "    pc = results[\"tests\"][\"psnr_consistency\"]\n",
    "    print(f\"PSNR difference (max): {pc['max_difference_db']:.3f} dB\")\n",
    "\n",
    "# Save results to JSON\n",
    "results_path = RESULTS_DIR / 'full_inference_results.json'\n",
    "\n",
    "# Convert numpy types for JSON serialization\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "results_json = convert_numpy(results)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "\n",
    "# Final status\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_passed:\n",
    "    print(\"VALIDATION COMPLETE: All Phase 5 criteria met!\")\n",
    "else:\n",
    "    print(\"VALIDATION COMPLETE: Some tests failed - review results above\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
