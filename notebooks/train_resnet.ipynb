{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ResNet Autoencoder Training\n",
    "\n",
    "Train ResNet autoencoder at 4x and 8x compression ratios to complete the rate-distortion curve.\n",
    "\n",
    "**Note:** 16x already trained in sweep_all_16x.ipynb (21.13 dB PSNR, 0.739 SSIM)\n",
    "\n",
    "**Proven hyperparameters** (from sweep_all_16x):\n",
    "- LR=1e-4, AdamW, ReduceLROnPlateau\n",
    "- base_channels=64 (22.4M params)\n",
    "- 35 epochs, patience=12\n",
    "\n",
    "**Monitor with TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "from src.data.datamodule import SARDataModule\n",
    "from src.models.resnet_autoencoder import ResNetAutoencoder\n",
    "from src.losses.combined import CombinedLoss\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Training Modes:**\n",
    "- `SINGLE_RATIO`: Train at one specific compression ratio (default: 16x)\n",
    "- `MULTI_RATIO`: Train at 4x, 8x, 16x for rate-distortion curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODE: MULTI-RATIO (4x, 8x)\n",
      "============================================================\n",
      "Model: ResNetAutoencoder (base_channels=64)\n",
      "Training: 10% data, 35 epochs, batch=16\n",
      "Optimizer: ADAMW, LR=0.0001, scheduler=plateau\n",
      "Loss: 0.5 MSE + 0.5 SSIM\n",
      "\n",
      "Runs planned:\n",
      "  resnet_c64_b64_cr4x\n",
      "  resnet_c32_b64_cr8x\n",
      "\n",
      "Note: 16x already trained in sweep_all_16x.ipynb\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - ResNet Autoencoder (Proven Parameters)\n",
    "# ============================================================\n",
    "\n",
    "# =========================\n",
    "# TRAINING MODE\n",
    "# =========================\n",
    "MULTI_RATIO = True   # True = train 4x, 8x | False = single ratio only\n",
    "TARGET_RATIO = 8     # Used when MULTI_RATIO=False\n",
    "\n",
    "# Data settings\n",
    "DATA_PATH = \"D:/Projects/CNNAutoencoderProject/data/patches/metadata.npy\"\n",
    "BATCH_SIZE = 16       # ResNet b=64 needs smaller batch (OOM at 32+)\n",
    "NUM_WORKERS = 4       # Parallel data loading\n",
    "VAL_FRACTION = 0.1    # 10% validation split\n",
    "TRAIN_SUBSET = 0.10   # Use 10% of data for faster iteration\n",
    "\n",
    "# Model settings - PROVEN from sweep_all_16x\n",
    "BASE_CHANNELS = 64    # Full ResNet (22.4M params) - best results\n",
    "\n",
    "# Loss settings\n",
    "MSE_WEIGHT = 0.5\n",
    "SSIM_WEIGHT = 0.5\n",
    "\n",
    "# Training settings - PROVEN from sweep_all_16x\n",
    "EPOCHS = 35\n",
    "LEARNING_RATE = 1e-4          # Proven optimal\n",
    "OPTIMIZER = 'adamw'           # AdamW > Adam\n",
    "SCHEDULER = 'plateau'         # ReduceLROnPlateau\n",
    "EARLY_STOPPING_PATIENCE = 12\n",
    "LR_PATIENCE = 10\n",
    "LR_FACTOR = 0.5\n",
    "MAX_GRAD_NORM = 1.0\n",
    "USE_AMP = True                # Mixed precision\n",
    "\n",
    "# Compression ratios to train (16x already done in sweep_all_16x)\n",
    "if MULTI_RATIO:\n",
    "    # latent_channels: 64->4x, 32->8x\n",
    "    SWEEP_CONFIGS = [\n",
    "        {'latent_channels': 64, 'ratio': 4},\n",
    "        {'latent_channels': 32, 'ratio': 8},\n",
    "    ]\n",
    "else:\n",
    "    # Single ratio training\n",
    "    lc = {4: 64, 8: 32, 16: 16}.get(TARGET_RATIO, 32)\n",
    "    SWEEP_CONFIGS = [{'latent_channels': lc, 'ratio': TARGET_RATIO}]\n",
    "\n",
    "# Results folder\n",
    "RESULTS_DIR = Path('results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"MODE: {'MULTI-RATIO (4x, 8x)' if MULTI_RATIO else f'SINGLE RATIO ({TARGET_RATIO}x)'}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: ResNetAutoencoder (base_channels={BASE_CHANNELS})\")\n",
    "print(f\"Training: {TRAIN_SUBSET*100:.0f}% data, {EPOCHS} epochs, batch={BATCH_SIZE}\")\n",
    "print(f\"Optimizer: {OPTIMIZER.upper()}, LR={LEARNING_RATE}, scheduler={SCHEDULER}\")\n",
    "print(f\"Loss: {MSE_WEIGHT} MSE + {SSIM_WEIGHT} SSIM\")\n",
    "print(f\"\\nRuns planned:\")\n",
    "for cfg in SWEEP_CONFIGS:\n",
    "    name = f\"resnet_c{cfg['latent_channels']}_b{BASE_CHANNELS}_cr{cfg['ratio']}x\"\n",
    "    print(f\"  {name}\")\n",
    "print(f\"\\nNote: 16x already trained in sweep_all_16x.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading metadata from D:\\Projects\\CNNAutoencoderProject\\data\\patches\\metadata.npy\n",
      "Total patches: 696277\n",
      "Train: 626650, Val: 69627\n",
      "Using 10% subset:\n",
      "  Train: 62,665 of 626,650\n",
      "  Val: 6,962 of 69,627\n",
      "\n",
      "Preprocessing params: {'vmin': np.float32(14.768799), 'vmax': np.float32(24.54073)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "dm = SARDataModule(\n",
    "    patches_path=DATA_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    val_fraction=VAL_FRACTION,\n",
    ")\n",
    "\n",
    "if TRAIN_SUBSET < 1.0:\n",
    "    full_train_size = len(dm.train_dataset)\n",
    "    train_subset_size = int(full_train_size * TRAIN_SUBSET)\n",
    "    train_indices = random.sample(range(full_train_size), train_subset_size)\n",
    "    dm.train_dataset = torch.utils.data.Subset(dm.train_dataset, train_indices)\n",
    "\n",
    "    full_val_size = len(dm.val_dataset)\n",
    "    val_subset_size = int(full_val_size * TRAIN_SUBSET)\n",
    "    val_indices = random.sample(range(full_val_size), val_subset_size)\n",
    "    dm.val_dataset = torch.utils.data.Subset(dm.val_dataset, val_indices)\n",
    "\n",
    "    print(f\"Using {TRAIN_SUBSET*100:.0f}% subset:\")\n",
    "    print(f\"  Train: {train_subset_size:,} of {full_train_size:,}\")\n",
    "    print(f\"  Val: {val_subset_size:,} of {full_val_size:,}\")\n",
    "\n",
    "print(f\"\\nPreprocessing params: {dm.preprocessing_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Verify a sample batch\nsample_batch = next(iter(dm.train_dataloader()))\nprint(f\"Sample batch shape: {sample_batch.shape}\")\nprint(f\"Sample batch dtype: {sample_batch.dtype}\")\nprint(f\"Sample batch range: [{sample_batch.min():.4f}, {sample_batch.max():.4f}]\")"
  },
  {
   "cell_type": "markdown",
   "id": "yb1aczj1dh",
   "source": "## 4. Find Optimal Learning Rate (Optional)\n\nRun a learning rate range test to find the optimal LR. This sweeps from 1e-7 to 1 and plots loss vs LR.\n\n**Skip this if using proven LR (1e-4 from sweep_all_16x).**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "03a8eywd3ksa",
   "source": "# LR Finder - Run this to find optimal learning rate for a new architecture\n# Skip if using proven LR (1e-4)\n\nRUN_LR_FINDER = False  # Set to True to run LR finder\n\nif RUN_LR_FINDER:\n    from tqdm import tqdm\n    \n    def find_lr_minimal(model, sample_batch, loss_fn, device='cuda',\n                        start_lr=1e-7, end_lr=1, num_iter=100):\n        \"\"\"\n        Minimal LR range test. Sweeps LR and records smoothed loss.\n        Returns (lrs, losses) arrays for plotting.\n        \"\"\"\n        print(\"Saving initial state...\")\n        initial_state = {k: v.clone() for k, v in model.state_dict().items()}\n        \n        print(\"Moving model to device...\")\n        model.train()\n        model.to(device)\n        \n        print(\"Moving batch to device...\")\n        batch = sample_batch.to(device)\n        print(f\"Batch on device: {batch.device}, shape: {batch.shape}\")\n        \n        print(\"Creating optimizer...\")\n        optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, weight_decay=1e-4)\n        gamma = (end_lr / start_lr) ** (1 / num_iter)\n        \n        lrs, losses = [], []\n        smoothed_loss = None\n        best_loss = float('inf')\n        \n        print(\"Starting LR sweep...\")\n        for i in tqdm(range(num_iter), desc=\"LR sweep\"):\n            optimizer.zero_grad()\n            output, _ = model(batch)\n            loss, _ = loss_fn(output, batch)\n            \n            if torch.isnan(loss) or loss.item() > 4 * best_loss:\n                print(f\"\\nStopped at iter {i}: lr={optimizer.param_groups[0]['lr']:.2e}\")\n                break\n            \n            lrs.append(optimizer.param_groups[0]['lr'])\n            \n            if smoothed_loss is None:\n                smoothed_loss = loss.item()\n            else:\n                smoothed_loss = 0.1 * loss.item() + 0.9 * smoothed_loss\n            losses.append(smoothed_loss)\n            best_loss = min(best_loss, smoothed_loss)\n            \n            loss.backward()\n            optimizer.step()\n            \n            for pg in optimizer.param_groups:\n                pg['lr'] *= gamma\n        \n        print(\"Restoring initial state...\")\n        model.load_state_dict(initial_state)\n        return np.array(lrs), np.array(losses)\n    \n    # Create fresh model for LR finder\n    lr_model = ResNetAutoencoder(\n        latent_channels=SWEEP_CONFIGS[0]['latent_channels'],\n        base_channels=BASE_CHANNELS,\n        in_channels=1,\n    )\n    lr_loss_fn = CombinedLoss(mse_weight=MSE_WEIGHT, ssim_weight=SSIM_WEIGHT)\n    \n    # Run LR finder with small batch\n    lrs, losses = find_lr_minimal(\n        model=lr_model,\n        sample_batch=sample_batch[:4],  # Just 4 samples\n        loss_fn=lr_loss_fn,\n        num_iter=100\n    )\n    \n    # Plot results\n    plt.figure(figsize=(10, 6))\n    plt.semilogx(lrs, losses)\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Loss (smoothed)')\n    plt.title('LR Range Test - ResNet Autoencoder')\n    plt.axvline(x=1e-4, color='r', linestyle='--', label='1e-4 (proven)')\n    plt.axvline(x=3e-4, color='g', linestyle='--', label='3e-4')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    # Find suggested LR (steepest descent)\n    if len(losses) > 10:\n        gradients = np.gradient(losses)\n        min_grad_idx = np.argmin(gradients)\n        suggested_lr = lrs[min_grad_idx]\n        print(f\"\\nSuggested LR (steepest descent): {suggested_lr:.2e}\")\n        print(f\"Proven LR from sweep: 1e-4\")\n    \n    del lr_model, lr_loss_fn\n    gc.collect()\n    torch.cuda.empty_cache()\nelse:\n    print(\"Skipping LR finder - using proven LR=1e-4 from sweep_all_16x\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Train Model(s)\n",
    "\n",
    "Trains at all configured compression ratios. Skips existing checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1191e22f4b894833a9daa4fde98fe562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/2 [00:00<?, ?run/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "  resnet_c64_b64_cr4x | 4x compression\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2026-01-28 18:04:24,472 - Log directory: runs\\resnet_c64_b64_cr4x_20260128_180424\n",
      "2026-01-28 18:04:24,473 - Checkpoint directory: checkpoints\\resnet_c64_b64_cr4x_20260128_180424\n",
      "2026-01-28 18:04:24,474 - Mixed Precision (AMP): enabled\n",
      "2026-01-28 18:04:24,474 - Starting training for 35 epochs\n",
      "2026-01-28 18:04:24,475 - Model: ResNetAutoencoder\n",
      "2026-01-28 18:04:24,475 - Config: {'learning_rate': 0.0001, 'optimizer': 'adamw', 'scheduler': 'plateau', 'lr_patience': 10, 'lr_factor': 0.5, 'max_grad_norm': 1.0, 'use_amp': True, 'notebook': True, 'run_name': 'resnet_c64_b64_cr4x', 'preprocessing_params': {'vmin': np.float32(14.768799), 'vmax': np.float32(24.54073)}, 'model_type': 'resnet', 'latent_channels': 64, 'base_channels': 64, 'mse_weight': 0.5, 'ssim_weight': 0.5, 'batch_size': 16, 'compression_ratio': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters: 22,922,241\n",
      "Using device: cuda\n",
      "GPU memory: 1880MB / 8192MB (23% used, 6.2 GB free)\n",
      "Using AdamW optimizer with weight_decay=1e-05\n",
      "Using ReduceLROnPlateau: patience=10\n",
      "Mixed Precision (AMP) enabled - ~2x training speedup\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c647b7bd512445bba08771522187fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/3916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m    111\u001b[39m t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m history = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEARLY_STOPPING_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m elapsed = time.time() - t0\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Collect results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\src\\training\\trainer.py:514\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, epochs, early_stopping_patience)\u001b[39m\n\u001b[32m    511\u001b[39m in_warmup = \u001b[38;5;28mself\u001b[39m._update_warmup_lr(epoch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.warmup_epochs > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m train_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    517\u001b[39m val_metrics = \u001b[38;5;28mself\u001b[39m.validate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\src\\training\\trainer.py:239\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, enabled=\u001b[38;5;28mself\u001b[39m.use_amp):\n\u001b[32m    238\u001b[39m     x_hat, z = \u001b[38;5;28mself\u001b[39m.model(x)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     loss, metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Skip batch if loss is NaN (numerical instability protection)\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(loss) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(loss):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\src\\losses\\combined.py:80\u001b[39m, in \u001b[36mCombinedLoss.forward\u001b[39m\u001b[34m(self, x_hat, x)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03mCompute combined loss.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m \u001b[33;03m    metrics: Dict with 'loss', 'mse', 'ssim', 'psnr'\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m mse = \u001b[38;5;28mself\u001b[39m.mse_loss(x_hat, x)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m ssim_l = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssim_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Returns 1 - SSIM\u001b[39;00m\n\u001b[32m     82\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.mse_weight * mse + \u001b[38;5;28mself\u001b[39m.ssim_weight * ssim_l\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Compute metrics (detached for logging)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\src\\losses\\ssim.py:74\u001b[39m, in \u001b[36mSSIMLoss.forward\u001b[39m\u001b[34m(self, x_hat, x)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_hat: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     64\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03m    Compute SSIM loss (1 - SSIM).\u001b[39;00m\n\u001b[32m     66\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m \u001b[33;03m        1 - SSIM (scalar tensor, 0 when identical)\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     ssim_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssim_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m - ssim_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\pytorch_msssim\\ssim.py:278\u001b[39m, in \u001b[36mSSIM.forward\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: Tensor, Y: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnonnegative_ssim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnonnegative_ssim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\pytorch_msssim\\ssim.py:154\u001b[39m, in \u001b[36mssim\u001b[39m\u001b[34m(X, Y, data_range, size_average, win_size, win_sigma, win, K, nonnegative_ssim)\u001b[39m\n\u001b[32m    151\u001b[39m     win = _fspecial_gauss_1d(win_size, win_sigma)\n\u001b[32m    152\u001b[39m     win = win.repeat([X.shape[\u001b[32m1\u001b[39m]] + [\u001b[32m1\u001b[39m] * (\u001b[38;5;28mlen\u001b[39m(X.shape) - \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m ssim_per_channel, cs = \u001b[43m_ssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nonnegative_ssim:\n\u001b[32m    156\u001b[39m     ssim_per_channel = torch.relu(ssim_per_channel)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\CNNAutoencoderProject\\venv\\Lib\\site-packages\\pytorch_msssim\\ssim.py:84\u001b[39m, in \u001b[36m_ssim\u001b[39m\u001b[34m(X, Y, data_range, win, size_average, K)\u001b[39m\n\u001b[32m     81\u001b[39m C1 = (K1 * data_range) ** \u001b[32m2\u001b[39m\n\u001b[32m     82\u001b[39m C2 = (K2 * data_range) ** \u001b[32m2\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m win = \u001b[43mwin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m mu1 = gaussian_filter(X, win)\n\u001b[32m     87\u001b[39m mu2 = gaussian_filter(Y, win)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "results = []\n",
    "trainers = {}  # Store trainers for later analysis\n",
    "\n",
    "for cfg in tqdm(SWEEP_CONFIGS, desc=\"Training Progress\", unit=\"run\"):\n",
    "    lc = cfg['latent_channels']\n",
    "    ratio = cfg['ratio']\n",
    "    run_name = f\"resnet_c{lc}_b{BASE_CHANNELS}_cr{ratio}x\"\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    existing = glob.glob(f'checkpoints/{run_name}_*/best.pth')\n",
    "    if existing:\n",
    "        checkpoint_path = sorted(existing)[-1]  # Most recent\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"  SKIPPING {run_name} - checkpoint exists\")\n",
    "        print(f\"  {checkpoint_path}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Load checkpoint to extract metrics\n",
    "        ckpt = torch.load(checkpoint_path, weights_only=False)\n",
    "        \n",
    "        # Quick validation to get current metrics\n",
    "        model = ResNetAutoencoder(latent_channels=lc, base_channels=BASE_CHANNELS, in_channels=1)\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        model.eval().cuda()\n",
    "        params = model.count_parameters()\n",
    "        \n",
    "        loss_fn = CombinedLoss(mse_weight=MSE_WEIGHT, ssim_weight=SSIM_WEIGHT)\n",
    "        val_loader = dm.val_dataloader()\n",
    "        \n",
    "        val_losses, val_psnrs, val_ssims = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.cuda()\n",
    "                output, _ = model(batch)\n",
    "                loss, metrics = loss_fn(output, batch)\n",
    "                val_losses.append(loss.item())\n",
    "                val_psnrs.append(metrics['psnr'])\n",
    "                val_ssims.append(metrics['ssim'])\n",
    "        \n",
    "        result = {\n",
    "            'run_name': Path(checkpoint_path).parent.name,\n",
    "            'latent_channels': lc,\n",
    "            'compression_ratio': ratio,\n",
    "            'parameters': params['total'],\n",
    "            'epochs_trained': ckpt.get('epoch', 0) + 1,\n",
    "            'checkpoint': checkpoint_path,\n",
    "            'best_val_loss': sum(val_losses) / len(val_losses),\n",
    "            'best_psnr': sum(val_psnrs) / len(val_psnrs),\n",
    "            'best_ssim': sum(val_ssims) / len(val_ssims),\n",
    "            'skipped': True,\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(f\"  Validated: PSNR={result['best_psnr']:.2f} dB, SSIM={result['best_ssim']:.4f}\")\n",
    "        \n",
    "        del model, loss_fn, val_loader\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {run_name} | {ratio}x compression\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = ResNetAutoencoder(\n",
    "        latent_channels=lc,\n",
    "        base_channels=BASE_CHANNELS,\n",
    "        in_channels=1,\n",
    "    )\n",
    "    params = model.count_parameters()\n",
    "    print(f\"  Parameters: {params['total']:,}\")\n",
    "    \n",
    "    # Loss\n",
    "    loss_fn = CombinedLoss(mse_weight=MSE_WEIGHT, ssim_weight=SSIM_WEIGHT)\n",
    "    \n",
    "    # Trainer config\n",
    "    config = {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'optimizer': OPTIMIZER,\n",
    "        'scheduler': SCHEDULER,\n",
    "        'lr_patience': LR_PATIENCE,\n",
    "        'lr_factor': LR_FACTOR,\n",
    "        'max_grad_norm': MAX_GRAD_NORM,\n",
    "        'use_amp': USE_AMP,\n",
    "        'notebook': True,\n",
    "        'run_name': run_name,\n",
    "        'preprocessing_params': dm.preprocessing_params,\n",
    "        'model_type': 'resnet',\n",
    "        'latent_channels': lc,\n",
    "        'base_channels': BASE_CHANNELS,\n",
    "        'mse_weight': MSE_WEIGHT,\n",
    "        'ssim_weight': SSIM_WEIGHT,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'compression_ratio': ratio,\n",
    "    }\n",
    "    \n",
    "    train_loader = dm.train_dataloader()\n",
    "    val_loader = dm.val_dataloader()\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        loss_fn=loss_fn,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    t0 = time.time()\n",
    "    history = trainer.train(\n",
    "        epochs=EPOCHS,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    # Collect results\n",
    "    result = {\n",
    "        'run_name': trainer.run_name,\n",
    "        'latent_channels': lc,\n",
    "        'compression_ratio': ratio,\n",
    "        'parameters': params['total'],\n",
    "        'epochs_trained': len(history),\n",
    "        'elapsed_min': elapsed / 60,\n",
    "        'checkpoint': str(trainer.checkpoint_dir / 'best.pth'),\n",
    "        'log_dir': str(trainer.log_dir),\n",
    "        'skipped': False,\n",
    "    }\n",
    "    \n",
    "    if history:\n",
    "        best_epoch = min(history, key=lambda h: h.get('val_loss', float('inf')))\n",
    "        result['best_val_loss'] = best_epoch.get('val_loss')\n",
    "        result['best_psnr'] = best_epoch.get('val_psnr')\n",
    "        result['best_ssim'] = best_epoch.get('val_ssim')\n",
    "        result['history'] = history\n",
    "    \n",
    "    results.append(result)\n",
    "    trainers[run_name] = trainer\n",
    "    \n",
    "    psnr_str = f\"{result.get('best_psnr', 0):.2f} dB\" if result.get('best_psnr') else \"N/A\"\n",
    "    print(f\"\\n  Done: {psnr_str} | {elapsed/60:.1f} min\")\n",
    "    \n",
    "    # Cleanup GPU\n",
    "    del model, loss_fn, train_loader, val_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'Run':<45} {'Ratio':>6} {'Params':>10} {'PSNR':>10} {'SSIM':>10}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for r in results:\n",
    "    name = r['run_name']\n",
    "    ratio = f\"{r['compression_ratio']}x\"\n",
    "    params_str = f\"{r['parameters']/1e6:.1f}M\"\n",
    "    psnr = f\"{r['best_psnr']:.2f} dB\" if r.get('best_psnr') else \"N/A\"\n",
    "    ssim = f\"{r['best_ssim']:.4f}\" if r.get('best_ssim') else \"N/A\"\n",
    "    print(f\"{name:<45} {ratio:>6} {params_str:>10} {psnr:>10} {ssim:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for each run\n",
    "for r in results:\n",
    "    # Get history from result or load from checkpoint\n",
    "    if 'history' in r and r['history']:\n",
    "        history = r['history']\n",
    "    else:\n",
    "        # Load from checkpoint\n",
    "        ckpt = torch.load(r['checkpoint'], weights_only=False)\n",
    "        history = ckpt.get('history', [])\n",
    "    \n",
    "    if not history:\n",
    "        print(f\"No history available for {r['run_name']}\")\n",
    "        continue\n",
    "    \n",
    "    epochs = [h['epoch'] + 1 for h in history]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs, [h['train_loss'] for h in history], label='Train')\n",
    "    ax.plot(epochs, [h['val_loss'] for h in history], label='Val')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # PSNR\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs, [h['train_psnr'] for h in history], label='Train')\n",
    "    ax.plot(epochs, [h['val_psnr'] for h in history], label='Val')\n",
    "    ax.axhline(y=25, color='r', linestyle='--', label='Target (25 dB)')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('PSNR (dB)')\n",
    "    ax.set_title('PSNR')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # SSIM\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(epochs, [h['train_ssim'] for h in history], label='Train')\n",
    "    ax.plot(epochs, [h['val_ssim'] for h in history], label='Val')\n",
    "    ax.axhline(y=0.85, color='r', linestyle='--', label='Target (0.85)')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_title('SSIM')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(epochs, [h['learning_rate'] for h in history])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('Learning Rate Schedule')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.suptitle(f\"ResNet Training: {r['compression_ratio']}x Compression\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to results folder with clean model name (no timestamp)\n",
    "    clean_name = f\"resnet_c{r['latent_channels']}_b{BASE_CHANNELS}_cr{r['compression_ratio']}x\"\n",
    "    model_results_dir = RESULTS_DIR / clean_name\n",
    "    model_results_dir.mkdir(exist_ok=True)\n",
    "    save_path = model_results_dir / 'training_curves.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Checkpoint Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    checkpoint_path = r['checkpoint']\n",
    "    \n",
    "    if not Path(checkpoint_path).exists():\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "        continue\n",
    "    \n",
    "    ckpt = torch.load(checkpoint_path, weights_only=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Checkpoint: {r['run_name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Path: {checkpoint_path}\")\n",
    "    print(f\"  Keys: {list(ckpt.keys())}\")\n",
    "    print(f\"  Epoch: {ckpt.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Best val loss: {ckpt.get('best_val_loss', 'N/A'):.4f}\")\n",
    "    print(f\"  Preprocessing params: {ckpt.get('preprocessing_params', 'MISSING')}\")\n",
    "    \n",
    "    # Test loading into fresh model\n",
    "    test_model = ResNetAutoencoder(\n",
    "        latent_channels=r['latent_channels'],\n",
    "        base_channels=BASE_CHANNELS,\n",
    "        in_channels=1,\n",
    "    )\n",
    "    test_model.load_state_dict(ckpt['model_state_dict'])\n",
    "    test_model.eval()\n",
    "    \n",
    "    x = torch.rand(1, 1, 256, 256)\n",
    "    with torch.no_grad():\n",
    "        x_hat, z = test_model(x)\n",
    "    \n",
    "    print(f\"  Inference: {x.shape} -> {z.shape} -> {x_hat.shape}\")\n",
    "    print(f\"  Verification: PASS\")\n",
    "    \n",
    "    # Save checkpoint info to results with clean model name\n",
    "    clean_name = f\"resnet_c{r['latent_channels']}_b{BASE_CHANNELS}_cr{r['compression_ratio']}x\"\n",
    "    model_results_dir = RESULTS_DIR / clean_name\n",
    "    model_results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    info = {\n",
    "        'checkpoint_path': str(checkpoint_path),\n",
    "        'epoch': ckpt.get('epoch'),\n",
    "        'best_val_loss': ckpt.get('best_val_loss'),\n",
    "        'preprocessing_params': {k: float(v) for k, v in ckpt.get('preprocessing_params', {}).items()},\n",
    "        'config': ckpt.get('config', {}),\n",
    "    }\n",
    "    \n",
    "    with open(model_results_dir / 'checkpoint_info.json', 'w') as f:\n",
    "        json.dump(info, f, indent=2, default=str)\n",
    "    \n",
    "    del test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Sample Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    # Load model\n",
    "    ckpt = torch.load(r['checkpoint'], weights_only=False)\n",
    "    model = ResNetAutoencoder(\n",
    "        latent_channels=r['latent_channels'],\n",
    "        base_channels=BASE_CHANNELS,\n",
    "        in_channels=1,\n",
    "    )\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Get validation batch\n",
    "    val_batch = next(iter(dm.val_dataloader()))[:4].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstructed, latent = model(val_batch)\n",
    "    \n",
    "    originals = val_batch.cpu().numpy()\n",
    "    reconstructions = reconstructed.cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    \n",
    "    for i in range(4):\n",
    "        axes[0, i].imshow(originals[i, 0], cmap='gray')\n",
    "        axes[0, i].set_title(f'Original {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(reconstructions[i, 0], cmap='gray')\n",
    "        axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        diff = abs(originals[i, 0] - reconstructions[i, 0])\n",
    "        axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
    "        axes[2, i].set_title(f'Difference {i+1}')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Reconstructed', fontsize=12)\n",
    "    axes[2, 0].set_ylabel('Difference', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(f\"ResNet Reconstructions ({r['compression_ratio']}x, PSNR: {r['best_psnr']:.2f} dB)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with clean model name\n",
    "    clean_name = f\"resnet_c{r['latent_channels']}_b{BASE_CHANNELS}_cr{r['compression_ratio']}x\"\n",
    "    model_results_dir = RESULTS_DIR / clean_name\n",
    "    model_results_dir.mkdir(exist_ok=True)\n",
    "    save_path = model_results_dir / 'sample_reconstructions.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Rate-Distortion Curve (Multi-Ratio Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results) > 1:\n",
    "    # Sort by compression ratio\n",
    "    sorted_results = sorted(results, key=lambda x: x['compression_ratio'])\n",
    "    \n",
    "    ratios = [r['compression_ratio'] for r in sorted_results]\n",
    "    psnrs = [r['best_psnr'] for r in sorted_results]\n",
    "    ssims = [r['best_ssim'] for r in sorted_results]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # PSNR\n",
    "    ax1.plot(ratios, psnrs, 'go-', markersize=10, linewidth=2, label='ResNet')\n",
    "    for r_val, p in zip(ratios, psnrs):\n",
    "        ax1.annotate(f'{p:.1f}', (r_val, p), textcoords='offset points',\n",
    "                     xytext=(0, 10), ha='center', fontsize=9)\n",
    "    ax1.axhline(y=25, color='r', linestyle='--', alpha=0.5, label='Target (25 dB)')\n",
    "    ax1.set_xlabel('Compression Ratio', fontsize=12)\n",
    "    ax1.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "    ax1.set_title('Rate-Distortion: PSNR', fontsize=14)\n",
    "    ax1.set_xscale('log', base=2)\n",
    "    ax1.set_xticks(ratios)\n",
    "    ax1.set_xticklabels([f'{int(r)}x' for r in ratios])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.invert_xaxis()\n",
    "    \n",
    "    # SSIM\n",
    "    ax2.plot(ratios, ssims, 'go-', markersize=10, linewidth=2, label='ResNet')\n",
    "    for r_val, s in zip(ratios, ssims):\n",
    "        ax2.annotate(f'{s:.3f}', (r_val, s), textcoords='offset points',\n",
    "                     xytext=(0, 10), ha='center', fontsize=9)\n",
    "    ax2.axhline(y=0.85, color='r', linestyle='--', alpha=0.5, label='Target (0.85)')\n",
    "    ax2.set_xlabel('Compression Ratio', fontsize=12)\n",
    "    ax2.set_ylabel('SSIM', fontsize=12)\n",
    "    ax2.set_title('Rate-Distortion: SSIM', fontsize=14)\n",
    "    ax2.set_xscale('log', base=2)\n",
    "    ax2.set_xticks(ratios)\n",
    "    ax2.set_xticklabels([f'{int(r)}x' for r in ratios])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.invert_xaxis()\n",
    "    \n",
    "    plt.suptitle('ResNet Rate-Distortion Curves', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    datestamp = datetime.now().strftime('%Y%m%d')\n",
    "    save_path = RESULTS_DIR / f'resnet_rate_distortion_{datestamp}.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "else:\n",
    "    print(\"Single ratio mode - no R-D curve to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "datestamp = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "# Remove history from results for JSON (too large)\n",
    "results_for_json = []\n",
    "for r in results:\n",
    "    r_copy = {k: v for k, v in r.items() if k != 'history'}\n",
    "    results_for_json.append(r_copy)\n",
    "\n",
    "output = {\n",
    "    'sweep_type': 'resnet_training',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'model': 'resnet',\n",
    "        'base_channels': BASE_CHANNELS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'optimizer': OPTIMIZER,\n",
    "        'scheduler': SCHEDULER,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'train_subset': TRAIN_SUBSET,\n",
    "    },\n",
    "    'results': results_for_json,\n",
    "}\n",
    "\n",
    "output_path = RESULTS_DIR / f'resnet_training_{datestamp}.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 11. Comparison with Baseline\n",
    "\n",
    "Load baseline results and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results if available\n",
    "baseline_json = sorted(Path('runs').glob('sweep_baseline_ratios*.json'))\n",
    "if baseline_json:\n",
    "    with open(baseline_json[-1]) as f:\n",
    "        baseline_sweep = json.load(f)\n",
    "    baseline_results = baseline_sweep['results']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BASELINE vs RESNET COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Ratio':<8} {'Baseline PSNR':>15} {'ResNet PSNR':>15} {'Delta':>12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for r in results:\n",
    "        ratio = r['compression_ratio']\n",
    "        resnet_psnr = r['best_psnr']\n",
    "        \n",
    "        # Find matching baseline\n",
    "        bl = next((b for b in baseline_results if abs(b['compression_ratio'] - ratio) < 1), None)\n",
    "        if bl:\n",
    "            bl_psnr = bl['best_psnr']\n",
    "            delta = resnet_psnr - bl_psnr\n",
    "            print(f\"{ratio}x{'':<5} {bl_psnr:>15.2f} {resnet_psnr:>15.2f} {delta:>+12.2f}\")\n",
    "        else:\n",
    "            print(f\"{ratio}x{'':<5} {'N/A':>15} {resnet_psnr:>15.2f} {'-':>12}\")\n",
    "else:\n",
    "    print(\"No baseline results found. Run sweep_baseline_ratios.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Best & Worst Reconstructions\n",
    "\n",
    "Find the best and worst reconstruction cases to understand model strengths and failure modes."
   ],
   "id": "c4e54fdf"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Find best and worst reconstruction cases",
    "from src.evaluation.evaluator import Evaluator",
    "",
    "print(\"Finding best and worst reconstructions...\")",
    "",
    "# Create evaluator",
    "evaluator = Evaluator(model, device=device)",
    "",
    "# Find best and worst cases",
    "n_cases = 8",
    "worst_cases = evaluator.find_failure_cases(val_loader, n_worst=n_cases)",
    "best_cases = evaluator.find_best_cases(val_loader, n_best=n_cases)",
    "",
    "print(f\"",
    "Worst reconstructions (highest MSE):\")",
    "for i, case in enumerate(worst_cases[:5]):",
    "    print(f\"  {i+1}. MSE = {case['mse']:.6f}\")",
    "",
    "print(f\"",
    "Best reconstructions (lowest MSE):\")",
    "for i, case in enumerate(best_cases[:5]):",
    "    print(f\"  {i+1}. MSE = {case['mse']:.6f}\")",
    "",
    "# Plot best reconstructions",
    "fig, axes = plt.subplots(3, n_cases, figsize=(2.5 * n_cases, 8))",
    "fig.suptitle('Best Reconstructions (Top: Original, Middle: Reconstructed, Bottom: Difference)', fontsize=14)",
    "",
    "for i in range(min(n_cases, len(best_cases))):",
    "    orig = best_cases[i]['original'].numpy().squeeze()",
    "    recon = best_cases[i]['reconstructed'].numpy().squeeze()",
    "    diff = np.abs(orig - recon)",
    "    ",
    "    axes[0, i].imshow(orig, cmap='gray', vmin=0, vmax=1)",
    "    axes[0, i].axis('off')",
    "    axes[0, i].set_title(f\"MSE: {best_cases[i]['mse']:.4f}\", fontsize=8)",
    "    ",
    "    axes[1, i].imshow(recon, cmap='gray', vmin=0, vmax=1)",
    "    axes[1, i].axis('off')",
    "    ",
    "    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.2)",
    "    axes[2, i].axis('off')",
    "",
    "plt.tight_layout()",
    "plt.savefig(MODEL_RESULTS_DIR / 'best_reconstructions.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "# Plot worst reconstructions",
    "fig, axes = plt.subplots(3, n_cases, figsize=(2.5 * n_cases, 8))",
    "fig.suptitle('Worst Reconstructions (Failure Cases)', fontsize=14)",
    "",
    "for i in range(min(n_cases, len(worst_cases))):",
    "    orig = worst_cases[i]['original'].numpy().squeeze()",
    "    recon = worst_cases[i]['reconstructed'].numpy().squeeze()",
    "    diff = np.abs(orig - recon)",
    "    ",
    "    axes[0, i].imshow(orig, cmap='gray', vmin=0, vmax=1)",
    "    axes[0, i].axis('off')",
    "    axes[0, i].set_title(f\"MSE: {worst_cases[i]['mse']:.4f}\", fontsize=8)",
    "    ",
    "    axes[1, i].imshow(recon, cmap='gray', vmin=0, vmax=1)",
    "    axes[1, i].axis('off')",
    "    ",
    "    axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=0.3)",
    "    axes[2, i].axis('off')",
    "",
    "plt.tight_layout()",
    "plt.savefig(MODEL_RESULTS_DIR / 'worst_reconstructions.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "print(f\"",
    "Saved: {MODEL_RESULTS_DIR / 'best_reconstructions.png'}\")",
    "print(f\"Saved: {MODEL_RESULTS_DIR / 'worst_reconstructions.png'}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "b89e66b9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Latent Channel Visualization\n",
    "\n",
    "Visualize activations in the latent space to understand what the model encodes."
   ],
   "id": "2a0938da"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize latent channel activations",
    "print(\"Latent space visualization (first sample):\")",
    "",
    "with torch.no_grad():",
    "    sample = next(iter(val_loader))[:1].to(device)",
    "    _, latent = model(sample)",
    "    latent_np = latent[0].cpu().numpy()",
    "",
    "n_channels = min(16, latent_np.shape[0])",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))",
    "fig.suptitle(f'Latent Channel Activations ({latent_np.shape[1]}x{latent_np.shape[2]} spatial, {latent_np.shape[0]} channels)', fontsize=14)",
    "",
    "for i, ax in enumerate(axes.flatten()):",
    "    if i < n_channels:",
    "        channel = latent_np[i]",
    "        vmax = max(abs(channel.min()), abs(channel.max()), 0.1)",
    "        im = ax.imshow(channel, cmap='RdBu_r', vmin=-vmax, vmax=vmax)",
    "        ax.set_title(f'Ch {i}: std={channel.std():.2f}', fontsize=8)",
    "        ax.axis('off')",
    "    else:",
    "        ax.axis('off')",
    "",
    "plt.tight_layout()",
    "plt.savefig(MODEL_RESULTS_DIR / 'latent_channels.png', dpi=150, bbox_inches='tight')",
    "plt.show()",
    "",
    "# Compute latent statistics",
    "active_channels = sum(1 for i in range(latent_np.shape[0]) if latent_np[i].std() > 0.01)",
    "print(f\"",
    "Latent statistics:\")",
    "print(f\"  Shape: {latent_np.shape}\")",
    "print(f\"  Active channels (std > 0.01): {active_channels}/{latent_np.shape[0]}\")",
    "print(f\"  Mean activation: {latent_np.mean():.4f}\")",
    "print(f\"  Std activation: {latent_np.std():.4f}\")",
    "print(f\"",
    "Saved: {MODEL_RESULTS_DIR / 'latent_channels.png'}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "1cc7ce62"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done!\n",
    "\n",
    "**Results saved to:** `results/resnet_*/`\n",
    "\n",
    "**View in TensorBoard:**\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "1. Compare with baseline at same ratios\n",
    "2. Run full evaluation with SAR metrics\n",
    "3. Proceed to Phase 6 (Final Experiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}