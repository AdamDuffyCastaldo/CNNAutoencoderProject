---
phase: 04-architecture
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/models/attention_autoencoder.py
autonomous: true

must_haves:
  truths:
    - "AttentionAutoencoder input (B,1,256,256) produces output (B,1,256,256) and latent (B,16,16,16)"
    - "CBAM is applied after every residual block (16 CBAM modules total)"
    - "Forward pass works with batch_size=2 on GPU"
  artifacts:
    - path: "src/models/attention_autoencoder.py"
      provides: "AttentionEncoder, AttentionDecoder, AttentionAutoencoder"
      exports: ["AttentionAutoencoder"]
  key_links:
    - from: "src/models/attention_autoencoder.py"
      to: "src/models/blocks.py"
      via: "import PreActResidualBlock, CBAM"
      pattern: "from .blocks import.*CBAM"
---

<objective>
Implement Variant C: Pre-Activation Residual + CBAM Attention Autoencoder.

Purpose: This is the second enhanced architecture variant. It adds CBAM attention after every residual block, providing both channel and spatial attention for improved feature extraction. Target: +0.5 dB PSNR over Variant B (residual-only).

Output: New `src/models/attention_autoencoder.py` with working AttentionAutoencoder class.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-architecture/04-CONTEXT.md
@.planning/phases/04-architecture/04-RESEARCH.md
@.planning/phases/04-architecture/04-01-SUMMARY.md
@src/models/blocks.py
@src/models/resnet_autoencoder.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Attention Encoder and Decoder with CBAM</name>
  <files>src/models/attention_autoencoder.py</files>
  <action>
Create a new file `src/models/attention_autoencoder.py` with:

1. **ResidualBlockWithCBAM** helper class:
   - Combines PreActResidualBlock + CBAM sequentially
   - `__init__(self, in_channels, out_channels, stride=1, reduction=16, kernel_size=7)`
   - Forward: `x = residual_block(x)` then `x = cbam(x)`

2. **AttentionEncoder** class:
   - Same structure as PreActResidualEncoder but with CBAM after every block
   - `__init__(self, in_channels=1, base_channels=64, latent_channels=16, reduction=16)`
   - Architecture (per CONTEXT.md):
     - Stem: Conv2d(1, base_channels, 7x7, stride=1, padding=3) -> BN -> ReLU
     - Stage 1: ResidualBlockWithCBAM(base->base*2, stride=2) + ResidualBlockWithCBAM(base*2->base*2, stride=1)
     - Stage 2: ResidualBlockWithCBAM(base*2->base*4, stride=2) + ResidualBlockWithCBAM(base*4->base*4, stride=1)
     - Stage 3: ResidualBlockWithCBAM(base*4->base*8, stride=2) + ResidualBlockWithCBAM(base*8->base*8, stride=1)
     - Stage 4: ResidualBlockWithCBAM(base*8->latent, stride=2) + ResidualBlockWithCBAM(latent->latent, stride=1)
   - Total: 8 residual blocks + 8 CBAM modules

3. **AttentionDecoder** class:
   - Mirrors encoder with CBAM after every block
   - Use bilinear upsample + 1x1 conv at start of each stage, then 2 ResidualBlockWithCBAM
   - Final output: Conv2d(base, 1, 7x7, padding=3) -> Sigmoid

4. Implement Kaiming weight initialization

Key details from CONTEXT.md:
- CBAM after EVERY residual block (both encoder and decoder)
- Reduction ratio 16 for channel attention
- 7x7 kernel for spatial attention
- CBAM positioned AFTER residual block (not inside)
  </action>
  <verify>
Run in Python:
```python
import torch
from src.models.attention_autoencoder import AttentionEncoder, AttentionDecoder

# Test encoder
encoder = AttentionEncoder(in_channels=1, base_channels=64, latent_channels=16)
x = torch.randn(2, 1, 256, 256)
z = encoder(x)
assert z.shape == (2, 16, 16, 16), f"Encoder output shape wrong: {z.shape}"
print(f"Encoder: {x.shape} -> {z.shape}")

# Test decoder
decoder = AttentionDecoder(out_channels=1, base_channels=64, latent_channels=16)
x_hat = decoder(z)
assert x_hat.shape == (2, 1, 256, 256), f"Decoder output shape wrong: {x_hat.shape}"
print(f"Decoder: {z.shape} -> {x_hat.shape}")

# Verify output bounded
assert x_hat.min() >= 0 and x_hat.max() <= 1, "Output not bounded [0,1]"

# Count CBAM modules
cbam_count = sum(1 for m in encoder.modules() if m.__class__.__name__ == 'CBAM')
cbam_count += sum(1 for m in decoder.modules() if m.__class__.__name__ == 'CBAM')
print(f"Total CBAM modules: {cbam_count} (expected 16)")

print("Encoder/Decoder tests passed!")
```
  </verify>
  <done>AttentionEncoder compresses (B,1,256,256) to (B,16,16,16), AttentionDecoder reconstructs back. CBAM applied after every residual block (16 total CBAM modules across encoder+decoder).</done>
</task>

<task type="auto">
  <name>Task 2: Create AttentionAutoencoder Wrapper and Test</name>
  <files>src/models/attention_autoencoder.py</files>
  <action>
Add to `src/models/attention_autoencoder.py`:

1. **AttentionAutoencoder** class (wrapper):
   - `__init__(self, in_channels=1, base_channels=64, latent_channels=16, reduction=16)`
   - Compose AttentionEncoder + AttentionDecoder
   - Same interface as ResidualAutoencoder:
     - `forward(self, x)` -> (x_hat, z) tuple
     - `encode(self, x)` -> z
     - `decode(self, z)` -> x_hat
     - `get_compression_ratio(self)` -> float
     - `get_latent_size(self)` -> tuple
     - `count_parameters(self)` -> dict

2. Add test function `test_attention_autoencoder()`:
   - Test forward pass
   - Verify gradient flow
   - Print parameter count and compare to ResidualAutoencoder

3. Update `src/models/__init__.py` to export AttentionAutoencoder:
   - Add: `from .attention_autoencoder import AttentionAutoencoder`

4. Memory consideration:
   - CBAM adds parameters and memory overhead
   - Monitor GPU memory usage in test
   - If OOM with batch_size=32, document this for training adjustments
  </action>
  <verify>
Run in Python:
```python
import torch
from src.models import AttentionAutoencoder

# Test full autoencoder
model = AttentionAutoencoder(latent_channels=16, base_channels=64)
x = torch.randn(2, 1, 256, 256)
x_hat, z = model(x)

assert x_hat.shape == x.shape, f"Output shape mismatch: {x_hat.shape}"
assert z.shape == (2, 16, 16, 16), f"Latent shape wrong: {z.shape}"
print(f"Forward: {x.shape} -> {z.shape} -> {x_hat.shape}")

# Test properties
params = model.count_parameters()
print(f"Parameters: {params['total']:,} (encoder: {params['encoder']:,}, decoder: {params['decoder']:,})")
print(f"Compression ratio: {model.get_compression_ratio():.1f}x")

# Test gradient flow
model.train()
x = torch.randn(2, 1, 256, 256, requires_grad=True)
x_hat, z = model(x)
loss = torch.nn.functional.mse_loss(x_hat, torch.zeros_like(x_hat))
loss.backward()
print("Gradient flow: OK")

# Test on GPU if available
if torch.cuda.is_available():
    model_gpu = model.cuda()
    x_gpu = torch.randn(2, 1, 256, 256).cuda()
    x_hat_gpu, z_gpu = model_gpu(x_gpu)
    mem_gb = torch.cuda.memory_allocated() / 1024**3
    print(f"GPU test: OK (memory: {mem_gb:.2f} GB)")

    # Test with larger batch to estimate training capacity
    torch.cuda.empty_cache()
    try:
        x_batch = torch.randn(16, 1, 256, 256).cuda()
        x_hat_batch, _ = model_gpu(x_batch)
        print(f"Batch 16: OK (memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB)")
    except RuntimeError as e:
        if "out of memory" in str(e):
            print("Batch 16: OOM - may need smaller batch for training")
        else:
            raise

print("All AttentionAutoencoder tests passed!")
```
  </verify>
  <done>AttentionAutoencoder forward pass works, returns (x_hat, z) tuple, gradients flow correctly, all helper methods work. Model importable from src.models. GPU memory usage documented.</done>
</task>

</tasks>

<verification>
After both tasks complete:

1. Run model test: `python -c "from src.models.attention_autoencoder import test_attention_autoencoder; test_attention_autoencoder()"`
2. Verify import from package: `python -c "from src.models import AttentionAutoencoder; print('Import OK')"`
3. Compare parameter count to ResidualAutoencoder - CBAM adds ~small overhead
4. Note any memory constraints for training notebook
</verification>

<success_criteria>
1. AttentionAutoencoder accepts (B, 1, 256, 256) input and produces (B, 1, 256, 256) output
2. Latent representation shape is (B, 16, 16, 16) for 16x compression
3. CBAM is applied after every residual block (16 total across encoder+decoder)
4. Output bounded [0, 1] via Sigmoid
5. Gradients flow through entire network including CBAM
6. Model exportable from src.models package
7. GPU memory requirements documented
</success_criteria>

<output>
After completion, create `.planning/phases/04-architecture/04-03-SUMMARY.md`
</output>
