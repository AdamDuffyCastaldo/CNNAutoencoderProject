---
phase: 04-architecture
task: training-sweeps
total_tasks: 2
status: in_progress
last_updated: 2026-01-28
---

<current_state>
Baseline ratios sweep COMPLETE. Architecture comparison sweep (baseline vs ResNet @16x)
is currently RUNNING in `notebooks/sweep_all_16x.ipynb`. The baseline checkpoint was
detected and skipped; ResNet training is in progress (~22 min/epoch, 35 epochs).
Estimated completion: ~12 hours from start.
</current_state>

<completed_work>

## Previous Sessions
- All model architectures implemented and tested (baseline, resnet, residual, attention)
- Sweep infrastructure built (CLI script, YAML configs, notebooks)
- All configs standardized: LR=1e-4, AdamW, ReduceLROnPlateau

## This Session
1. Analyzed baseline ratios sweep results:
   - 4x: 24.15 dB / 0.855 SSIM (35 epochs, no early stop)
   - 8x: 21.34 dB / 0.675 SSIM (35 epochs, no early stop)
   - 12x: 19.48 dB / 0.595 SSIM (35 epochs, no early stop)
   - All models still improving at epoch 35 — undertrained
2. Updated `sweep_all_16x.yaml` and notebook to baseline + resnet only (dropped residual, attention)
3. Changed batch_size to 16 for ResNet (base_channels=64 = ~20M params, OOM at 36)
4. Added checkpoint skip logic to sweep loop (loads existing, runs quick validation, skips training)
5. Added combined R-D curve cell (merges baseline ratios + 16x results)
6. Added datestamps to all save paths (PNG, JSON)
7. Updated `sweep_baseline_ratios.ipynb` save paths with datestamps

</completed_work>

<remaining_work>

- Task 1 (baseline ratios sweep): COMPLETE
- Task 2 (architecture comparison): IN PROGRESS — ResNet training running overnight
- After sweeps complete:
  1. Analyze ResNet vs baseline results at 16x
  2. Check if ResNet loss was still declining at epoch 35
  3. Run combined R-D curve cell in sweep_all_16x.ipynb
  4. Pick best architecture
  5. Update STATE.md with final metrics
  6. Transition to Phase 6 (Final Experiments)

</remaining_work>

<decisions_made>

- Dropped residual and attention from sweep — attention failed catastrophically before, residual can wait
- batch_size=16 for ResNet b=64 — batch_size=36 caused GPU memory thrashing (4h50/epoch)
- 35 epochs / patience 12 to match baseline ratios sweep for direct comparison
- TRAIN_SUBSET=0.10 across all sweeps for consistency
- Loss weight ablation deferred — keep 0.5/0.5 MSE/SSIM until architecture is settled

</decisions_made>

<blockers>

- ResNet training is running and will take ~12 hours — must wait for completion
- tqdm progress bars still not rendering in VS Code Jupyter (minor UX, not blocking)

</blockers>

<context>
Phase 4 is at ~90% completion. The architecture implementations are done; we're in the
final training comparison stage. The baseline ratios sweep showed all models are undertrained
at 10%/35 epochs (none triggered early stopping, all still improving). The key question
for the overnight run: does ResNet with base_channels=64 and proper hyperparameters
(LR=1e-4, AdamW, plateau scheduler) beat the baseline at 16x?

Previous ResNet attempt regressed (19.09 dB vs baseline 20.47 dB) due to OneCycleLR,
base_channels=32, and LR too high. This run fixes all three issues.

After results are in, the plan is to pick the best architecture and move to Phase 6
(Final Experiments) — full evaluation with codec baselines, rate-distortion comparison.
Longer training with more data can happen in Phase 6 if needed.
</context>

<next_action>
1. Check if ResNet training completed: look at `notebooks/sweep_all_16x.ipynb` output
2. Run cells 10+ in the notebook to see results summary, architecture comparison chart,
   and combined R-D curve
3. If ResNet beat baseline: great, use ResNet going forward
4. If ResNet underperformed but loss still declining: consider longer training run
5. Update STATE.md with results and transition to Phase 6
</next_action>
</content>
</invoke>