---
phase: 04-architecture
plan: 04
task: training-improvements
total_plans: 6
status: in_progress
last_updated: 2026-01-26T11:30:00Z
---

<current_state>
Working on hyperparameter tuning for Variant B (ResidualAutoencoder) and Variant C (AttentionAutoencoder). Added training stability improvements to trainer.py and both training notebooks. User is running quick search training (5% data, 20 epochs) to validate hyperparameters before full training.

Residual v2 training completed with poor results (19.78 dB vs 20.47 baseline) due to LR=1e-5 being too conservative.
</current_state>

<completed_work>
- Plan 04-01: Building Blocks - COMPLETE
- Plan 04-02: ResidualAutoencoder - COMPLETE
- Plan 04-03: AttentionAutoencoder - COMPLETE
- Plan 04-05: Attention training notebook - COMPLETE

This session:
- Added warmup support to trainer.py (linear LR warmup over N epochs)
- Added AdamW optimizer option to trainer.py
- Updated train_residual.ipynb with stability improvements:
  - LEARNING_RATE: 1e-5 → 5e-5
  - WARMUP_EPOCHS: 3 (new)
  - OPTIMIZER: adamw (new)
  - WEIGHT_DECAY: 1e-5 (new)
  - MAX_GRAD_NORM: 1.0 → 0.5
  - Added QUICK_SEARCH toggle (5%/10% data vs 20%)
- Updated train_attention.ipynb with same improvements
- Reduced BASE_CHANNELS from 64 to 48 for AttentionAutoencoder (~13M params vs 24M)
- Fixed comparison table formatting (alignment, sign handling)
- Fixed SAR evaluation cells to use val_loader instead of raw file loading
</completed_work>

<remaining_work>
- Plan 04-04: Complete Residual v3 training with improved hyperparameters
- Plan 04-04: Complete Attention v2 training with improved hyperparameters
- Plan 04-06: Architecture comparison notebook (depends on training completion)
- Full training runs (20% data, 30 epochs) once hyperparameters validated
</remaining_work>

<decisions_made>
- LR warmup: 3 epochs, linear from LR/10 to LR - stabilizes early training
- AdamW over Adam: better weight decay handling, weight_decay=1e-5
- Gradient clipping 0.5 instead of 1.0: more aggressive for stability
- BASE_CHANNELS=48 for Attention: ~13M params, faster training (~30 min/epoch vs 55 min)
- QUICK_SEARCH mode: 5% data, 20 epochs for fast hyperparameter validation
- Use val_loader for SAR metrics instead of raw .npy files: avoids file loading issues on Windows
</decisions_made>

<blockers>
None currently. Training notebooks ready, user running quick search validation.
</blockers>

<context>
The Residual v2 training (19.78 dB) underperformed baseline (20.47 dB) because:
1. LR=1e-5 was too conservative (10x lower than typical)
2. Model didn't converge properly in 30 epochs

Strategy now: Use quick search (5% data, 20 epochs) to validate that:
1. Training is stable (no NaN)
2. Loss is decreasing
3. Metrics trend upward

Once validated, run full training (20% data, 30 epochs) with the stable config.

Key config changes in both notebooks:
- QUICK_SEARCH = True → 5% data, 20 epochs
- QUICK_SEARCH = False → 20% data, 30 epochs
- LR=5e-5 with 3 epoch warmup
- AdamW with weight_decay=1e-5
- Gradient clipping at 0.5
</context>

<files_modified>
- src/training/trainer.py: Added warmup_epochs, optimizer choice (adam/adamw), _update_warmup_lr()
- notebooks/train_residual.ipynb: Updated config, fixed tables, added QUICK_SEARCH toggle
- notebooks/train_attention.ipynb: Updated config, fixed tables, fixed SAR eval cells, BASE_CHANNELS=48
</files_modified>

<next_action>
1. Wait for user's quick search training to complete (~15-20 min with 5% data)
2. Review results - if stable and improving, set QUICK_SEARCH=False
3. Run full training (20% data, 30 epochs)
4. Compare results to baseline (20.47 dB) and ResNet-Lite (21.20 dB)
5. Create 04-04-SUMMARY.md and 04-06 comparison notebook
</next_action>
