---
phase: 04-architecture
task: training-sweep
total_tasks: N/A (pre-Phase 6 prep)
status: in_progress
last_updated: 2026-01-27
---

<current_state>
Returned to Phase 4 (Architecture) to complete model training before Phase 6.
Reviewed overnight training results — all runs regressed below baseline (20.47 dB).
Standardized naming convention across all 4 training notebooks. Now need to
retrain all architectures at multiple compression ratios for rate-distortion curves.
</current_state>

<completed_work>

## This Session

1. **Reviewed overnight training results** — Discovered all overnight ResNet runs (19.0 dB)
   performed worse than baseline (20.47 dB). Root causes identified:
   - LR too high (7e-3 and 3e-3 vs baseline's 1e-4)
   - base_channels=32 insufficient capacity (baseline uses 64)
   - OneCycleLR scheduler overshooting

2. **Standardized naming convention** across all 4 training notebooks:
   - `train_baseline.ipynb` — RUN_NAME: `baseline_c{LC}_b{BC}_cr{ratio}x`
   - `train_resnet.ipynb` — RUN_NAME: `resnet_c{LC}_b{BC}_cr{ratio}x`
   - `train_residual.ipynb` — RUN_NAME: `residual_{mode}_c{LC}_b{BC}_cr{ratio}x`
   - `train_attention.ipynb` — RUN_NAME: `attention_{mode}_c{LC}_b{BC}_cr{ratio}x`
   - Trainer auto-appends `_{YYYYMMDD_HHMMSS}` for uniqueness
   - All manual saves (training curves, reconstructions, SAR eval) now use
     `trainer.log_dir` instead of `Path(f"runs/{RUN_NAME}/...")` to ensure
     all outputs go to the same unique timestamped directory

3. **Built architecture x compression ratio status table**:
   All existing training is at 16x compression only (latent_channels=16).
   Only baseline (20.47 dB) is usable — ResNet/Residual/Attention all regressed or didn't converge.

</completed_work>

<remaining_work>

## Training Matrix (Architecture x Compression Ratio)

Compression ratio is controlled by `latent_channels`:
- LC=4 → 64x, LC=8 → 32x, LC=16 → 16x, LC=32 → 8x, LC=64 → 4x

| Architecture     | Params | 4x  | 8x  | 16x               | 32x | 64x |
|------------------|--------|-----|-----|--------------------|-----|-----|
| Baseline (b=64)  | 2.3M   | --  | --  | 20.47 dB / 0.646   | --  | --  |
| ResNet (b=32)    | 5.6M   | --  | --  | 19.06 dB (broken)  | --  | --  |
| Residual (b=32)  | 6.0M   | --  | --  | 19.78 dB (broken)  | --  | --  |
| Attention (b=48) | 13.5M  | --  | --  | 11.29 dB (broken)  | --  | --  |

`--` = not yet trained, `(broken)` = regressed below baseline, needs retraining

### Priorities

1. **Fix 16x training** — All non-baseline models need retraining with proper hyperparams:
   - Lower LR (1e-4 with ReduceLROnPlateau, not OneCycleLR)
   - base_channels=64 for ResNet (match baseline capacity)
   - More data (20% subset minimum), more epochs (50+)
   - Early stopping patience 10+

2. **Multi-ratio sweep** — Train each architecture at 4x, 8x, 16x, 32x, 64x
   for rate-distortion curves (user explicitly requested this)

3. **Rate-distortion graphs** — Plot PSNR vs compression ratio per architecture
   (needed for Phase 6 final experiments)

</remaining_work>

<decisions_made>

- **Standard naming**: `{model}_c{latent}_b{base}_cr{ratio}x` with auto-timestamp
- **All saves via trainer.log_dir**: Prevents path mismatches between trainer and manual saves
- **Overnight regression root cause**: LR too high + OneCycleLR + small base_channels
- **Accept that 25 dB at 16x is very aggressive**: May not be achievable, but
  lower compression ratios (8x, 4x) should reach it easily

</decisions_made>

<blockers>
- No hard blockers
- Training time is a practical constraint (~5-10 hours per run on RTX 3070)
- 20 runs needed × ~5h avg = ~100 GPU-hours (will need overnight batches)
</blockers>

<context>
User wants to push training harder AND sweep all compression ratios for rate-distortion
graphs. The naming convention fix was a prerequisite — now every run will produce
uniquely named outputs with model/channels/ratio in the name.

The next step is to set up the training runs. User will likely want to queue up
multiple runs to execute overnight. Consider creating a single training script
that can be parameterized (model, latent_channels, base_channels, lr, epochs)
rather than editing notebooks manually for each configuration.

Key insight: the baseline with bc=64 and LR=1e-4 is still the best model.
All other architectures need to match those training conditions before we can
fairly compare architectures.
</context>

<next_action>
When resuming:
1. Decide approach: individual notebook runs vs parameterized training script
2. Set up first batch of training runs (suggest starting with baseline at all 5 ratios
   since it's the only proven architecture, then sweep others)
3. Consider creating `scripts/train_sweep.py` to automate multi-config training
</next_action>
