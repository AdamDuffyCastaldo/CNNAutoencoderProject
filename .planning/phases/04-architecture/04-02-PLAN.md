---
phase: 04-architecture
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/models/residual_autoencoder.py
autonomous: true

must_haves:
  truths:
    - "ResidualAutoencoder input (B,1,256,256) produces output (B,1,256,256) and latent (B,16,16,16)"
    - "Encoder has 4 stages with 2 residual blocks each"
    - "Decoder mirrors encoder structure exactly"
    - "Forward pass works with batch_size=2 on GPU"
  artifacts:
    - path: "src/models/residual_autoencoder.py"
      provides: "PreActResidualEncoder, PreActResidualDecoder, ResidualAutoencoder"
      exports: ["ResidualAutoencoder"]
  key_links:
    - from: "src/models/residual_autoencoder.py"
      to: "src/models/blocks.py"
      via: "import PreActResidualBlock"
      pattern: "from .blocks import PreActResidualBlock"
---

<objective>
Implement Variant B: Pre-Activation Residual Autoencoder using the building blocks from Plan 01.

Purpose: This is the first enhanced architecture variant. It uses pre-activation residual blocks throughout encoder and decoder for better gradient flow, targeting +1.5 dB PSNR improvement over baseline (20.47 dB).

Output: New `src/models/residual_autoencoder.py` with working ResidualAutoencoder class.
</objective>

<architecture_notes>
## Comparison with Existing ResNetAutoencoder

| Aspect | ResNetAutoencoder (existing) | ResidualAutoencoder (new) |
|--------|------------------------------|---------------------------|
| Block type | Post-activation (Conv→BN→ReLU→...→ReLU) | Pre-activation (BN→ReLU→Conv→...+x) |
| Blocks per stage | 1 ResidualBlock + 1 ResidualBlockWithDown/Up | 2 PreActResidualBlock (first with stride) |
| Skip activation | ReLU after addition | NO activation after addition |
| Parameters | ~5.6M | Similar (~5-7M expected) |

## Code Reuse Strategy

**Reuse from ResNetAutoencoder (`src/models/resnet_autoencoder.py`):**
- Same overall structure: Stem → 4 Stages → Output
- Same helper methods: `encode()`, `decode()`, `get_compression_ratio()`, `get_latent_size()`, `count_parameters()`
- Same `_init_weights()` function (Kaiming initialization)
- Same docstring style and type hints

**Changes needed:**
- Replace ResidualBlock/ResidualBlockWithDownsample with PreActResidualBlock
- Remove ReLU after skip connection (handled inside PreActResidualBlock)
- Adjust stage structure: 2 PreActResidualBlocks per stage instead of ResidualBlock+ResidualBlockWithDown

**Copy the interface, not the implementation details.**
</architecture_notes>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-architecture/04-CONTEXT.md
@.planning/phases/04-architecture/04-RESEARCH.md
@.planning/phases/04-architecture/04-01-SUMMARY.md
@src/models/blocks.py
@src/models/resnet_autoencoder.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Pre-Activation Residual Encoder and Decoder</name>
  <files>src/models/residual_autoencoder.py</files>
  <action>
Create a new file `src/models/residual_autoencoder.py` with:

1. **PreActResidualEncoder** class:
   - `__init__(self, in_channels=1, base_channels=64, latent_channels=16)`
   - Architecture (per CONTEXT.md: 2 blocks per stage, 4 stages):
     - Stem: Conv2d(1, base_channels, 7x7, stride=1, padding=3) -> BN -> ReLU
     - Stage 1: 2x PreActResidualBlock (base_channels, base_channels*2, first with stride=2)
       - Block 1: stride=2, in=base, out=base*2 (256->128)
       - Block 2: stride=1, in=base*2, out=base*2
     - Stage 2: 2x PreActResidualBlock (128->64, channels: base*2 -> base*4)
     - Stage 3: 2x PreActResidualBlock (64->32, channels: base*4 -> base*8)
     - Stage 4: 2x PreActResidualBlock (32->16, channels: base*8 -> latent_channels)
   - Total: 8 residual blocks + stem
   - Output: (B, latent_channels, 16, 16)

2. **PreActResidualDecoder** class:
   - `__init__(self, out_channels=1, base_channels=64, latent_channels=16)`
   - Mirrors encoder exactly (per CONTEXT.md):
     - Stage 1: Upsample 16->32, 2x PreActResidualBlock (latent -> base*8)
     - Stage 2: Upsample 32->64, 2x PreActResidualBlock (base*8 -> base*4)
     - Stage 3: Upsample 64->128, 2x PreActResidualBlock (base*4 -> base*2)
     - Stage 4: Upsample 128->256, 2x PreActResidualBlock (base*2 -> base)
     - Output: Conv2d(base, 1, 7x7, stride=1, padding=3) -> Sigmoid
   - Use bilinear upsample + 1x1 conv for each stage (stable upsampling)

3. Implement Kaiming weight initialization for all conv layers

Key architecture details from CONTEXT.md:
- 2 residual blocks per encoder/decoder stage
- Pre-activation ordering (BN->ReLU->Conv)
- ReLU (not LeakyReLU) for residual blocks
- No dropout - rely on BatchNorm
- 3x3 kernels inside residual blocks
  </action>
  <verify>
Run in Python:
```python
import torch
from src.models.residual_autoencoder import PreActResidualEncoder, PreActResidualDecoder

# Test encoder
encoder = PreActResidualEncoder(in_channels=1, base_channels=64, latent_channels=16)
x = torch.randn(2, 1, 256, 256)
z = encoder(x)
assert z.shape == (2, 16, 16, 16), f"Encoder output shape wrong: {z.shape}"
print(f"Encoder: {x.shape} -> {z.shape}")

# Test decoder
decoder = PreActResidualDecoder(out_channels=1, base_channels=64, latent_channels=16)
x_hat = decoder(z)
assert x_hat.shape == (2, 1, 256, 256), f"Decoder output shape wrong: {x_hat.shape}"
print(f"Decoder: {z.shape} -> {x_hat.shape}")

# Verify output is bounded [0, 1]
assert x_hat.min() >= 0 and x_hat.max() <= 1, "Output not bounded [0,1]"

print("Encoder/Decoder tests passed!")
```
  </verify>
  <done>PreActResidualEncoder compresses (B,1,256,256) to (B,16,16,16), PreActResidualDecoder reconstructs (B,16,16,16) to (B,1,256,256) with Sigmoid output bounded [0,1].</done>
</task>

<task type="auto">
  <name>Task 2: Create ResidualAutoencoder Wrapper and Test</name>
  <files>src/models/residual_autoencoder.py</files>
  <action>
Add to `src/models/residual_autoencoder.py`:

1. **ResidualAutoencoder** class (wrapper):
   - `__init__(self, in_channels=1, base_channels=64, latent_channels=16)`
   - Compose PreActResidualEncoder + PreActResidualDecoder
   - `forward(self, x)` -> returns (x_hat, z) tuple (same interface as existing autoencoders)
   - `encode(self, x)` -> z
   - `decode(self, z)` -> x_hat
   - `get_compression_ratio(self)` -> float (256*256 / (16*16*latent_channels))
   - `get_latent_size(self)` -> (latent_channels, 16, 16)
   - `count_parameters(self)` -> dict with encoder/decoder/total

2. Add test function `test_residual_autoencoder()`:
   - Test forward pass with multiple batch sizes
   - Verify gradient flow (backward works)
   - Print parameter count and compare to baseline

3. Update `src/models/__init__.py` to export ResidualAutoencoder:
   - Add: `from .residual_autoencoder import ResidualAutoencoder`
  </action>
  <verify>
Run in Python:
```python
import torch
from src.models import ResidualAutoencoder

# Test full autoencoder
model = ResidualAutoencoder(latent_channels=16, base_channels=64)
x = torch.randn(2, 1, 256, 256)
x_hat, z = model(x)

assert x_hat.shape == x.shape, f"Output shape mismatch: {x_hat.shape}"
assert z.shape == (2, 16, 16, 16), f"Latent shape wrong: {z.shape}"
print(f"Forward: {x.shape} -> {z.shape} -> {x_hat.shape}")

# Test properties
params = model.count_parameters()
print(f"Parameters: {params['total']:,} (encoder: {params['encoder']:,}, decoder: {params['decoder']:,})")
print(f"Compression ratio: {model.get_compression_ratio():.1f}x")
print(f"Latent size: {model.get_latent_size()}")

# Test gradient flow
model.train()
x = torch.randn(2, 1, 256, 256, requires_grad=True)
x_hat, z = model(x)
loss = torch.nn.functional.mse_loss(x_hat, torch.zeros_like(x_hat))
loss.backward()
print("Gradient flow: OK")

# Test on GPU if available
if torch.cuda.is_available():
    model_gpu = model.cuda()
    x_gpu = torch.randn(2, 1, 256, 256).cuda()
    x_hat_gpu, z_gpu = model_gpu(x_gpu)
    print(f"GPU test: OK (memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB)")

print("All ResidualAutoencoder tests passed!")
```
  </verify>
  <done>ResidualAutoencoder forward pass works, returns (x_hat, z) tuple, gradients flow correctly, all helper methods (encode, decode, count_parameters, get_compression_ratio, get_latent_size) work. Model importable from src.models.</done>
</task>

</tasks>

<verification>
After both tasks complete:

1. Run model test: `python -c "from src.models.residual_autoencoder import test_residual_autoencoder; test_residual_autoencoder()"`
2. Verify import from package: `python -c "from src.models import ResidualAutoencoder; print('Import OK')"`
3. Quick memory check on GPU: Should fit in 8GB VRAM with batch_size=32
</verification>

<success_criteria>
1. ResidualAutoencoder accepts (B, 1, 256, 256) input and produces (B, 1, 256, 256) output
2. Latent representation shape is (B, 16, 16, 16) for 16x compression
3. Encoder has 4 stages with 2 residual blocks each (8 total)
4. Decoder mirrors encoder with 4 stages and 2 blocks each
5. Output bounded [0, 1] via Sigmoid
6. Gradients flow through entire network
7. Model fits in GPU memory with reasonable batch size
</success_criteria>

<output>
After completion, create `.planning/phases/04-architecture/04-02-SUMMARY.md`
</output>
