---
phase: 04-architecture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/blocks.py
autonomous: true

must_haves:
  truths:
    - "PreActResidualBlock preserves spatial dimensions when stride=1"
    - "PreActResidualBlock downsamples by 2x when stride=2"
    - "CBAM applies channel-then-spatial attention without errors"
    - "All blocks can be instantiated and run forward pass"
  artifacts:
    - path: "src/models/blocks.py"
      provides: "PreActResidualBlock, ChannelAttention, SpatialAttention, CBAM"
      contains: "class PreActResidualBlock"
  key_links:
    - from: "PreActResidualBlock"
      to: "nn.BatchNorm2d, nn.Conv2d"
      via: "pre-activation ordering BN->ReLU->Conv"
      pattern: "bn1.*relu.*conv1"
    - from: "CBAM"
      to: "ChannelAttention, SpatialAttention"
      via: "sequential channel-then-spatial"
      pattern: "channel_attention.*spatial_attention"
---

<objective>
Implement pre-activation residual blocks and CBAM attention modules as building blocks for enhanced autoencoder architectures.

Purpose: These blocks are the foundational components for Variant B (Residual) and Variant C (Residual+CBAM) autoencoders. Pre-activation residual blocks enable better gradient flow in deeper networks, while CBAM provides channel and spatial attention for improved feature extraction.

Output: Updated `src/models/blocks.py` with working PreActResidualBlock and CBAM implementations.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-architecture/04-CONTEXT.md
@.planning/phases/04-architecture/04-RESEARCH.md
@src/models/blocks.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Pre-Activation Residual Block</name>
  <files>src/models/blocks.py</files>
  <action>
Add PreActResidualBlock class implementing ResNet v2 style pre-activation:

1. **PreActResidualBlock** - Basic block with BN->ReLU->Conv ordering:
   - `__init__(self, in_channels: int, out_channels: int, stride: int = 1)`
   - Main path: BN1 -> ReLU -> Conv1(stride) -> BN2 -> ReLU -> Conv2
   - Use 3x3 kernels with padding=1 for all convs, bias=False
   - Projection shortcut: When stride != 1 OR in_channels != out_channels, use 1x1 conv with stride to match dimensions
   - Forward: return `out + identity` (NO activation after addition - key for pre-activation)

2. **PreActResidualBlockDown** - Convenience wrapper for downsampling:
   - `__init__(self, in_channels: int, out_channels: int)`
   - Wraps PreActResidualBlock with stride=2
   - Halves spatial dimensions

3. **PreActResidualBlockUp** - For decoder upsampling:
   - `__init__(self, in_channels: int, out_channels: int, scale_factor: int = 2)`
   - Uses bilinear upsample + 1x1 conv to change channels
   - Then 2 residual convolutions (BN->ReLU->Conv) maintaining channels
   - Skip connection from upsampled input

Key implementation details from RESEARCH.md:
- NO ReLU after skip connection addition
- Use Kaiming initialization for conv weights
- BatchNorm weight=1, bias=0 initialization
  </action>
  <verify>
Run in Python:
```python
import torch
from src.models.blocks import PreActResidualBlock, PreActResidualBlockDown, PreActResidualBlockUp

# Test stride=1 (preserve spatial)
block = PreActResidualBlock(64, 64, stride=1)
x = torch.randn(2, 64, 32, 32)
y = block(x)
assert y.shape == (2, 64, 32, 32), f"stride=1 shape wrong: {y.shape}"

# Test stride=2 (downsample)
block_down = PreActResidualBlock(64, 128, stride=2)
y = block_down(x)
assert y.shape == (2, 128, 16, 16), f"stride=2 shape wrong: {y.shape}"

# Test PreActResidualBlockDown wrapper
block_down2 = PreActResidualBlockDown(64, 128)
y = block_down2(x)
assert y.shape == (2, 128, 16, 16), f"down wrapper shape wrong: {y.shape}"

# Test PreActResidualBlockUp
block_up = PreActResidualBlockUp(128, 64)
x_up = torch.randn(2, 128, 16, 16)
y = block_up(x_up)
assert y.shape == (2, 64, 32, 32), f"up block shape wrong: {y.shape}"

print("All PreActResidualBlock tests passed!")
```
  </verify>
  <done>PreActResidualBlock with stride=1 preserves shape, stride=2 downsamples 2x, PreActResidualBlockUp upsamples 2x. All forward passes work without errors.</done>
</task>

<task type="auto">
  <name>Task 2: Implement CBAM Attention Module</name>
  <files>src/models/blocks.py</files>
  <action>
Replace the stub implementations of ChannelAttention, SpatialAttention, and CBAM with working code:

1. **ChannelAttention** - Squeeze-and-excitation style:
   - `__init__(self, channels: int, reduction: int = 16)`
   - Use AdaptiveMaxPool2d(1) and AdaptiveAvgPool2d(1)
   - Shared MLP via 1x1 convolutions: Conv(channels, channels//reduction) -> ReLU -> Conv(channels//reduction, channels)
   - Handle edge case: `reduced = max(channels // reduction, 1)` to avoid zero channels
   - NO BatchNorm inside MLP (per CBAM paper)
   - Forward: `sigmoid(mlp(maxpool(x)) + mlp(avgpool(x)))`
   - Return attention weights (B, C, 1, 1) to be multiplied with input

2. **SpatialAttention** - Location-based attention:
   - `__init__(self, kernel_size: int = 7)`
   - Forward: Compute channel-wise max and mean -> concat -> Conv2d(2, 1, kernel_size, padding=kernel_size//2) -> Sigmoid
   - Return attention weights (B, 1, H, W) to be multiplied with input

3. **CBAM** - Combined attention:
   - `__init__(self, channels: int, reduction: int = 16, kernel_size: int = 7)`
   - Compose ChannelAttention then SpatialAttention
   - Forward: `x = x * channel_attention(x)` then `x = x * spatial_attention(x)`
   - Return refined features

Key details from RESEARCH.md:
- Channel-first, then spatial (not parallel)
- Reduction=16 is standard
- Kernel=7 for spatial attention
  </action>
  <verify>
Run in Python:
```python
import torch
from src.models.blocks import ChannelAttention, SpatialAttention, CBAM

x = torch.randn(2, 64, 32, 32)

# Test ChannelAttention
ca = ChannelAttention(64, reduction=16)
ca_weights = ca(x)
assert ca_weights.shape == (2, 64, 1, 1), f"CA shape wrong: {ca_weights.shape}"
assert (ca_weights >= 0).all() and (ca_weights <= 1).all(), "CA not in [0,1]"

# Test SpatialAttention
sa = SpatialAttention(kernel_size=7)
sa_weights = sa(x)
assert sa_weights.shape == (2, 1, 32, 32), f"SA shape wrong: {sa_weights.shape}"
assert (sa_weights >= 0).all() and (sa_weights <= 1).all(), "SA not in [0,1]"

# Test CBAM
cbam = CBAM(64, reduction=16, kernel_size=7)
y = cbam(x)
assert y.shape == x.shape, f"CBAM shape changed: {y.shape}"

# Test with small channels (edge case)
cbam_small = CBAM(8, reduction=16)  # Would be 0 channels without max(1)
y_small = cbam_small(torch.randn(2, 8, 32, 32))
assert y_small.shape == (2, 8, 32, 32), "CBAM small channels failed"

print("All CBAM tests passed!")
```
  </verify>
  <done>ChannelAttention returns (B, C, 1, 1) weights in [0,1], SpatialAttention returns (B, 1, H, W) weights in [0,1], CBAM applies both sequentially and preserves input shape. Edge case with small channels handled.</done>
</task>

</tasks>

<verification>
After both tasks complete:

1. Run the test function in blocks.py: `python -c "from src.models.blocks import test_blocks; test_blocks()"`
2. Verify all new classes are importable: `python -c "from src.models.blocks import PreActResidualBlock, PreActResidualBlockDown, PreActResidualBlockUp, ChannelAttention, SpatialAttention, CBAM; print('All imports successful')"`
3. Check no regressions in existing blocks (ConvBlock, DeconvBlock, ResidualBlock still work)
</verification>

<success_criteria>
1. PreActResidualBlock implements BN->ReLU->Conv ordering (pre-activation style)
2. PreActResidualBlock with stride=2 correctly downsamples spatial dimensions by 2x
3. PreActResidualBlockUp correctly upsamples spatial dimensions by 2x
4. ChannelAttention and SpatialAttention produce attention weights in [0, 1]
5. CBAM combines both attentions and preserves input tensor shape
6. All existing blocks (ConvBlock, DeconvBlock, ResidualBlock) still function correctly
</success_criteria>

<output>
After completion, create `.planning/phases/04-architecture/04-01-SUMMARY.md`
</output>
