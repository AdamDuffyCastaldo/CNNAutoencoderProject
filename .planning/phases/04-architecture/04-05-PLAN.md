---
phase: 04-architecture
plan: 05
type: execute
wave: 3
depends_on: ["04-03"]
files_modified:
  - notebooks/train_attention.ipynb
  - notebooks/checkpoints/attention_v1_c16/best.pth
autonomous: true

must_haves:
  truths:
    - "Training completes without NaN or divergence"
    - "Validation PSNR is at least 0.5 dB higher than Variant B"
    - "ENL ratio is within 0.7-1.3 range"
    - "Checkpoint saved with preprocessing params"
  artifacts:
    - path: "notebooks/train_attention.ipynb"
      provides: "Training notebook for Variant C"
    - path: "notebooks/checkpoints/attention_v1_c16/best.pth"
      provides: "Trained checkpoint"
  key_links:
    - from: "notebooks/train_attention.ipynb"
      to: "src/models/attention_autoencoder.py"
      via: "from src.models import AttentionAutoencoder"
      pattern: "AttentionAutoencoder"
---

<objective>
Train Variant C (Pre-Activation Residual + CBAM Attention) autoencoder at 16x compression and evaluate against Variant B.

Purpose: Validate that adding CBAM attention provides additional quality improvement over residual-only architecture. Target: +0.5 dB PSNR over Variant B.

Output: Trained checkpoint `notebooks/checkpoints/attention_v1_c16/best.pth` and training notebook with results.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-architecture/04-CONTEXT.md
@.planning/phases/04-architecture/04-03-SUMMARY.md
@.planning/phases/04-architecture/04-04-SUMMARY.md
@notebooks/train_resnet.ipynb
@src/training/trainer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create and Run Training Notebook for Variant C</name>
  <files>notebooks/train_attention.ipynb</files>
  <action>
Create `notebooks/train_attention.ipynb` by adapting the residual training notebook:

**Configuration changes:**
- Model: AttentionAutoencoder (from src.models)
- Same loss weights as Variant B: 0.7 MSE + 0.3 SSIM
- Same 20% data subset for fair comparison
- Learning rate: 1e-4 (same as Variant B for fair comparison)
- Epochs: 30 or early stopping
- Batch size: May need to reduce to 16-24 if CBAM causes memory issues (per RESEARCH.md)
- Run name: "attention_v1_c16"

**Notebook structure:**
1. Setup and imports (use AttentionAutoencoder)
2. Configuration
3. Data loading (same subset as Variant B)
4. Model creation with parameter count
5. Loss function: CombinedLoss(mse_weight=0.7, ssim_weight=0.3)
6. Trainer initialization
7. Training loop with memory monitoring
8. Results summary
9. Comparison with baseline, ResNet-Lite, and Variant B

**Memory considerations:**
- CBAM adds memory overhead at high-resolution stages
- If OOM at batch_size=32: reduce to 24 or 16
- Document actual batch size used

**Training execution:**
- Execute notebook to train
- Monitor GPU memory usage
- Adjust batch size if needed
- Save checkpoint to notebooks/checkpoints/attention_v1_c16/

**Key metrics to record:**
- Best validation PSNR (target: Variant B + 0.5 dB)
- Best validation SSIM
- Best validation loss
- Training time
- Actual batch size used
  </action>
  <verify>
After training completes:

1. Check checkpoint exists:
   ```python
   from pathlib import Path
   assert Path("notebooks/checkpoints/attention_v1_c16/best.pth").exists()
   ```

2. Load and verify:
   ```python
   import torch
   from src.models import AttentionAutoencoder

   checkpoint = torch.load("notebooks/checkpoints/attention_v1_c16/best.pth", weights_only=False)
   model = AttentionAutoencoder(latent_channels=16, base_channels=64)
   model.load_state_dict(checkpoint['model_state_dict'])
   print(f"Epoch: {checkpoint['epoch']}, Best val loss: {checkpoint['best_val_loss']:.4f}")
   ```

3. Training completed without crash
  </verify>
  <done>Training notebook created and executed, checkpoint saved, training completed without OOM or NaN.</done>
</task>

<task type="auto">
  <name>Task 2: Evaluate and Document Variant C Results</name>
  <files>notebooks/train_attention.ipynb</files>
  <action>
Add evaluation cells to the end of `notebooks/train_attention.ipynb`:

**Evaluation steps:**
1. Load best checkpoint
2. Run validation to get final metrics
3. Compute SAR-specific metrics:
   - ENL ratio (target: 0.7-1.3)
   - EPI (edge preservation index)
4. Create visual comparison
5. Document CBAM impact (does attention help?)

**Full comparison table:**
```
| Model            | Params   | PSNR    | SSIM   | ENL Ratio | vs Baseline |
|------------------|----------|---------|--------|-----------|-------------|
| Baseline         | 2.3M     | 20.47   | 0.646  | -         | -           |
| ResNet-Lite v2   | 5.6M     | 21.20   | 0.726  | 0.851     | +0.73 dB    |
| Residual v1      | ???      | ???     | ???    | ???       | +??? dB     |
| Attention v1     | ???      | ???     | ???    | ???       | +??? dB     |
```

**Analysis questions:**
- Did CBAM improve PSNR over Variant B?
- How does memory/parameter increase compare to PSNR gain?
- Is the CBAM overhead worth the improvement?

**If targets not met:**
- Document actual results
- Note potential explanations (not enough data, wrong hyperparameters, etc.)
- Record observations for future work
  </action>
  <verify>
After evaluation:

```python
import torch
from src.models import AttentionAutoencoder

checkpoint = torch.load("notebooks/checkpoints/attention_v1_c16/best.pth", weights_only=False)
model = AttentionAutoencoder(latent_channels=16, base_channels=64)
model.load_state_dict(checkpoint['model_state_dict'])

if 'history' in checkpoint:
    final = checkpoint['history'][-1]
    print(f"Final val PSNR: {final.get('val_psnr', 'N/A')}")
    print(f"Final val SSIM: {final.get('val_ssim', 'N/A')}")

print(f"Parameters: {model.count_parameters()['total']:,}")
```
  </verify>
  <done>Variant C evaluation complete with metrics, visual comparisons, and comparison to Variant B documented. CBAM impact assessed.</done>
</task>

</tasks>

<verification>
After training and evaluation:

1. Checkpoint file exists: `notebooks/checkpoints/attention_v1_c16/best.pth`
2. Training completed all epochs or early stopped
3. Metrics documented in notebook
4. Comparison with Variant B recorded
5. CBAM impact analyzed
</verification>

<success_criteria>
1. Training notebook runs without errors
2. Training completes without NaN, divergence, or OOM
3. Checkpoint saved with model weights and preprocessing_params
4. Validation PSNR documented (target: Variant B + 0.5 dB)
5. ENL ratio computed and within 0.7-1.3
6. Full comparison table with all variants documented
7. Visual reconstructions saved
8. CBAM benefit (or lack thereof) analyzed
</success_criteria>

<output>
After completion, create `.planning/phases/04-architecture/04-05-SUMMARY.md`
</output>
