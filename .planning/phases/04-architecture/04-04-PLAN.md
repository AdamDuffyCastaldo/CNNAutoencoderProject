---
phase: 04-architecture
plan: 04
type: execute
wave: 3
depends_on: ["04-02"]
files_modified:
  - notebooks/train_residual.ipynb
  - notebooks/checkpoints/residual_v1_c16/best.pth
autonomous: true

must_haves:
  truths:
    - "Training completes without NaN or divergence"
    - "Validation PSNR is at least 1.5 dB higher than baseline (20.47 dB) -> target 22.0 dB"
    - "ENL ratio is within 0.7-1.3 range"
    - "Checkpoint saved with preprocessing params"
  artifacts:
    - path: "notebooks/train_residual.ipynb"
      provides: "Training notebook for Variant B"
    - path: "notebooks/checkpoints/residual_v1_c16/best.pth"
      provides: "Trained checkpoint"
  key_links:
    - from: "notebooks/train_residual.ipynb"
      to: "src/models/residual_autoencoder.py"
      via: "from src.models import ResidualAutoencoder"
      pattern: "ResidualAutoencoder"
---

<objective>
Train Variant B (Pre-Activation Residual) autoencoder at 16x compression and evaluate against baseline.

Purpose: Validate that the pre-activation residual architecture provides meaningful quality improvement over the baseline model. Target: +1.5 dB PSNR improvement (from 20.47 dB to 22.0 dB minimum).

Output: Trained checkpoint `notebooks/checkpoints/residual_v1_c16/best.pth` and training notebook with results.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-architecture/04-CONTEXT.md
@.planning/phases/04-architecture/04-02-SUMMARY.md
@notebooks/train_resnet.ipynb
@src/training/trainer.py
@src/losses/combined.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create and Run Training Notebook for Variant B</name>
  <files>notebooks/train_residual.ipynb</files>
  <action>
Create `notebooks/train_residual.ipynb` by adapting `train_resnet.ipynb`:

**Configuration changes (per CONTEXT.md):**
- Model: ResidualAutoencoder (from src.models)
- Loss weights: 0.7 MSE + 0.3 SSIM (emphasize pixel accuracy for PSNR)
- Same 20% data subset for fair comparison
- Learning rate: Start with 1e-4, consider warmup (RESEARCH.md recommends 5 epochs)
- Epochs: 30 (same as baseline) or train to convergence with early stopping
- Batch size: 32 with AMP (adjust if memory issues)
- Run name: "residual_v1_c16"

**Notebook structure:**
1. Setup and imports (use ResidualAutoencoder instead of ResNetAutoencoder)
2. Configuration with updated loss weights
3. Data loading (same as train_resnet.ipynb)
4. Model creation with parameter count
5. Loss function: CombinedLoss(mse_weight=0.7, ssim_weight=0.3)
6. Trainer initialization
7. Training loop
8. Results summary (final PSNR, SSIM, loss)
9. Comparison with baseline and ResNet-Lite

**Training execution:**
- Execute the notebook cells to train the model
- Monitor for NaN issues or training instability
- If unstable: reduce learning rate, add warmup, or reduce batch size
- Save checkpoint to notebooks/checkpoints/residual_v1_c16/

**Key metrics to record:**
- Best validation PSNR (target: >= 22.0 dB)
- Best validation SSIM
- Best validation loss
- Training time
- Parameter count
  </action>
  <verify>
After training completes:

1. Check checkpoint exists:
   ```python
   from pathlib import Path
   assert Path("notebooks/checkpoints/residual_v1_c16/best.pth").exists()
   ```

2. Load and verify checkpoint:
   ```python
   import torch
   from src.models import ResidualAutoencoder

   checkpoint = torch.load("notebooks/checkpoints/residual_v1_c16/best.pth", weights_only=False)
   model = ResidualAutoencoder(latent_channels=16, base_channels=64)
   model.load_state_dict(checkpoint['model_state_dict'])
   print(f"Epoch: {checkpoint['epoch']}, Best val loss: {checkpoint['best_val_loss']:.4f}")
   assert 'preprocessing_params' in checkpoint
   ```

3. Check training logs for final metrics
  </verify>
  <done>Training notebook created and executed, checkpoint saved with preprocessing params, no NaN or divergence during training.</done>
</task>

<task type="auto">
  <name>Task 2: Evaluate and Document Variant B Results</name>
  <files>notebooks/train_residual.ipynb</files>
  <action>
Add evaluation cells to the end of `notebooks/train_residual.ipynb`:

**Evaluation steps:**
1. Load best checkpoint
2. Run validation to get final metrics (PSNR, SSIM, loss)
3. Compute SAR-specific metrics on sample patches:
   - ENL ratio (target: 0.7-1.3)
   - EPI (edge preservation index)
4. Create visual comparison:
   - Original vs Reconstructed vs Difference
   - Side-by-side with baseline reconstructions if available

**Comparison table to generate:**
```
| Model          | Params   | PSNR    | SSIM   | ENL Ratio | Improvement |
|----------------|----------|---------|--------|-----------|-------------|
| Baseline       | 2.3M     | 20.47   | 0.646  | -         | -           |
| ResNet-Lite v2 | 5.6M     | 21.20   | 0.726  | 0.851     | +0.73 dB    |
| Residual v1    | ???      | ???     | ???    | ???       | +??? dB     |
```

**Success verification:**
- PSNR >= 22.0 dB (1.5 dB above baseline)
- ENL ratio in [0.7, 1.3]
- Training stable (no early termination due to NaN)

**If PSNR target not met:**
- Document actual results
- Note potential improvements (longer training, larger model, different lr)
- Continue anyway - results inform next steps
  </action>
  <verify>
After evaluation:

```python
# Quick metrics verification
import torch
from src.models import ResidualAutoencoder
from src.losses.combined import CombinedLoss

checkpoint = torch.load("notebooks/checkpoints/residual_v1_c16/best.pth", weights_only=False)
model = ResidualAutoencoder(latent_channels=16, base_channels=64)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Get history if available
if 'history' in checkpoint:
    final = checkpoint['history'][-1]
    print(f"Final val PSNR: {final.get('val_psnr', 'N/A')}")
    print(f"Final val SSIM: {final.get('val_ssim', 'N/A')}")

print(f"Parameters: {model.count_parameters()['total']:,}")
```
  </verify>
  <done>Variant B evaluation complete with metrics table, visual comparisons, and documented results. PSNR improvement over baseline documented (whether or not target met).</done>
</task>

</tasks>

<verification>
After training and evaluation:

1. Checkpoint file exists: `notebooks/checkpoints/residual_v1_c16/best.pth`
2. Training completed all epochs or early stopped (not crashed)
3. Metrics documented in notebook
4. Comparison with baseline and ResNet-Lite v2 recorded

Note: If training takes >10 hours, document progress and continue in next session. The checkpoint system supports resumption.
</verification>

<success_criteria>
1. Training notebook runs without errors
2. Training completes without NaN or divergence
3. Checkpoint saved with model weights and preprocessing_params
4. Validation PSNR documented (target: >= 22.0 dB)
5. ENL ratio computed and within 0.7-1.3
6. Comparison with baseline (20.47 dB) and ResNet-Lite v2 (21.20 dB) documented
7. Visual reconstructions saved
</success_criteria>

<output>
After completion, create `.planning/phases/04-architecture/04-04-SUMMARY.md`
</output>
