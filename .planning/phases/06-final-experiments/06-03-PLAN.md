---
phase: 06-final-experiments
plan: 03
type: execute
wave: 3
depends_on: [06-02]
files_modified:
  - reports/final_comparison.ipynb
  - reports/final_comparison.md
  - reports/figures/rate_distortion_psnr.png
  - reports/figures/rate_distortion_ssim.png
  - reports/figures/visual_comparison_*.png
  - reports/tables/statistical_tests.csv
autonomous: true

must_haves:
  truths:
    - "Rate-distortion curves show PSNR vs BPP for all methods"
    - "Statistical tests compare autoencoder vs JPEG-2000 with p-values"
    - "Visual comparison gallery shows 5 examples per compression ratio"
    - "Final report summarizes findings with conclusions"
  artifacts:
    - path: "reports/final_comparison.ipynb"
      provides: "Reproducible analysis notebook"
    - path: "reports/final_comparison.md"
      provides: "Readable markdown report"
    - path: "reports/figures/rate_distortion_psnr.png"
      provides: "PSNR vs BPP curve"
    - path: "reports/figures/rate_distortion_ssim.png"
      provides: "SSIM vs BPP curve"
    - path: "reports/figures/visual_comparison_16x.png"
      provides: "Visual examples at 16x compression"
    - path: "reports/tables/statistical_tests.csv"
      provides: "Statistical test results"
  key_links:
    - from: "reports/final_comparison.ipynb"
      to: "reports/data/per_sample_metrics.json"
      via: "JSON load for statistical tests"
      pattern: "per_sample_metrics"
    - from: "reports/final_comparison.ipynb"
      to: "scipy.stats"
      via: "ttest_rel, wilcoxon imports"
      pattern: "from scipy.stats import"
---

<objective>
Generate comprehensive analysis with rate-distortion curves, statistical tests, visual comparisons, and final report.

Purpose: Transform raw evaluation data into publishable results that demonstrate autoencoder performance vs JPEG-2000.

Output: Complete analysis notebook, markdown report, publication-quality figures, and statistical test results.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/06-final-experiments/06-CONTEXT.md
@.planning/phases/06-final-experiments/06-RESEARCH.md
@src/evaluation/visualizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create analysis notebook with rate-distortion curves</name>
  <files>reports/final_comparison.ipynb, reports/figures/rate_distortion_psnr.png, reports/figures/rate_distortion_ssim.png</files>
  <action>
Create Jupyter notebook `reports/final_comparison.ipynb` with the following sections:

**Cell 1: Setup and Data Loading**
```python
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_rel, wilcoxon, shapiro

# Publication-quality figure settings
plt.rcParams.update({
    'font.size': 10, 'font.family': 'serif',
    'figure.figsize': (8, 5), 'figure.dpi': 150,
    'savefig.dpi': 300, 'savefig.bbox': 'tight',
})

# Load evaluation data
with open('data/all_results.json') as f:
    all_results = json.load(f)
with open('data/per_sample_metrics.json') as f:
    per_sample = json.load(f)
summary_df = pd.read_csv('tables/results_summary.csv')

print(f"Loaded {len(all_results)} model results")
print(f"Per-sample data for {len(per_sample)} models")
```

**Cell 2: Rate-Distortion Data Preparation**
```python
# Prepare R-D data
rd_data = []
for name, result in all_results.items():
    ratio = result['compression_ratio']
    bpp = 32.0 / ratio  # 32-bit float -> bits per pixel
    metrics = result['metrics']

    # Determine model type and style
    if 'baseline' in name:
        model_type = 'Baseline'
        marker = 'o'
        color = 'tab:blue'
    elif 'resnet' in name:
        model_type = 'ResNet'
        marker = 's'
        color = 'tab:orange'
    else:  # codec
        model_type = 'JPEG-2000'
        marker = '^'
        color = 'tab:gray'

    rd_data.append({
        'name': name,
        'model_type': model_type,
        'ratio': ratio,
        'bpp': bpp,
        'psnr': metrics['psnr']['mean'],
        'psnr_std': metrics['psnr']['std'],
        'ssim': metrics['ssim']['mean'],
        'ssim_std': metrics['ssim']['std'],
        'marker': marker,
        'color': color,
    })

rd_df = pd.DataFrame(rd_data)
print(rd_df[['name', 'model_type', 'ratio', 'bpp', 'psnr', 'ssim']])
```

**Cell 3: PSNR Rate-Distortion Curve**
```python
fig, ax = plt.subplots(figsize=(8, 5))

for model_type in ['Baseline', 'ResNet', 'JPEG-2000']:
    subset = rd_df[rd_df['model_type'] == model_type].sort_values('bpp')
    if len(subset) == 0:
        continue

    marker = subset.iloc[0]['marker']
    color = subset.iloc[0]['color']

    ax.errorbar(
        subset['bpp'], subset['psnr'], yerr=subset['psnr_std'],
        marker=marker, color=color, label=model_type,
        capsize=3, linewidth=2, markersize=8
    )

ax.set_xlabel('Bits Per Pixel (BPP)')
ax.set_ylabel('PSNR (dB)')
ax.set_title('Rate-Distortion: PSNR vs Compression')
ax.legend(loc='lower right')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('figures/rate_distortion_psnr.png', dpi=300)
plt.show()
print("Saved: figures/rate_distortion_psnr.png")
```

**Cell 4: SSIM Rate-Distortion Curve**
```python
fig, ax = plt.subplots(figsize=(8, 5))

for model_type in ['Baseline', 'ResNet', 'JPEG-2000']:
    subset = rd_df[rd_df['model_type'] == model_type].sort_values('bpp')
    if len(subset) == 0:
        continue

    marker = subset.iloc[0]['marker']
    color = subset.iloc[0]['color']

    ax.errorbar(
        subset['bpp'], subset['ssim'], yerr=subset['ssim_std'],
        marker=marker, color=color, label=model_type,
        capsize=3, linewidth=2, markersize=8
    )

ax.set_xlabel('Bits Per Pixel (BPP)')
ax.set_ylabel('SSIM')
ax.set_title('Rate-Distortion: SSIM vs Compression')
ax.legend(loc='lower right')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('figures/rate_distortion_ssim.png', dpi=300)
plt.show()
print("Saved: figures/rate_distortion_ssim.png")
```

Use nbformat to create the notebook programmatically, or create cells manually in Jupyter.
  </action>
  <verify>
Notebook cells execute without error:
```bash
cd reports && jupyter nbconvert --execute final_comparison.ipynb --to notebook --inplace
```
Figures exist: `reports/figures/rate_distortion_psnr.png`, `reports/figures/rate_distortion_ssim.png`
  </verify>
  <done>Rate-distortion curves generated and saved as PNG</done>
</task>

<task type="auto">
  <name>Task 2: Perform statistical analysis and save results</name>
  <files>reports/tables/statistical_tests.csv, reports/final_comparison.ipynb (updated)</files>
  <action>
Add statistical analysis cells to the notebook:

**Cell 5: Statistical Comparison Functions**
```python
def compare_methods(per_sample, ae_name: str, codec_name: str, metric: str = 'psnr'):
    """Paired statistical test between autoencoder and codec on same samples."""
    ae_samples = per_sample[ae_name]
    codec_samples = per_sample[codec_name]

    # Get values for matched sample indices
    common_indices = set(ae_samples.keys()) & set(codec_samples.keys())
    ae_vals = np.array([ae_samples[idx][metric] for idx in sorted(common_indices)])
    codec_vals = np.array([codec_samples[idx][metric] for idx in sorted(common_indices)])
    diff = ae_vals - codec_vals

    # Check normality (on subset for Shapiro limit)
    _, p_normal = shapiro(diff[:min(50, len(diff))])

    # Select appropriate test
    if p_normal > 0.05:
        stat, p_value = ttest_rel(ae_vals, codec_vals)
        test_name = "paired t-test"
    else:
        stat, p_value = wilcoxon(ae_vals, codec_vals)
        test_name = "Wilcoxon"

    return {
        'ae_model': ae_name,
        'codec': codec_name,
        'metric': metric,
        'test': test_name,
        'statistic': stat,
        'p_value': p_value,
        'ae_mean': np.mean(ae_vals),
        'codec_mean': np.mean(codec_vals),
        'difference': np.mean(diff),
        'n_samples': len(common_indices),
    }
```

**Cell 6: Run Statistical Tests**
```python
# Comparisons: ResNet vs JPEG-2000 at each ratio (primary)
#              Baseline vs JPEG-2000 at each ratio (secondary)
comparisons = []

for ratio in [4, 8, 16]:
    codec_name = f'jpeg2000_{ratio}x'

    # ResNet vs JPEG-2000
    resnet_name = f'resnet_{ratio}x'
    if resnet_name in per_sample and codec_name in per_sample:
        for metric in ['psnr', 'ssim']:
            result = compare_methods(per_sample, resnet_name, codec_name, metric)
            comparisons.append(result)

    # Baseline vs JPEG-2000
    baseline_name = f'baseline_{ratio}x'
    if baseline_name in per_sample and codec_name in per_sample:
        for metric in ['psnr', 'ssim']:
            result = compare_methods(per_sample, baseline_name, codec_name, metric)
            comparisons.append(result)

# Apply Bonferroni correction
n_tests = len(comparisons)
alpha = 0.05
bonferroni_alpha = alpha / n_tests

for comp in comparisons:
    comp['bonferroni_alpha'] = bonferroni_alpha
    comp['significant_bonferroni'] = comp['p_value'] < bonferroni_alpha

# Save to CSV
stat_df = pd.DataFrame(comparisons)
stat_df.to_csv('tables/statistical_tests.csv', index=False)
print(f"Statistical tests saved: tables/statistical_tests.csv")
print(f"Bonferroni-corrected alpha: {bonferroni_alpha:.6f}")
print(stat_df[['ae_model', 'codec', 'metric', 'difference', 'p_value', 'significant_bonferroni']])
```

Key points:
- Uses per-sample metrics with explicit index matching for paired tests
- Checks normality to choose t-test vs Wilcoxon
- Applies Bonferroni correction for multiple comparisons
- Reports both raw and corrected significance
  </action>
  <verify>Statistical tests CSV exists: `reports/tables/statistical_tests.csv` with columns including p_value and significant_bonferroni</verify>
  <done>Statistical tests complete with Bonferroni correction applied</done>
</task>

<task type="auto">
  <name>Task 3: Generate visual comparison gallery</name>
  <files>reports/figures/visual_comparison_4x.png, reports/figures/visual_comparison_8x.png, reports/figures/visual_comparison_16x.png</files>
  <action>
Add visual comparison cells to the notebook:

**Cell 7: Visual Comparison Setup**
```python
import sys
sys.path.insert(0, '..')
from src.data.datamodule import SARDataModule
from src.models.autoencoder import SARAutoencoder
from src.models.resnet_autoencoder import ResNetAutoencoder
from src.evaluation.codec_baselines import JPEG2000Codec
import torch
import glob

# Load test data
datamodule = SARDataModule(
    patches_path='../data/patches/metadata.npy',
    batch_size=16, val_fraction=0.1, num_workers=0, lazy=True
)
test_loader = datamodule.val_dataloader()

# Get 5 sample images
sample_images = []
for batch in test_loader:
    if isinstance(batch, (tuple, list)):
        batch = batch[0]
    for i in range(min(5 - len(sample_images), batch.shape[0])):
        sample_images.append(batch[i, 0].numpy())
    if len(sample_images) >= 5:
        break

print(f"Loaded {len(sample_images)} sample images")
```

**Cell 8: Generate Comparison Grid for Each Ratio**
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
codec = JPEG2000Codec()

def load_best_ae_for_ratio(ratio: int):
    """Load best autoencoder (ResNet if available, else Baseline) for given ratio."""
    # Map ratio to latent channels
    lc_map = {4: 64, 8: 32, 16: 16}
    lc = lc_map[ratio]

    # Try ResNet first
    resnet_pattern = f'../notebooks/checkpoints/resnet_c{lc}_b64_cr{ratio}x_*'
    matches = sorted(glob.glob(resnet_pattern))
    if matches:
        ckpt_path = matches[-1] + '/best.pth'
        if Path(ckpt_path).exists():
            ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)
            model = ResNetAutoencoder(1, 64, lc)
            model.load_state_dict(ckpt['model_state_dict'])
            return model.to(device).eval(), 'ResNet'

    # Fallback to Baseline
    baseline_pattern = f'../notebooks/checkpoints/baseline_c{lc}_b64_cr{ratio}x_*'
    matches = sorted(glob.glob(baseline_pattern))
    if matches:
        ckpt_path = matches[-1] + '/best.pth'
        if Path(ckpt_path).exists():
            ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)
            model = SARAutoencoder(1, lc)
            model.load_state_dict(ckpt['model_state_dict'])
            return model.to(device).eval(), 'Baseline'

    return None, None

for ratio in [4, 8, 16]:
    print(f"\nGenerating visual comparison for {ratio}x compression...")

    model, model_type = load_best_ae_for_ratio(ratio)
    if model is None:
        print(f"  No model found for {ratio}x, skipping")
        continue

    fig, axes = plt.subplots(5, 4, figsize=(16, 20))
    fig.suptitle(f'{ratio}x Compression: Original | {model_type} | JPEG-2000 | Error Map', fontsize=14)

    for row, img in enumerate(sample_images):
        # Original
        axes[row, 0].imshow(img, cmap='gray', vmin=0, vmax=1)
        axes[row, 0].set_title('Original' if row == 0 else '')
        axes[row, 0].axis('off')

        # Autoencoder reconstruction
        with torch.no_grad():
            img_t = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).float().to(device)
            ae_recon = model(img_t).cpu().numpy()[0, 0]
        axes[row, 1].imshow(ae_recon, cmap='gray', vmin=0, vmax=1)
        axes[row, 1].set_title(f'{model_type}' if row == 0 else '')
        axes[row, 1].axis('off')

        # JPEG-2000 reconstruction
        compressed = codec.compress(img, ratio)
        jp2_recon = codec.decompress(compressed)
        axes[row, 2].imshow(jp2_recon, cmap='gray', vmin=0, vmax=1)
        axes[row, 2].set_title('JPEG-2000' if row == 0 else '')
        axes[row, 2].axis('off')

        # Error map (AE error - JPEG2000 error): blue=AE better, red=JPEG2000 better
        ae_error = np.abs(ae_recon - img)
        jp2_error = np.abs(jp2_recon - img)
        error_diff = jp2_error - ae_error  # Positive = AE better
        axes[row, 3].imshow(error_diff, cmap='RdBu', vmin=-0.2, vmax=0.2)
        axes[row, 3].set_title('Error Diff (blue=AE better)' if row == 0 else '')
        axes[row, 3].axis('off')

    plt.tight_layout()
    save_path = f'figures/visual_comparison_{ratio}x.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  Saved: {save_path}")

    del model
    torch.cuda.empty_cache()
```

Produces 3 visual comparison images (4x, 8x, 16x) with 5 samples each, showing:
- Original image
- Best autoencoder reconstruction
- JPEG-2000 reconstruction
- Error difference map
  </action>
  <verify>
Visual comparisons exist:
```bash
ls -la reports/figures/visual_comparison_*.png
```
Should show 3 files: visual_comparison_4x.png, visual_comparison_8x.png, visual_comparison_16x.png
  </verify>
  <done>Visual comparison gallery generated for all 3 compression ratios</done>
</task>

<task type="auto">
  <name>Task 4: Generate final markdown report</name>
  <files>reports/final_comparison.md</files>
  <action>
Convert notebook to markdown and write final report:

**Step 1: Execute notebook and convert to markdown**
```bash
cd reports
jupyter nbconvert --execute final_comparison.ipynb --to markdown --output final_comparison.md
```

**Step 2: Add executive summary to the markdown file**

Edit `reports/final_comparison.md` to add header and executive summary section at the top:

```markdown
# SAR Image Compression: Autoencoder vs JPEG-2000 Comparison Study

## Executive Summary

This study compares CNN-based autoencoder compression against JPEG-2000 for Sentinel-1 SAR imagery at 4x, 8x, and 16x compression ratios.

### Key Findings

[To be filled based on statistical_tests.csv results:]

- **At 16x compression:** [ResNet/Baseline] achieves [X.XX] dB PSNR vs JPEG-2000's [Y.YY] dB (p < [Z.ZZ])
- **At 8x compression:** [Model comparison summary]
- **At 4x compression:** [Model comparison summary]

### Recommendations

[Based on results - which method is recommended for which use case]

---

[Rest of notebook content follows...]
```

**Step 3: Populate summary from results**

Read `tables/statistical_tests.csv` and `tables/results_summary.csv` to fill in actual values:

```python
import pandas as pd

# Load results
stats = pd.read_csv('reports/tables/statistical_tests.csv')
summary = pd.read_csv('reports/tables/results_summary.csv')

# Extract key comparisons for executive summary
for ratio in [4, 8, 16]:
    resnet = summary[(summary['name'] == f'resnet_{ratio}x')]
    baseline = summary[(summary['name'] == f'baseline_{ratio}x')]
    codec = summary[(summary['name'] == f'jpeg2000_{ratio}x')]

    if not resnet.empty:
        print(f"{ratio}x: ResNet {resnet['psnr_mean'].values[0]:.2f} dB vs JPEG-2000 {codec['psnr_mean'].values[0]:.2f} dB")
    if not baseline.empty:
        print(f"{ratio}x: Baseline {baseline['psnr_mean'].values[0]:.2f} dB vs JPEG-2000 {codec['psnr_mean'].values[0]:.2f} dB")

# Get significant results
sig = stats[stats['significant_bonferroni'] == True]
print("\nStatistically significant improvements:")
print(sig[['ae_model', 'codec', 'metric', 'difference', 'p_value']])
```

Use this data to fill in the executive summary section manually.
  </action>
  <verify>
Final report exists with content:
```bash
head -50 reports/final_comparison.md
```
Should show executive summary header and key findings section.
  </verify>
  <done>Final markdown report complete with executive summary and conclusions</done>
</task>

</tasks>

<verification>
1. Notebook executes: `reports/final_comparison.ipynb` runs without error
2. R-D curves: `reports/figures/rate_distortion_psnr.png`, `reports/figures/rate_distortion_ssim.png`
3. Statistical tests: `reports/tables/statistical_tests.csv` with p-values and Bonferroni correction
4. Visual gallery: `reports/figures/visual_comparison_4x.png`, `visual_comparison_8x.png`, `visual_comparison_16x.png`
5. Final report: `reports/final_comparison.md` with executive summary section
6. nbconvert executed: Markdown generated from notebook
</verification>

<success_criteria>
- Rate-distortion curves clearly show performance of all methods
- Statistical tests determine if autoencoder significantly outperforms JPEG-2000
- Visual gallery demonstrates quality differences with error maps
- Final report is readable with clear conclusions
- All figures are 300 DPI with proper fonts and labels
- Markdown report generated via nbconvert (not just written manually)
</success_criteria>

<output>
After completion, create `.planning/phases/06-final-experiments/06-03-SUMMARY.md`
</output>
