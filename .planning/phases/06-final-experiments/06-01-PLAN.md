---
phase: 06-final-experiments
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - notebooks/checkpoints/resnet_c64_b64_cr4x_*/best.pth
  - notebooks/checkpoints/resnet_c32_b64_cr8x_*/best.pth
  - configs/sweep_resnet_ratios.yaml
autonomous: true

must_haves:
  truths:
    - "ResNet model trained at 4x compression exists with val PSNR > 20 dB"
    - "ResNet model trained at 8x compression exists with val PSNR > 20 dB"
    - "Training completed without CUDA OOM errors"
  artifacts:
    - path: "notebooks/checkpoints/resnet_c64_b64_cr4x_*/best.pth"
      provides: "ResNet 4x trained checkpoint"
    - path: "notebooks/checkpoints/resnet_c32_b64_cr8x_*/best.pth"
      provides: "ResNet 8x trained checkpoint"
    - path: "configs/sweep_resnet_ratios.yaml"
      provides: "Sweep configuration for reproducibility"
  key_links:
    - from: "configs/sweep_resnet_ratios.yaml"
      to: "scripts/train_sweep.py"
      via: "--sweep argument"
      pattern: "sweep.*yaml"
---

<objective>
Train missing ResNet models at 4x and 8x compression ratios to complete the experiment matrix.

Purpose: ResNet 16x already shows +2 dB over baseline - need 4x/8x to build complete rate-distortion curve and demonstrate ResNet advantage across all compression levels.

Output: Two trained ResNet checkpoints (4x at latent_channels=64, 8x at latent_channels=32) ready for evaluation.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-final-experiments/06-CONTEXT.md
@.planning/phases/06-final-experiments/06-RESEARCH.md
@scripts/train_sweep.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sweep configuration for ResNet 4x and 8x</name>
  <files>configs/sweep_resnet_ratios.yaml</files>
  <action>
Create YAML sweep config for ResNet at 4x and 8x compression:

```yaml
# Sweep configuration for ResNet at 4x and 8x compression ratios
# Run with: python scripts/train_sweep.py --sweep configs/sweep_resnet_ratios.yaml

defaults:
  model: resnet
  base_channels: 64
  learning_rate: 1e-4
  epochs: 35
  early_stopping_patience: 15
  batch_size: 16  # ResNet needs smaller batch on 8GB VRAM
  scheduler: plateau
  optimizer: adamw
  use_amp: true
  mse_weight: 0.5
  ssim_weight: 0.5

runs:
  # 4x compression: latent_channels = 256*256 / (16*16*4) = 64
  - latent_channels: 64
    batch_size: 8  # ResNet with 64 latent channels is memory-heavy

  # 8x compression: latent_channels = 256*256 / (16*16*8) = 32
  - latent_channels: 32
    batch_size: 12
```

Place in `configs/` directory (create if needed).
  </action>
  <verify>cat configs/sweep_resnet_ratios.yaml shows valid YAML with 2 runs</verify>
  <done>Sweep config exists with ResNet 4x (LC=64) and 8x (LC=32) configurations</done>
</task>

<task type="auto">
  <name>Task 2: Train ResNet at 4x and 8x using sweep script</name>
  <files>notebooks/checkpoints/</files>
  <action>
Execute training sweep for ResNet models:

```bash
python scripts/train_sweep.py --sweep configs/sweep_resnet_ratios.yaml --data-path D:/Projects/CNNAutoencoderProject/data/patches/metadata.npy
```

Training parameters per CONTEXT.md:
- 10% data (default via SARDataModule val_fraction handling)
- 35 epochs
- LR=1e-4 with ReduceLROnPlateau
- AdamW optimizer
- AMP enabled for VRAM efficiency

Monitor for:
- CUDA OOM: Reduce batch_size if needed (8 -> 6 for 4x, 12 -> 8 for 8x)
- Training progress: val_psnr should increase over epochs
- Checkpoint saving: best.pth created after first validation

Expected runtime: ~2-3 hours total (35 epochs each, two models)

Note: If training takes too long, can train 4x first (more memory-heavy), then 8x:
```bash
python scripts/train_sweep.py --model resnet --latent-channels 64 --base-channels 64 --batch-size 8 --epochs 35
python scripts/train_sweep.py --model resnet --latent-channels 32 --base-channels 64 --batch-size 12 --epochs 35
```
  </action>
  <verify>
Verify checkpoints exist:
```bash
ls -la notebooks/checkpoints/resnet_c64_b64_cr4x_*/best.pth
ls -la notebooks/checkpoints/resnet_c32_b64_cr8x_*/best.pth
```
Both should exist with file size > 50MB (ResNet model is ~22M params)
  </verify>
  <done>ResNet 4x and 8x checkpoints exist in notebooks/checkpoints/ with valid best.pth files</done>
</task>

<task type="auto">
  <name>Task 3: Quick validation of trained models</name>
  <files></files>
  <action>
Run quick evaluation on both new checkpoints to verify they trained correctly:

```bash
# Find the checkpoint paths
RESNET_4X=$(ls -d notebooks/checkpoints/resnet_c64_b64_cr4x_* | tail -1)
RESNET_8X=$(ls -d notebooks/checkpoints/resnet_c32_b64_cr8x_* | tail -1)

# Quick evaluation (limited samples)
python scripts/evaluate_model.py --checkpoint "$RESNET_4X/best.pth" --n-samples 500 --no-visualizations
python scripts/evaluate_model.py --checkpoint "$RESNET_8X/best.pth" --n-samples 500 --no-visualizations
```

Expected results:
- ResNet 4x: PSNR > 24 dB (similar to Baseline 4x at 24.15 dB or better)
- ResNet 8x: PSNR > 22 dB (improvement over Baseline 8x at 21.34 dB)

If PSNR is significantly lower, check training logs for issues.
  </action>
  <verify>Evaluation output shows PSNR values. ResNet 4x > 23 dB, ResNet 8x > 21 dB (reasonable for untrained-to-completion models)</verify>
  <done>Both ResNet checkpoints pass quick validation with PSNR above threshold</done>
</task>

</tasks>

<verification>
1. Sweep config exists: `configs/sweep_resnet_ratios.yaml`
2. ResNet 4x checkpoint: `notebooks/checkpoints/resnet_c64_b64_cr4x_*/best.pth` (file > 50MB)
3. ResNet 8x checkpoint: `notebooks/checkpoints/resnet_c32_b64_cr8x_*/best.pth` (file > 50MB)
4. Quick eval confirms reasonable PSNR for both models
</verification>

<success_criteria>
- Two new ResNet checkpoints trained and saved
- ResNet 4x achieves PSNR > 23 dB on validation
- ResNet 8x achieves PSNR > 21 dB on validation
- Training completed without OOM errors
- Models ready for systematic evaluation in Plan 02
</success_criteria>

<output>
After completion, create `.planning/phases/06-final-experiments/06-01-SUMMARY.md`
</output>
