---
phase: 06-final-experiments
plan: 02
type: execute
wave: 2
depends_on: [06-01]
files_modified:
  - reports/data/all_results.json
  - reports/data/per_sample_metrics.json
  - reports/tables/results_summary.csv
  - scripts/run_evaluation_sweep.py
autonomous: true

must_haves:
  truths:
    - "All 6 autoencoder models evaluated with full metrics (PSNR, SSIM, MS-SSIM, ENL ratio, EPI)"
    - "JPEG-2000 evaluated at 4x, 8x, 16x compression ratios"
    - "Per-sample metrics saved for statistical analysis"
  artifacts:
    - path: "reports/data/all_results.json"
      provides: "Aggregated evaluation results for all models and codecs"
    - path: "reports/data/per_sample_metrics.json"
      provides: "Per-sample metrics for paired statistical tests"
    - path: "reports/tables/results_summary.csv"
      provides: "Summary metrics table for report"
    - path: "scripts/run_evaluation_sweep.py"
      provides: "Reproducible evaluation script"
  key_links:
    - from: "scripts/run_evaluation_sweep.py"
      to: "src/evaluation/evaluator.py"
      via: "Evaluator class"
      pattern: "Evaluator\\("
    - from: "scripts/run_evaluation_sweep.py"
      to: "src/evaluation/codec_baselines.py"
      via: "CodecEvaluator class"
      pattern: "CodecEvaluator\\("
---

<objective>
Systematically evaluate all models and JPEG-2000 codec, collecting per-sample metrics for statistical comparison.

Purpose: Build the complete dataset for rate-distortion analysis and statistical hypothesis testing (autoencoder vs JPEG-2000).

Output: JSON files with evaluation results, per-sample metrics for statistics, and summary CSV for reporting.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/06-final-experiments/06-CONTEXT.md
@.planning/phases/06-final-experiments/06-RESEARCH.md
@src/evaluation/evaluator.py
@src/evaluation/codec_baselines.py
@scripts/evaluate_model.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create systematic evaluation script</name>
  <files>scripts/run_evaluation_sweep.py</files>
  <action>
Create a comprehensive evaluation script that:
1. Evaluates all 6 autoencoder checkpoints (Baseline 4x/8x/16x + ResNet 4x/8x/16x)
2. Evaluates JPEG-2000 at matching compression ratios
3. Saves per-sample metrics for statistical analysis
4. Outputs structured JSON and CSV

Key implementation points:
- Use existing `Evaluator` class from `src/evaluation/evaluator.py`
- Use existing `CodecEvaluator` from `src/evaluation/codec_baselines.py`
- Use existing `load_model_from_checkpoint` from `scripts/evaluate_model.py`
- Save per-sample metrics (not just aggregates) for paired t-tests/Wilcoxon
- Match autoencoder and codec evaluation on SAME test samples

Model checkpoints to evaluate:
```python
CHECKPOINTS = {
    # Baseline models
    'baseline_4x': 'notebooks/checkpoints/baseline_c64_b64_cr4x_20260127_195355/best.pth',
    'baseline_8x': 'notebooks/checkpoints/baseline_c32_b64_cr8x_20260127_205741/best.pth',
    'baseline_16x': 'notebooks/checkpoints/baseline_c16_b64_cr16x_20260127_231730/best.pth',
    # ResNet models (paths from Plan 01 + existing)
    'resnet_16x': 'notebooks/checkpoints/resnet_c16_b64_cr16x_20260128_003926/best.pth',
    # resnet_4x and resnet_8x paths determined dynamically
}
```

Output structure:
```
reports/
    data/
        all_results.json       # Full evaluation results
        per_sample_metrics.json # Per-image metrics for stats
    tables/
        results_summary.csv    # Quick reference table
```

Use JPEG-2000 only (per CONTEXT.md - JPEG excluded as known inferior).
  </action>
  <verify>python scripts/run_evaluation_sweep.py --help shows usage</verify>
  <done>Evaluation script exists with all required functionality</done>
</task>

<task type="auto">
  <name>Task 2: Run evaluation sweep on all models</name>
  <files>reports/data/all_results.json, reports/data/per_sample_metrics.json</files>
  <action>
Execute the evaluation sweep:

```bash
# Create output directories
mkdir -p reports/data reports/tables reports/figures

# Run evaluation
python scripts/run_evaluation_sweep.py \
    --output-dir reports \
    --n-samples 2000 \
    --batch-size 16
```

This will:
1. Load each checkpoint and evaluate on test set
2. Compute PSNR, SSIM, MS-SSIM, ENL ratio, EPI for each sample
3. Evaluate JPEG-2000 at 4x, 8x, 16x on same images
4. Save all results to JSON files

Expected runtime: ~30-45 minutes (2000 samples x 6 models + codec)

Monitor for:
- Memory usage during codec evaluation
- All models loading correctly (check for state_dict errors)
- JSON files being written incrementally (if script supports it)
  </action>
  <verify>
Check output files exist and contain data:
```bash
cat reports/data/all_results.json | python -c "import json,sys; d=json.load(sys.stdin); print(f'Models: {len(d)}')"
cat reports/tables/results_summary.csv | head -10
```
  </verify>
  <done>Evaluation complete with all 6 autoencoders and JPEG-2000 at 3 ratios</done>
</task>

<task type="auto">
  <name>Task 3: Validate evaluation results</name>
  <files></files>
  <action>
Verify the evaluation produced sensible results:

```python
import json
import pandas as pd

# Load results
with open('reports/data/all_results.json') as f:
    results = json.load(f)

# Check model count
print(f"Total entries: {len(results)}")

# Verify per-sample data exists
with open('reports/data/per_sample_metrics.json') as f:
    per_sample = json.load(f)

for model_name, samples in per_sample.items():
    print(f"{model_name}: {len(samples)} samples")

# Quick sanity check: PSNR values should be positive and reasonable
df = pd.read_csv('reports/tables/results_summary.csv')
print(df[['name', 'compression_ratio', 'psnr_mean', 'ssim_mean']])
```

Expected observations:
- All 6 autoencoder models present
- JPEG-2000 at 3 ratios present
- PSNR values: 18-30 dB range
- SSIM values: 0.5-0.95 range
- Per-sample counts: All should have same number of samples (~2000)
  </action>
  <verify>Summary table shows 9 rows (6 autoencoders + 3 JPEG-2000) with reasonable metrics</verify>
  <done>Evaluation data validated - all models present with reasonable metrics</done>
</task>

</tasks>

<verification>
1. Script exists: `scripts/run_evaluation_sweep.py`
2. Results JSON: `reports/data/all_results.json` with 9 entries
3. Per-sample JSON: `reports/data/per_sample_metrics.json` with per-image data
4. Summary CSV: `reports/tables/results_summary.csv` with all models
5. PSNR/SSIM values are in reasonable ranges
</verification>

<success_criteria>
- All 6 autoencoder models evaluated
- JPEG-2000 evaluated at 4x, 8x, 16x
- Per-sample metrics saved for statistical analysis
- Results in structured format ready for analysis
- No missing data or NaN values in key metrics
</success_criteria>

<output>
After completion, create `.planning/phases/06-final-experiments/06-02-SUMMARY.md`
</output>
