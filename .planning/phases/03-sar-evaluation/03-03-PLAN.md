---
phase: 03-sar-evaluation
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/evaluation/evaluator.py
  - src/evaluation/visualizer.py
  - src/evaluation/__init__.py
  - scripts/evaluate_model.py
autonomous: true

must_haves:
  truths:
    - "Evaluation script generates complete metrics report for any trained model"
    - "Visual comparison produces side-by-side images with difference maps and zoomed crops"
    - "Rate-distortion curve generation works for multiple models/codecs"
    - "JSON output contains all metrics with mean/std statistics"
  artifacts:
    - path: "src/evaluation/evaluator.py"
      provides: "Updated Evaluator with JSON output and ENL ratio integration"
      exports: ["Evaluator", "print_evaluation_report"]
      min_lines: 350
    - path: "src/evaluation/visualizer.py"
      provides: "Enhanced visualizer with zoomed crops and better difference maps"
      exports: ["Visualizer"]
      min_lines: 400
    - path: "scripts/evaluate_model.py"
      provides: "CLI evaluation script"
      min_lines: 150
  key_links:
    - from: "scripts/evaluate_model.py"
      to: "src/evaluation/evaluator.py"
      via: "Evaluator class"
      pattern: "from src.evaluation import Evaluator"
    - from: "src/evaluation/evaluator.py"
      to: "src/evaluation/metrics.py"
      via: "compute_all_metrics, enl_ratio"
      pattern: "from .metrics import"
    - from: "src/evaluation/evaluator.py"
      to: "evaluations/"
      via: "JSON output"
      pattern: "json.dump"
---

<objective>
Create complete evaluation infrastructure: update Evaluator with structured JSON output and ENL ratio integration, enhance Visualizer with zoomed crops and diverging colormaps, build evaluation script for CLI usage.

Purpose: Tie together all metrics and visualization into a cohesive evaluation pipeline that can assess any trained model and produce publication-quality outputs.

Output: Updated evaluator.py and visualizer.py, new scripts/evaluate_model.py, evaluation outputs in evaluations/ directory.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-sar-evaluation/03-RESEARCH.md
@.planning/phases/03-sar-evaluation/03-CONTEXT.md
@.planning/phases/03-sar-evaluation/03-01-SUMMARY.md
@.planning/phases/03-sar-evaluation/03-02-SUMMARY.md
@src/evaluation/evaluator.py
@src/evaluation/visualizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update Evaluator with comprehensive metrics and JSON output</name>
  <files>src/evaluation/evaluator.py</files>
  <action>
Enhance Evaluator class with full metric suite and structured JSON output.

1. Update imports to use new metrics:
   ```python
   from .metrics import (
       SARMetrics, compute_all_metrics, compute_ms_ssim,
       enl_ratio, compute_bpp, compute_compression_ratio
   )
   ```

2. Update `evaluate_batch` to include all metrics:
   - Add ms_ssim, enl_ratio (full dict), local_variance_ratio
   - Use compute_all_metrics for consistency
   - Handle edge cases (NaN values, small images)

3. Add `evaluate_dataset` improvements:
   - Return both summary stats AND per-sample detailed results
   - Include model metadata (name, checkpoint path, compression ratio)
   - Include preprocessing params from checkpoint

4. Add structured JSON output methods:
   ```python
   def save_summary(self, results: dict, output_path: str):
       """Save summary metrics to JSON."""
       # Include model_name, checkpoint, evaluation_date, num_samples
       # Include mean/std for all metrics
       # Include preprocessing params for reproducibility

   def save_detailed(self, per_sample_results: list, output_path: str):
       """Save per-sample metrics to JSON."""
       # Each sample: index, all metrics

   def save_results(self, results: dict, output_dir: str, model_name: str):
       """Save both summary and detailed results."""
       # Create evaluations/{model_name}/ directory
       # Save {model_name}_eval.json (summary)
       # Save {model_name}_detailed.json (per-sample)
   ```

5. Add rate-distortion data collection:
   ```python
   def collect_rd_point(self, compression_ratio: float, metrics: dict) -> dict:
       """Format metrics as rate-distortion data point."""
       return {
           'name': self.model_name,
           'compression_ratio': compression_ratio,
           'bpp': compute_bpp(...),
           'psnr': metrics['psnr']['mean'],
           'ssim': metrics['ssim']['mean'],
           'ms_ssim': metrics['ms_ssim']['mean'] if 'ms_ssim' in metrics else None,
       }
   ```

6. Update the duplicate SARMetrics class in evaluator.py:
   - Remove it - use the one from metrics.py instead
   - This consolidates implementations
  </action>
  <verify>
Test updated Evaluator:
```python
import torch
import numpy as np
from pathlib import Path

# Create dummy model for testing
class DummyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = torch.nn.Conv2d(1, 16, 3, padding=1)

    def forward(self, x):
        z = self.encoder(x)
        x_hat = x + 0.02 * torch.randn_like(x)
        return x_hat.clamp(0, 1), z

    def encode(self, x):
        return self.encoder(x)

from src.evaluation.evaluator import Evaluator

model = DummyModel()
evaluator = Evaluator(model, device='cpu')
evaluator.model_name = 'test_model'

# Create test batch
batch = torch.rand(4, 1, 256, 256)
x_hat, metrics = evaluator.evaluate_batch(batch)

print(f"Metrics keys: {list(metrics.keys())}")
assert 'psnr' in metrics and 'ssim' in metrics
assert 'enl_ratio' in metrics or 'epi' in metrics  # depending on implementation

# Test JSON output structure would go here once implemented
print("Evaluator update test passed")
```
  </verify>
  <done>
Evaluator uses compute_all_metrics from metrics.py. evaluate_batch includes enl_ratio and ms_ssim. save_summary and save_detailed output valid JSON. Rate-distortion point collection works.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance Visualizer with zoomed crops and better difference maps</name>
  <files>src/evaluation/visualizer.py</files>
  <action>
Enhance Visualizer per CONTEXT.md decisions.

1. Update `plot_reconstruction_grid` or add new method for comprehensive comparison:
   ```python
   def plot_comparison(self, original: np.ndarray, reconstructed: np.ndarray,
                       metrics: dict, save_path: str,
                       zoom_regions: List[tuple] = None,
                       auto_zoom: bool = True):
       """
       Comprehensive comparison visualization.

       Layout:
       - Row 1: Original, Reconstructed, Difference (diverging colormap)
       - Row 2: Zoomed crops (auto-selected or manual regions)
       - Include metrics in title/figure text

       Args:
           zoom_regions: List of (y0, y1, x0, x1) tuples for manual regions
           auto_zoom: If True and no zoom_regions, auto-select high-error regions
       """
   ```

2. Implement auto-zoom region selection:
   ```python
   def _find_interesting_regions(self, diff: np.ndarray, n_regions: int = 2) -> List[tuple]:
       """Find regions with highest absolute error."""
       # Divide image into grid
       # Find cells with highest mean absolute difference
       # Return (y0, y1, x0, x1) for each region
   ```

3. Improve difference map visualization:
   - Use RdBu_r colormap (already done in existing code)
   - Ensure vmin=-max_abs, vmax=max_abs (center at 0)
   - Add colorbar with label "Difference (blue=under, red=over)"

4. Add `plot_rate_distortion` function:
   ```python
   def plot_rate_distortion(self, results: List[dict], output_path: str,
                            title: str = "Rate-Distortion Comparison"):
       """
       Plot PSNR vs BPP and SSIM vs BPP.

       Args:
           results: List of dicts with keys: name, bpp, psnr, ssim
       """
       # Two subplots: PSNR vs BPP, SSIM vs BPP
       # Group by name, plot as lines with markers
       # Include legend, grid, labels
   ```

5. Add `plot_histogram_overlay` method for intensity distribution comparison:
   - Use semitransparent overlaid histograms
   - Include histogram similarity metric in title

6. Add `save_enl_mask` method (per CONTEXT.md):
   - Save homogeneous region mask as PNG for inspection
  </action>
  <verify>
Test enhanced Visualizer:
```python
import numpy as np
from src.evaluation.visualizer import Visualizer

viz = Visualizer(save_dir='test_viz')

# Test data
orig = np.random.rand(256, 256).astype(np.float32)
recon = np.clip(orig + 0.05 * np.random.randn(256, 256), 0, 1).astype(np.float32)

# Test comprehensive comparison
metrics = {'psnr': 26.5, 'ssim': 0.92, 'epi': 0.95}
fig = viz.plot_comparison(orig, recon, metrics, 'test_comparison.png',
                          auto_zoom=True, show=False)
print(f"Figure has {len(fig.axes)} axes")

# Test rate-distortion plot
rd_data = [
    {'name': 'Autoencoder', 'bpp': 0.5, 'psnr': 28, 'ssim': 0.92},
    {'name': 'Autoencoder', 'bpp': 0.25, 'psnr': 25, 'ssim': 0.85},
    {'name': 'JPEG-2000', 'bpp': 0.5, 'psnr': 26, 'ssim': 0.88},
    {'name': 'JPEG-2000', 'bpp': 0.25, 'psnr': 22, 'ssim': 0.78},
]
viz.plot_rate_distortion(rd_data, 'test_rd.png', show=False)
print("Rate-distortion plot created")

# Cleanup
import shutil
shutil.rmtree('test_viz', ignore_errors=True)
print("Visualizer tests passed")
```
  </verify>
  <done>
plot_comparison shows original, reconstructed, difference, and zoomed crops. Difference maps use diverging colormap centered at 0. plot_rate_distortion creates PSNR/SSIM vs BPP curves. Auto-zoom finds high-error regions.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create CLI evaluation script</name>
  <files>scripts/evaluate_model.py</files>
  <action>
Create comprehensive evaluation script for CLI usage.

1. Create `scripts/evaluate_model.py` with argparse:
   ```python
   """
   Evaluate trained SAR autoencoder model.

   Usage:
       python scripts/evaluate_model.py --checkpoint path/to/best.pth --output evaluations/model_name
       python scripts/evaluate_model.py --checkpoint path/to/best.pth --compare-codecs
   """
   import argparse
   from pathlib import Path
   import torch
   import json
   # ... imports
   ```

2. Command-line arguments:
   - `--checkpoint`: Path to model checkpoint (required)
   - `--output`: Output directory (default: evaluations/{checkpoint_name})
   - `--data-dir`: Path to test data (default: from datamodule)
   - `--batch-size`: Batch size for evaluation (default: 16)
   - `--n-samples`: Number of samples to evaluate (default: all)
   - `--compare-codecs`: Also evaluate JPEG-2000 and JPEG at same compression ratio
   - `--compression-ratios`: List of ratios for codec comparison (default: 8,16,32)
   - `--n-visualizations`: Number of comparison images to generate (default: 5)
   - `--device`: cuda or cpu (default: cuda if available)

3. Main evaluation flow:
   ```python
   def main(args):
       # Load checkpoint and model
       checkpoint = torch.load(args.checkpoint)
       model = load_model_from_checkpoint(checkpoint)
       compression_ratio = get_compression_ratio(model)

       # Load test data
       datamodule = SARDataModule(...)
       test_loader = datamodule.test_dataloader()

       # Evaluate model
       evaluator = Evaluator(model, device=args.device)
       results = evaluator.evaluate_dataset(test_loader)

       # Save results
       output_dir = Path(args.output)
       output_dir.mkdir(parents=True, exist_ok=True)
       evaluator.save_results(results, output_dir, model_name)

       # Generate visualizations
       visualizer = Visualizer(save_dir=output_dir / 'comparisons')
       for i, sample in enumerate(samples[:args.n_visualizations]):
           visualizer.plot_comparison(...)

       # Compare with codecs if requested
       if args.compare_codecs:
           for codec_class in [JPEG2000Codec, JPEGCodec]:
               codec_evaluator = CodecEvaluator(codec_class())
               codec_results = codec_evaluator.evaluate_batch(...)
               # Add to rate-distortion data

       # Generate rate-distortion plot
       visualizer.plot_rate_distortion(rd_data, output_dir / 'rate_distortion.png')

       print_evaluation_report(results)
   ```

4. Handle model loading:
   - Support both SARAutoencoder and ResNetAutoencoder
   - Extract model type from checkpoint metadata if available
   - Fallback to inferring from architecture

5. Create evaluations/ directory structure:
   ```
   evaluations/
     {model_name}/
       {model_name}_eval.json       # Summary metrics
       {model_name}_detailed.json   # Per-sample metrics
       rate_distortion.csv          # R-D data
       rate_distortion.png          # R-D plot
       comparisons/
         sample_01.png
         sample_02.png
         ...
   ```

6. Add progress output:
   - Use tqdm for batch evaluation
   - Print summary at end
  </action>
  <verify>
Test evaluation script (dry run without actual model):
```bash
cd D:\Projects\CNNAutoencoderProject
python scripts/evaluate_model.py --help
```

If checkpoint exists, run actual evaluation:
```bash
cd D:\Projects\CNNAutoencoderProject
# Check if checkpoint exists
if [ -f "notebooks/checkpoints/resnet_lite_v2_c16/best.pth" ]; then
    python scripts/evaluate_model.py \
        --checkpoint notebooks/checkpoints/resnet_lite_v2_c16/best.pth \
        --output evaluations/resnet_lite_v2_c16 \
        --n-samples 100 \
        --n-visualizations 3
fi
```
  </verify>
  <done>
scripts/evaluate_model.py runs with --help. Script loads checkpoint, evaluates on test data, saves JSON results, generates visualizations. Output structure matches specification.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `python scripts/evaluate_model.py --help` shows all options
2. Evaluator.save_summary outputs valid JSON with all metrics
3. Visualizer.plot_comparison creates figure with zoomed crops
4. Visualizer.plot_rate_distortion creates multi-model comparison
5. Full pipeline: checkpoint -> evaluation -> JSON + visualizations
</verification>

<success_criteria>
1. Evaluation script generates complete metrics report (PSNR, SSIM, MS-SSIM, ENL ratio, EPI) for trained model
2. Visual comparison produces side-by-side images with difference maps and zoomed crops
3. Rate-distortion curve generation works for multiple models/codecs on same plot
4. JSON output contains model metadata, mean/std for all metrics, preprocessing params
5. evaluations/{model_name}/ directory created with all outputs
</success_criteria>

<output>
After completion, create `.planning/phases/03-sar-evaluation/03-03-SUMMARY.md`
</output>
