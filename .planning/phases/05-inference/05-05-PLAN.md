---
phase: 05-inference
plan: 05
type: execute
wave: 4
depends_on: ["05-04"]
files_modified:
  - notebooks/test_full_inference.ipynb
autonomous: false

must_haves:
  truths:
    - "10000x10000 image processes without OOM"
    - "No visible tile boundaries in reconstructed image"
    - "Round-trip PSNR within 0.5 dB of patch-level metrics"
    - "Processing time under 5 minutes for full scene"
    - "GeoTIFF metadata preserved through round-trip"
  artifacts:
    - path: "notebooks/test_full_inference.ipynb"
      provides: "Full inference validation notebook"
      contains: "SARCompressor"
    - path: "notebooks/evaluations/full_inference_results.json"
      provides: "Benchmark results"
  key_links:
    - from: "notebooks/test_full_inference.ipynb"
      to: "scripts/sarcodec.py"
      via: "CLI validation"
      pattern: "sarcodec"
---

<objective>
Validate the full inference pipeline on real SAR data and benchmark performance.

Purpose: Verify that the complete tiled inference system meets all Phase 5 success criteria - seamless reconstruction, memory efficiency, performance targets, and metadata preservation.

Output: Validation notebook with benchmarks and visual verification of seamless blending.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-inference/05-RESEARCH.md
@.planning/phases/05-inference/05-03-SUMMARY.md
@.planning/phases/05-inference/05-04-SUMMARY.md

# Phase 5 success criteria from ROADMAP.md:
# 1. 10000x10000 processes without OOM
# 2. Seamless blending (no visible tile boundaries)
# 3. Processing time < 5 minutes
# 4. Inverse preprocessing restores linear values
# 5. Round-trip PSNR within 0.5 dB of patch-level
# 6. Raw GeoTIFF compress/decompress via CLI
# 7. Geospatial metadata preserved
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create validation notebook</name>
  <files>notebooks/test_full_inference.ipynb</files>
  <action>
Create `notebooks/test_full_inference.ipynb` with the following sections:

**Cell 1: Setup**
```python
import numpy as np
import torch
import time
import json
from pathlib import Path
import matplotlib.pyplot as plt

from src.inference import SARCompressor
from src.inference.geotiff import read_geotiff, write_geotiff, GeoMetadata
from src.inference.tiling import create_cosine_ramp_weights, visualize_blend_weights
from src.evaluation.metrics import compute_psnr, compute_ssim

MODEL_PATH = "checkpoints/resnet_lite_v2_c16/best.pth"
```

**Cell 2: Test 1 - Synthetic Large Image (Memory Test)**
- Create 4096x4096 synthetic normalized image (cosine patterns)
- Initialize SARCompressor
- Time compress + decompress
- Report memory usage (torch.cuda.max_memory_allocated)
- Verify no OOM

**Cell 3: Test 2 - Seamless Blending Verification**
- Create 1024x1024 image with high-frequency pattern (grid or checkerboard)
- Compress and decompress
- Compute difference image
- Check that tile boundaries are NOT visible in difference
- Visualize: original, reconstructed, difference, highlight tile grid overlay

**Cell 4: Test 3 - PSNR Consistency**
- Load validation patches from training data (10 random patches)
- For each patch:
  - Direct model inference (single patch) -> patch_psnr
  - Embed patch in 512x512 image, full pipeline -> full_psnr
  - Compare: |patch_psnr - full_psnr| should be < 0.5 dB
- Report statistics

**Cell 5: Test 4 - Preprocessing Round-Trip**
- Create synthetic SAR-like data (exponential distribution)
- Full pipeline: raw -> preprocess -> compress -> decompress -> inverse_preprocess
- Verify output is in similar range as input (not normalized [0,1])
- Compute correlation between original and reconstructed

**Cell 6: Test 5 - CLI Smoke Test**
- Create temp GeoTIFF with synthetic data and mock CRS
- Run: `!python ../scripts/sarcodec.py compress temp.tif -o temp.npz`
- Run: `!python ../scripts/sarcodec.py decompress temp.npz -o temp_out.tif`
- Load temp_out.tif, verify metadata preserved
- Clean up temp files

**Cell 7: Results Summary**
- Compile all test results into dict
- Save to notebooks/evaluations/full_inference_results.json
- Print pass/fail for each criterion
  </action>
  <verify>
Notebook file exists at notebooks/test_full_inference.ipynb
  </verify>
  <done>
- Notebook has all 5 test sections plus summary
- Tests cover memory, blending, PSNR, preprocessing, CLI
- Results saved to JSON
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Full inference pipeline with tiled processing, blending, and CLI</what-built>
  <how-to-verify>
1. Open `notebooks/test_full_inference.ipynb` in Jupyter
2. Run all cells
3. Verify each test passes:
   - Test 1: No OOM on 4096x4096 image
   - Test 2: Difference image shows no visible tile grid
   - Test 3: PSNR difference < 0.5 dB
   - Test 4: Output in linear SAR range (not [0,1])
   - Test 5: CLI round-trip works, metadata preserved
4. Check visualizations - blending should be seamless
5. Review timing - should be reasonable (< 2 min for 4096x4096)
6. Check notebooks/evaluations/full_inference_results.json exists
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
1. Notebook exists: `notebooks/test_full_inference.ipynb`
2. All notebook cells execute without error
3. All 5 tests pass
4. Results JSON exists: `notebooks/evaluations/full_inference_results.json`
5. Visualizations show seamless blending
</verification>

<success_criteria>
- 4096x4096 synthetic image processes without OOM
- No visible tile boundaries in difference image
- PSNR within 0.5 dB of patch-level metrics
- CLI round-trip preserves data and metadata
- All tests documented in results JSON
</success_criteria>

<output>
After completion, create `.planning/phases/05-inference/05-05-SUMMARY.md`
</output>
