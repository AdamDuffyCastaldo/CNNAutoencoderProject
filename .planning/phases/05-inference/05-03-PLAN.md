---
phase: 05-inference
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/inference/compressor.py
  - src/inference/__init__.py
autonomous: true

must_haves:
  truths:
    - "Model loads from checkpoint with preprocessing parameters"
    - "Single patch compresses and decompresses correctly"
    - "Full image compresses via tiled processing without OOM"
    - "Decompressed image matches original within PSNR tolerance"
    - "Preprocessing uses saved model parameters automatically"
  artifacts:
    - path: "src/inference/compressor.py"
      provides: "Complete SARCompressor class"
      exports: ["SARCompressor"]
    - path: "src/inference/__init__.py"
      provides: "Module exports"
      contains: "SARCompressor"
  key_links:
    - from: "SARCompressor.compress"
      to: "src/inference/tiling.py"
      via: "extract_tiles, reconstruct_from_tiles"
      pattern: "from.*tiling.*import"
    - from: "SARCompressor._load_model"
      to: "src/models/resnet_autoencoder.py"
      via: "model instantiation"
      pattern: "ResNetAutoencoder"
---

<objective>
Implement the complete SARCompressor class for tiled image compression and decompression.

Purpose: Enable compression of full Sentinel-1 scenes (10000x10000+ pixels) through memory-efficient tiled processing with the trained ResNet-Lite v2 model.

Output: Fully functional `src/inference/compressor.py` replacing the existing stub.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-inference/05-RESEARCH.md
@.planning/phases/05-inference/05-01-SUMMARY.md

# Key info:
# - Best checkpoint: notebooks/checkpoints/resnet_lite_v2_c16/best.pth
# - Model: ResNetAutoencoder with latent_channels=16, base_channels=32
# - Preprocessing params stored in checkpoint: vmin, vmax
# - Default tile overlap: 64 pixels (25%)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement core SARCompressor functionality</name>
  <files>src/inference/compressor.py</files>
  <action>
Replace the stub implementation in `src/inference/compressor.py` with full functionality:

**__init__(self, model_path, device=None, patch_size=256, overlap=64, batch_size=None)**
- Auto-detect device (CUDA if available)
- Store patch_size, overlap, compute stride
- Call _load_model(model_path)
- If batch_size is None, call _auto_detect_batch_size()
- Create blend weights via create_cosine_ramp_weights()

**_load_model(self, model_path)**
- Load checkpoint with torch.load(model_path, map_location=self.device, weights_only=False)
- Extract config from checkpoint (latent_channels, base_channels)
- Extract preprocess_params (vmin, vmax) - CRITICAL for consistent preprocessing
- Determine model class from config (default: ResNetAutoencoder)
- Instantiate model with correct parameters
- Load state dict
- Move to device, set eval mode
- Store self.model, self.config, self.preprocess_params

**_auto_detect_batch_size(self) -> int**
- If not CUDA, return 1
- Get total VRAM from torch.cuda.get_device_properties
- Estimate ~3MB per 256x256 tile (conservative)
- Use 70% of free VRAM
- Clamp to range [1, 64]
- Return batch size

**preprocess(self, image: np.ndarray) -> np.ndarray**
- Use self.preprocess_params['vmin'] and ['vmax']
- Handle invalid values (<=0, NaN, Inf) with noise_floor=1e-10
- Convert to dB: 10 * log10(image_clean)
- Clip to [vmin, vmax]
- Normalize to [0, 1]
- Return float32 array

**inverse_preprocess(self, normalized: np.ndarray) -> np.ndarray**
- Denormalize: db = normalized * (vmax - vmin) + vmin
- Convert to linear: 10 ** (db / 10)
- Return float32 array

**compress(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]**
- Preprocess image
- Extract tiles via extract_tiles()
- Process tiles in batches through encoder
- Use torch.inference_mode() and optional AMP (torch.amp.autocast)
- Return latent_patches (N, C, 16, 16) and metadata dict

**decompress(self, latent_patches: np.ndarray, metadata: Dict) -> np.ndarray**
- Process latent patches through decoder in batches
- Reconstruct full image via reconstruct_from_tiles()
- Inverse preprocess to get linear SAR values
- Return reconstructed image

**get_compression_stats(self, image, latent_patches) -> Dict**
- Compute compression_ratio, bpp, latent_shape, original_shape

Import from src.inference.tiling: create_cosine_ramp_weights, extract_tiles, reconstruct_from_tiles
Import from src.models.resnet_autoencoder: ResNetAutoencoder
  </action>
  <verify>
Run: `python -c "from src.inference.compressor import SARCompressor; print('SARCompressor importable')"`
Expected: "SARCompressor importable" (no NotImplementedError on import)
  </verify>
  <done>
- SARCompressor initializes with model checkpoint
- Model loads with correct architecture and preprocessing params
- All stub methods replaced with implementations
- No NotImplementedError on import
  </done>
</task>

<task type="auto">
  <name>Task 2: Add batch processing and test on real data</name>
  <files>src/inference/compressor.py, src/inference/__init__.py</files>
  <action>
1. Add batched inference helper to SARCompressor:

**_process_tiles_batched(self, tiles: np.ndarray, encode: bool = True) -> np.ndarray**
- Process tiles through encoder (if encode=True) or decoder (if encode=False)
- Batch by self.batch_size
- Use AMP if device is CUDA
- Return processed tiles as numpy array

2. Add progress callback support:

**compress(self, image, progress_callback=None) -> ...**
**decompress(self, latent_patches, metadata, progress_callback=None) -> ...**
- If progress_callback provided, call it with (current_batch, total_batches)
- Useful for CLI progress bars

3. Update `src/inference/__init__.py`:
```python
from .compressor import SARCompressor
from .tiling import create_cosine_ramp_weights, extract_tiles, reconstruct_from_tiles

__all__ = ['SARCompressor', 'create_cosine_ramp_weights', 'extract_tiles', 'reconstruct_from_tiles']
```

4. Add test function `test_compressor()` in compressor.py:
- Load best checkpoint: notebooks/checkpoints/resnet_lite_v2_c16/best.pth
- Create synthetic 512x512 normalized image (values in [0, 1])
- Compress and decompress
- Verify output shape matches input
- Compute PSNR between input and output (expect > 15 dB for normalized data)
- Print stats

Note: For testing, create normalized test data directly (skip preprocess) to avoid needing raw SAR data.
  </action>
  <verify>
Run: `python -c "from src.inference.compressor import test_compressor; test_compressor()"`
Expected: Test passes, prints compression stats and PSNR
  </verify>
  <done>
- Batched inference works with configurable batch size
- Progress callbacks enable CLI progress bars
- src/inference/__init__.py exports SARCompressor
- test_compressor verifies round-trip on synthetic data
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.inference import SARCompressor"` - imports from package
2. `python -c "from src.inference.compressor import test_compressor; test_compressor()"` - test passes
3. No NotImplementedError in any SARCompressor method
</verification>

<success_criteria>
- SARCompressor loads model from checkpoint with preprocessing params
- Tiled compression/decompression works on 512x512 test image
- Batch processing handles GPU memory efficiently
- Progress callbacks enable integration with CLI
</success_criteria>

<output>
After completion, create `.planning/phases/05-inference/05-03-SUMMARY.md`
</output>
