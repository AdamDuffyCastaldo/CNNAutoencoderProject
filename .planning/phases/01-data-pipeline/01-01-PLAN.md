---
phase: 01-data-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/data/preprocessing.py
  - src/data/dataset.py
  - src/data/datamodule.py
autonomous: true

must_haves:
  truths:
    - "DataLoader delivers batches of shape (N, 1, 256, 256) to GPU"
    - "Patches load from disk without memory overflow"
    - "Augmentation creates variety in training data"
    - "Train/val split is reproducible with seed"
    - "Preprocessing parameters are accessible for inverse transform"
  artifacts:
    - path: "src/data/preprocessing.py"
      provides: "Complete preprocessing utilities"
      contains: "def handle_invalid_values"
    - path: "src/data/dataset.py"
      provides: "SARPatchDataset and LazyPatchDataset classes"
      contains: "class LazyPatchDataset"
    - path: "src/data/datamodule.py"
      provides: "SARDataModule for train/val loading"
      contains: "def train_dataloader"
  key_links:
    - from: "src/data/datamodule.py"
      to: "src/data/dataset.py"
      via: "imports SARPatchDataset, LazyPatchDataset"
      pattern: "from .dataset import"
    - from: "src/data/dataset.py"
      to: "data/patches/*.npy"
      via: "np.load with mmap_mode='r'"
      pattern: "np\\.load.*mmap_mode"
---

<objective>
Complete the PyTorch data loading infrastructure for SAR patch training.

Purpose: Enable efficient training by wrapping the existing 696,277 preprocessed patches (182GB across 44 .npy files) in PyTorch Dataset/DataLoader infrastructure with proper augmentation, train/val splitting, and memory-efficient lazy loading.

Output: Working SARPatchDataset, LazyPatchDataset, and SARDataModule classes that deliver (N, 1, 256, 256) batches to GPU.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-pipeline/01-RESEARCH.md
@src/data/preprocessing.py
@src/data/dataset.py
@src/data/datamodule.py
@src/utils/io.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Complete preprocessing.py stub functions</name>
  <files>src/data/preprocessing.py</files>
  <action>
Implement the three stub functions using the commented reference code already in the file:

1. `handle_invalid_values(image, noise_floor=1e-10)`:
   - Copy input array
   - Replace values <= 0 with noise_floor
   - Replace NaN with noise_floor
   - Replace Inf with noise_floor
   - Return cleaned array

2. `from_db(db)`:
   - Simple conversion: `return 10 ** (db / 10)`

3. `compute_clip_bounds(images, method='percentile', **kwargs)`:
   - Support three methods: 'percentile', 'fixed', 'sigma'
   - For 'percentile': use kwargs low_pct (default 1), high_pct (default 99)
   - For 'fixed': use kwargs vmin (default -25), vmax (default 5)
   - For 'sigma': use kwargs k (default 3) for mean +/- k*std
   - Return tuple (vmin, vmax)

Reference: The logic already exists in `preprocess_sar_complete()` lines 125-149.
  </action>
  <verify>
Run `python src/data/preprocessing.py` - all tests should pass with checkmarks.
  </verify>
  <done>
All three stub functions implemented, test_preprocessing() passes without NotImplementedError.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement SARPatchDataset and LazyPatchDataset</name>
  <files>src/data/dataset.py</files>
  <action>
Implement two Dataset classes:

**SARPatchDataset (in-memory, for small subsets):**
Complete the existing stub using commented reference code:

1. `__init__`: Store patches as float32, validate shape (N,H,W) and range [0,1], store augment flag
2. `__len__`: Return len(self.patches)
3. `__getitem__`:
   - Copy patch (critical for augmentation on mmap arrays)
   - Apply augmentation if enabled
   - Apply optional transform
   - Return torch tensor with channel dim: (1, H, W)
4. `_augment`:
   - 50% horizontal flip (np.fliplr)
   - 50% vertical flip (np.flipud)
   - Random 90-degree rotation (np.rot90 with k in 0-3)
   - Always .copy() after array operations

**LazyPatchDataset (new class, for 182GB multi-file dataset):**
Add new class for memory-efficient loading from multiple .npy files:

```python
class LazyPatchDataset(Dataset):
    """
    Memory-efficient dataset loading patches from multiple .npy files.

    Uses memory mapping to avoid loading all 182GB into RAM.

    Args:
        metadata_path: Path to metadata.npy containing file_index
        augment: Whether to apply augmentation
        shuffle_seed: Seed for deterministic shuffling (None = no shuffle)
    """
    def __init__(self, metadata_path, augment=True, shuffle_seed=42):
        metadata = np.load(metadata_path, allow_pickle=True).item()
        self.file_index = metadata['file_index']  # [(path, count), ...]
        self.augment = augment

        # Build cumulative sum for O(log n) file lookup
        self.cumsum = [0]
        for _, count in self.file_index:
            self.cumsum.append(self.cumsum[-1] + count)
        self.total = self.cumsum[-1]

        # Create shuffle index if seed provided
        if shuffle_seed is not None:
            rng = np.random.default_rng(shuffle_seed)
            self.shuffle_idx = rng.permutation(self.total)
        else:
            self.shuffle_idx = np.arange(self.total)

    def __len__(self):
        return self.total

    def __getitem__(self, idx):
        real_idx = self.shuffle_idx[idx]

        # Binary search for file
        file_idx = np.searchsorted(self.cumsum[1:], real_idx, side='right')
        local_idx = real_idx - self.cumsum[file_idx]

        fpath, _ = self.file_index[file_idx]
        patch = np.load(fpath, mmap_mode='r')[local_idx].copy()  # .copy() critical!

        if self.augment:
            patch = self._augment(patch)

        return torch.from_numpy(patch).unsqueeze(0).float()

    def _augment(self, patch):
        # Same as SARPatchDataset._augment
        ...
```

Also add a helper function to verify .npy file integrity:

```python
def verify_patch_files(metadata_path):
    """Verify all patch files can be loaded. Returns list of (path, error) for failures."""
    metadata = np.load(metadata_path, allow_pickle=True).item()
    errors = []
    for fpath, expected_count in metadata['file_index']:
        try:
            data = np.load(fpath, mmap_mode='r')
            if len(data) != expected_count:
                errors.append((fpath, f"Expected {expected_count} patches, got {len(data)}"))
        except Exception as e:
            errors.append((fpath, str(e)))
    return errors
```
  </action>
  <verify>
Run `python src/data/dataset.py` - test_dataset() should pass. Additionally test:
```python
from src.data.dataset import LazyPatchDataset
ds = LazyPatchDataset('data/patches/metadata.npy', augment=False)
print(f"Total patches: {len(ds)}")  # Should be 696,277
sample = ds[0]
print(f"Shape: {sample.shape}")  # Should be (1, 256, 256)
```
  </verify>
  <done>
SARPatchDataset passes existing tests, LazyPatchDataset loads from multi-file dataset, verify_patch_files detects corrupted files.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement SARDataModule with lazy loading support</name>
  <files>src/data/datamodule.py</files>
  <action>
Extend SARDataModule to support both in-memory and lazy loading:

1. Update `__init__` signature to support both modes:
```python
def __init__(
    self,
    patches_path: str,           # Single .npy file or metadata.npy
    val_fraction: float = 0.1,
    batch_size: int = 8,         # Changed default from 16 to 8 (8GB VRAM constraint)
    num_workers: int = 0,        # Changed default to 0 (Windows compatibility)
    augment_train: bool = True,
    seed: int = 42,
    lazy: bool = True,           # New: use LazyPatchDataset
    max_samples: int = None      # New: optional subset for testing/debugging
):
```

2. Implement data loading logic:
   - If lazy=True and patches_path ends with metadata.npy:
     - Use LazyPatchDataset
     - Split by index ranges (first 90% train, last 10% val)
   - If lazy=False or single .npy file:
     - Load all into memory
     - Use SARPatchDataset
   - If max_samples set, truncate datasets

3. Implement train_dataloader():
```python
def train_dataloader(self) -> DataLoader:
    return DataLoader(
        self.train_dataset,
        batch_size=self.batch_size,
        shuffle=True,
        num_workers=self.num_workers,
        pin_memory=True,
        drop_last=True,              # Stable BatchNorm
        persistent_workers=(self.num_workers > 0)
    )
```

4. Implement val_dataloader() similarly but with shuffle=False.

5. Add preprocessing_params property to expose vmin/vmax:
```python
@property
def preprocessing_params(self) -> dict:
    """Return preprocessing parameters for inverse transform."""
    if hasattr(self, '_metadata'):
        return {'vmin': self._metadata['vmin'], 'vmax': self._metadata['vmax']}
    return None
```

Update imports to include LazyPatchDataset.
  </action>
  <verify>
Run `python src/data/datamodule.py` - test_datamodule() should pass. Additionally test with real data:
```python
from src.data.datamodule import SARDataModule
dm = SARDataModule('data/patches/metadata.npy', batch_size=8, lazy=True)
batch = dm.get_sample_batch('train')
print(f"Batch shape: {batch.shape}")  # Should be (8, 1, 256, 256)
print(f"Train size: {dm.train_size}")
print(f"Val size: {dm.val_size}")
```
  </verify>
  <done>
SARDataModule works with both lazy and in-memory modes, DataLoader delivers (8, 1, 256, 256) batches, preprocessing params accessible.
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify Phase 1 success criteria:

1. **GeoTIFF loading and invalid handling:** Already working (preprocess_sar_complete exists)
   - Verify: `python -c "from src.data.preprocessing import handle_invalid_values; print('OK')"`

2. **Preprocessing produces normalized patches:** Already done (696,277 patches exist)
   - Verify: Load sample patch and confirm values in [0,1]

3. **1000+ quality-filtered patches:** Already done (696,277 patches)
   - Verify: `python -c "from src.data.dataset import LazyPatchDataset; print(len(LazyPatchDataset('data/patches/metadata.npy')))"`

4. **Preprocessing parameters saved:** Already done (global_bounds.npy, metadata.npy)
   - Verify: `python -c "import numpy as np; print(np.load('checkpoints/global_bounds.npy', allow_pickle=True).item())"`

5. **DataLoader delivers (N, 1, 256, 256) batches:**
   - Verify end-to-end test:
```python
import torch
from src.data.datamodule import SARDataModule

dm = SARDataModule('data/patches/metadata.npy', batch_size=8, lazy=True)
batch = next(iter(dm.train_dataloader()))

assert batch.shape == (8, 1, 256, 256), f"Wrong shape: {batch.shape}"
assert batch.dtype == torch.float32, f"Wrong dtype: {batch.dtype}"
assert batch.min() >= 0 and batch.max() <= 1, f"Values out of range"

# Test GPU transfer
if torch.cuda.is_available():
    batch_gpu = batch.cuda()
    print(f"GPU batch shape: {batch_gpu.shape}, device: {batch_gpu.device}")

print("All Phase 1 success criteria verified!")
```
</verification>

<success_criteria>
- [ ] preprocessing.py: All stub functions implemented and tests pass
- [ ] dataset.py: SARPatchDataset and LazyPatchDataset working
- [ ] datamodule.py: SARDataModule creates working DataLoaders
- [ ] End-to-end: Batches of shape (8, 1, 256, 256) delivered to GPU
- [ ] Memory: No OOM errors when loading training batch
</success_criteria>

<risk_mitigation>

**Risk 1: Windows DataLoader worker issues**
- Mitigation: Default num_workers=0, test incrementally if needed
- Detection: Freezing at epoch start, pickle errors
- Fallback: Keep num_workers=0 (slower but reliable)

**Risk 2: Memory mapping issues on Windows**
- Mitigation: Always .copy() before augmentation
- Detection: "read-only" errors during augmentation
- Fallback: Add explicit np.ascontiguousarray() calls

**Risk 3: Corrupted .npy files from interrupted saves**
- Mitigation: verify_patch_files() function checks integrity
- Detection: "could only read 0 elements" errors
- Fallback: Delete and regenerate corrupted files

**Risk 4: OOM with batch_size=8**
- Mitigation: Start with batch_size=8 (8GB VRAM budget)
- Detection: CUDA OOM errors during training
- Fallback: Reduce to batch_size=4, enable gradient checkpointing later

</risk_mitigation>

<output>
After completion, create `.planning/phases/01-data-pipeline/01-01-SUMMARY.md` using the summary template.

Document:
- Which stubs were implemented
- Any deviations from the plan
- Verification results
- Known issues or limitations
</output>
