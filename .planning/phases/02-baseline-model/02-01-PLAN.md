---
phase: 02-baseline-model
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/blocks.py
  - src/losses/ssim.py
  - src/losses/combined.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "ConvBlock halves spatial dimensions (256->128 with stride=2)"
    - "DeconvBlock doubles spatial dimensions (128->256 with stride=2, output_padding=1)"
    - "CombinedLoss returns loss tensor and metrics dict with loss, mse, ssim, psnr"
  artifacts:
    - path: "src/models/blocks.py"
      provides: "ConvBlock, DeconvBlock implementations"
      contains: "class ConvBlock"
    - path: "src/losses/ssim.py"
      provides: "SSIMLoss using pytorch-msssim"
      contains: "from pytorch_msssim import SSIM"
    - path: "src/losses/combined.py"
      provides: "CombinedLoss with configurable weights"
      exports: ["CombinedLoss"]
  key_links:
    - from: "src/losses/combined.py"
      to: "pytorch_msssim"
      via: "SSIMLoss wrapping SSIM module"
      pattern: "from pytorch_msssim import SSIM"
---

<objective>
Implement foundational building blocks (ConvBlock, DeconvBlock) and loss functions (SSIMLoss, CombinedLoss) that are prerequisites for the encoder, decoder, and trainer.

Purpose: These are the atomic components that all other Phase 2 modules depend on. Without these, encoder/decoder cannot be built and training cannot proceed.

Output: Working ConvBlock, DeconvBlock in blocks.py; SSIMLoss using pytorch-msssim; CombinedLoss returning (loss, metrics) with configurable 0.5/0.5 MSE/SSIM weights.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-model/02-CONTEXT.md
@.planning/phases/02-baseline-model/02-RESEARCH.md
@src/models/blocks.py
@src/losses/ssim.py
@src/losses/combined.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ConvBlock and DeconvBlock</name>
  <files>src/models/blocks.py</files>
  <action>
Implement ConvBlock and DeconvBlock classes in blocks.py, replacing the NotImplementedError stubs.

**ConvBlock (lines 43-68):**
- Conv2d with bias=False when use_bn=True (BatchNorm has its own bias)
- BatchNorm2d or nn.Identity() based on use_bn
- LeakyReLU with configurable negative_slope (default 0.2)
- Forward: conv -> bn -> activation

**DeconvBlock (lines 91-114):**
- ConvTranspose2d with output_padding=1 for exact 2x upsampling (kernel=5, stride=2, padding=2)
- bias=False when use_bn=True
- BatchNorm2d or nn.Identity() based on use_bn
- ReLU activation (not LeakyReLU - decoder uses ReLU per requirements)
- Forward: deconv -> bn -> activation

**Critical detail:** output_padding=1 is required for exact 2x upsampling with 5x5 kernels.
Formula: H_out = (H_in - 1) * stride - 2*padding + kernel + output_padding = (H-1)*2 - 4 + 5 + 1 = 2*H

Do NOT implement ResidualBlock, ResidualBlockWithDownsample, ResidualBlockWithUpsample, ChannelAttention, SpatialAttention, or CBAM - those are for Phase 4.
  </action>
  <verify>
Run `python -c "from src.models.blocks import ConvBlock, DeconvBlock; import torch; x = torch.randn(2, 64, 128, 128); c = ConvBlock(64, 128); d = DeconvBlock(128, 64); y = c(x); z = d(y); print(f'ConvBlock: {x.shape} -> {y.shape}'); print(f'DeconvBlock: {y.shape} -> {z.shape}'); assert y.shape == (2, 128, 64, 64); assert z.shape == (2, 64, 128, 128); print('PASS')"`
  </verify>
  <done>ConvBlock halves spatial dims (128->64), DeconvBlock doubles them back (64->128). Both pass shape assertions.</done>
</task>

<task type="auto">
  <name>Task 2: Implement SSIMLoss using pytorch-msssim</name>
  <files>src/losses/ssim.py</files>
  <action>
Implement SSIMLoss class using the pytorch-msssim library (already installed) instead of hand-rolling SSIM computation.

**Replace the entire SSIMLoss class with:**
- Import SSIM from pytorch_msssim
- Store SSIM module with: data_range=1.0, size_average=True, channel=1, win_size=window_size, nonnegative_ssim=True
- Forward method returns 1 - ssim_module(x_hat, x) as the loss

**Why pytorch-msssim:** It's faster (separable kernels), tested, GPU-accelerated, and handles edge cases. The hand-rolled version with Gaussian windows is complex and error-prone.

**Key config from CONTEXT.md:** window_size default is 11 (standard SSIM window).
  </action>
  <verify>
Run `python -c "from src.losses.ssim import SSIMLoss; import torch; x = torch.rand(2, 1, 64, 64); loss_fn = SSIMLoss(); loss_same = loss_fn(x, x); loss_diff = loss_fn(torch.rand(2,1,64,64), x); print(f'Same: {loss_same.item():.6f}, Diff: {loss_diff.item():.4f}'); assert loss_same < 0.01; assert loss_diff > loss_same; print('PASS')"`
  </verify>
  <done>SSIMLoss returns near-zero loss for identical images, higher loss for different images. Uses pytorch_msssim internally.</done>
</task>

<task type="auto">
  <name>Task 3: Implement CombinedLoss with metrics</name>
  <files>src/losses/combined.py</files>
  <action>
Implement CombinedLoss class that combines MSE and SSIM with configurable weights.

**__init__:**
- Store mse_weight (default 0.5 per CONTEXT.md), ssim_weight (default 0.5), window_size (default 11)
- Create nn.MSELoss() for MSE computation
- Create SSIMLoss(window_size=window_size) for SSIM computation

**forward(x_hat, x) -> Tuple[Tensor, Dict]:**
- Compute mse = self.mse_loss(x_hat, x)
- Compute ssim_loss_val = self.ssim_loss(x_hat, x) (this returns 1 - SSIM)
- Compute combined: loss = mse_weight * mse + ssim_weight * ssim_loss_val
- Compute metrics (with torch.no_grad()):
  - psnr = 10 * torch.log10(1.0 / (mse + 1e-10))
  - ssim = 1 - ssim_loss_val (convert back to SSIM value)
- Return (loss, metrics_dict) where metrics_dict = {'loss': loss.item(), 'mse': mse.item(), 'ssim': ssim.item(), 'psnr': psnr.item()}

**Important:** The loss tensor must remain a tensor (not .item()) for backprop. Only the metrics dict uses .item().

Do NOT implement EdgePreservingLoss - that's for future phases.
  </action>
  <verify>
Run `python -c "from src.losses.combined import CombinedLoss; import torch; x = torch.rand(2, 1, 64, 64); x_hat = x + 0.05*torch.randn_like(x); x_hat = x_hat.clamp(0,1); loss_fn = CombinedLoss(mse_weight=0.5, ssim_weight=0.5); loss, m = loss_fn(x_hat, x); print(f'loss={m[\"loss\"]:.4f}, mse={m[\"mse\"]:.4f}, ssim={m[\"ssim\"]:.4f}, psnr={m[\"psnr\"]:.2f}'); assert loss.requires_grad; assert 0 < m['ssim'] < 1; assert m['psnr'] > 20; print('PASS')"`
  </verify>
  <done>CombinedLoss returns differentiable loss tensor and metrics dict. Default weights 0.5/0.5. PSNR computed from MSE.</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Run block tests:
```bash
python -c "from src.models.blocks import ConvBlock, DeconvBlock; import torch; x = torch.randn(2, 1, 256, 256); c1 = ConvBlock(1, 64); c2 = ConvBlock(64, 128); d1 = DeconvBlock(128, 64); d2 = DeconvBlock(64, 1, use_bn=False); y1 = c1(x); y2 = c2(y1); z1 = d1(y2); z2 = d2(z1); print(f'{x.shape} -> {y1.shape} -> {y2.shape} -> {z1.shape} -> {z2.shape}'); assert z2.shape == (2, 1, 256, 256)"
```

2. Run loss tests:
```bash
python -c "from src.losses.combined import CombinedLoss; import torch; x = torch.rand(4, 1, 256, 256); loss_fn = CombinedLoss(); loss, m = loss_fn(x, x); print(f'Identical: loss={m[\"loss\"]:.6f}, psnr={m[\"psnr\"]:.1f}'); assert m['loss'] < 0.001; assert m['psnr'] > 40"
```
</verification>

<success_criteria>
- ConvBlock reduces spatial dims by 2x (256->128, 128->64, etc.)
- DeconvBlock increases spatial dims by 2x (64->128, 128->256, etc.)
- Full encode-decode chain (2 ConvBlocks + 2 DeconvBlocks) preserves input shape
- SSIMLoss uses pytorch_msssim library (not hand-rolled)
- CombinedLoss default weights are 0.5 MSE + 0.5 SSIM
- CombinedLoss.forward() returns (loss_tensor, metrics_dict)
- metrics_dict contains: loss, mse, ssim, psnr
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-model/02-01-SUMMARY.md`
</output>
