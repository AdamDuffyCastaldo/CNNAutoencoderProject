---
phase: 02-baseline-model
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/blocks.py
  - src/losses/ssim.py
  - src/losses/combined.py
autonomous: true
user_setup: []
source: learningnotebooks/phase4_sar_codec/day2_no_references.ipynb

must_haves:
  truths:
    - "ConvBlock halves spatial dimensions (256->128 with stride=2)"
    - "DeconvBlock doubles spatial dimensions (128->256 with stride=2, output_padding=1)"
    - "CombinedLoss returns loss tensor and metrics dict with loss, mse, ssim, psnr"
  artifacts:
    - path: "src/models/blocks.py"
      provides: "ConvBlock, DeconvBlock implementations"
      contains: "class ConvBlock"
    - path: "src/losses/ssim.py"
      provides: "SSIMLoss using pytorch-msssim"
      contains: "from pytorch_msssim import SSIM"
    - path: "src/losses/combined.py"
      provides: "CombinedLoss with configurable weights"
      exports: ["CombinedLoss"]
  key_links:
    - from: "src/losses/combined.py"
      to: "pytorch_msssim"
      via: "SSIMLoss wrapping SSIM module"
      pattern: "from pytorch_msssim import SSIM"
---

<objective>
Migrate foundational building blocks (ConvBlock, DeconvBlock) and loss functions (SSIMLoss, CombinedLoss) from the working notebook implementation to the source files.

Source: learningnotebooks/phase4_sar_codec/day2_no_references.ipynb (Cells 4, 10, 19, 23)

Purpose: These are the atomic components that all other Phase 2 modules depend on. The notebook contains tested, working implementations that need to be moved to the proper source files.

Output: Working ConvBlock, DeconvBlock in blocks.py; SSIMLoss using pytorch-msssim; CombinedLoss returning (loss, metrics) with configurable 0.5/0.5 MSE/SSIM weights.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-model/02-CONTEXT.md
@.planning/phases/02-baseline-model/02-RESEARCH.md
@src/models/blocks.py
@src/losses/ssim.py
@src/losses/combined.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate ConvBlock and DeconvBlock from notebook</name>
  <files>src/models/blocks.py</files>
  <action>
Migrate ConvBlock and DeconvBlock from day2_no_references.ipynb (Cells 4 and 10) to blocks.py, replacing the NotImplementedError stubs.

**ConvBlock (from notebook Cell 4):**
- Conv2d with bias=not use_bn (BatchNorm has its own bias)
- BatchNorm2d or nn.Identity() based on use_bn
- LeakyReLU(negative_slope=0.2)
- Forward: conv -> bn -> activation

**DeconvBlock (from notebook Cell 10):**
- ConvTranspose2d with output_padding=1, bias=not use_bn
- BatchNorm2d or nn.Identity() based on use_bn
- ReLU activation
- Forward: deconv -> bn -> activation

Preserve existing docstrings and type hints from the stub file. The notebook implementation is already tested and working.

Do NOT implement ResidualBlock, ChannelAttention, SpatialAttention, or CBAM - those are for Phase 4.
  </action>
  <verify>
Run `python -c "from src.models.blocks import ConvBlock, DeconvBlock; import torch; x = torch.randn(2, 64, 128, 128); c = ConvBlock(64, 128); d = DeconvBlock(128, 64); y = c(x); z = d(y); print(f'ConvBlock: {x.shape} -> {y.shape}'); print(f'DeconvBlock: {y.shape} -> {z.shape}'); assert y.shape == (2, 128, 64, 64); assert z.shape == (2, 64, 128, 128); print('PASS')"`
  </verify>
  <done>ConvBlock halves spatial dims (128->64), DeconvBlock doubles them back (64->128). Both pass shape assertions.</done>
</task>

<task type="auto">
  <name>Task 2: Migrate SSIMLoss from notebook</name>
  <files>src/losses/ssim.py</files>
  <action>
Migrate SSIMLoss class from day2_no_references.ipynb (Cell 19) to src/losses/ssim.py.

The notebook implementation:
- Uses F.conv2d with Gaussian window registered as buffer
- Computes local means, variances, covariance
- Returns 1 - SSIM as the loss

Alternatively, simplify using pytorch-msssim library (already installed):
- Import SSIM from pytorch_msssim
- Store SSIM module with: data_range=1.0, size_average=True, channel=1, win_size=window_size, nonnegative_ssim=True
- Forward returns 1 - ssim_module(x_hat, x)

**Key config from CONTEXT.md:** window_size default is 11 (standard SSIM window).
  </action>
  <verify>
Run `python -c "from src.losses.ssim import SSIMLoss; import torch; x = torch.rand(2, 1, 64, 64); loss_fn = SSIMLoss(); loss_same = loss_fn(x, x); loss_diff = loss_fn(torch.rand(2,1,64,64), x); print(f'Same: {loss_same.item():.6f}, Diff: {loss_diff.item():.4f}'); assert loss_same < 0.01; assert loss_diff > loss_same; print('PASS')"`
  </verify>
  <done>SSIMLoss returns near-zero loss for identical images, higher loss for different images. Uses pytorch_msssim internally.</done>
</task>

<task type="auto">
  <name>Task 3: Migrate CombinedLoss from notebook</name>
  <files>src/losses/combined.py</files>
  <action>
Migrate CombinedLoss class from day2_no_references.ipynb (Cell 23) to src/losses/combined.py.

From notebook implementation:
- Stores mse_weight, ssim_weight
- Creates MSELoss and SSIMLoss
- Forward returns (loss_tensor, metrics_dict)
- metrics_dict contains: loss, mse, ssim, psnr

**Adjust defaults per CONTEXT.md:**
- mse_weight = 0.5 (not 1.0 as in notebook)
- ssim_weight = 0.5 (not 0.1 as in notebook)
- window_size = 11

**Important:** The loss tensor must remain a tensor for backprop. Only metrics use .item().

Do NOT implement EdgePreservingLoss - that's for future phases.
  </action>
  <verify>
Run `python -c "from src.losses.combined import CombinedLoss; import torch; x = torch.rand(2, 1, 64, 64); x_hat = x + 0.05*torch.randn_like(x); x_hat = x_hat.clamp(0,1); loss_fn = CombinedLoss(mse_weight=0.5, ssim_weight=0.5); loss, m = loss_fn(x_hat, x); print(f'loss={m[\"loss\"]:.4f}, mse={m[\"mse\"]:.4f}, ssim={m[\"ssim\"]:.4f}, psnr={m[\"psnr\"]:.2f}'); assert loss.requires_grad; assert 0 < m['ssim'] < 1; assert m['psnr'] > 20; print('PASS')"`
  </verify>
  <done>CombinedLoss returns differentiable loss tensor and metrics dict. Default weights 0.5/0.5. PSNR computed from MSE.</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Run block tests:
```bash
python -c "from src.models.blocks import ConvBlock, DeconvBlock; import torch; x = torch.randn(2, 1, 256, 256); c1 = ConvBlock(1, 64); c2 = ConvBlock(64, 128); d1 = DeconvBlock(128, 64); d2 = DeconvBlock(64, 1, use_bn=False); y1 = c1(x); y2 = c2(y1); z1 = d1(y2); z2 = d2(z1); print(f'{x.shape} -> {y1.shape} -> {y2.shape} -> {z1.shape} -> {z2.shape}'); assert z2.shape == (2, 1, 256, 256)"
```

2. Run loss tests:
```bash
python -c "from src.losses.combined import CombinedLoss; import torch; x = torch.rand(4, 1, 256, 256); loss_fn = CombinedLoss(); loss, m = loss_fn(x, x); print(f'Identical: loss={m[\"loss\"]:.6f}, psnr={m[\"psnr\"]:.1f}'); assert m['loss'] < 0.001; assert m['psnr'] > 40"
```
</verification>

<success_criteria>
- ConvBlock reduces spatial dims by 2x (256->128, 128->64, etc.)
- DeconvBlock increases spatial dims by 2x (64->128, 128->256, etc.)
- Full encode-decode chain (2 ConvBlocks + 2 DeconvBlocks) preserves input shape
- SSIMLoss uses pytorch_msssim library (not hand-rolled)
- CombinedLoss default weights are 0.5 MSE + 0.5 SSIM
- CombinedLoss.forward() returns (loss_tensor, metrics_dict)
- metrics_dict contains: loss, mse, ssim, psnr
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-model/02-01-SUMMARY.md`
</output>
