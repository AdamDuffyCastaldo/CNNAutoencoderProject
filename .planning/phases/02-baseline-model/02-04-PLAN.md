---
phase: 02-baseline-model
plan: 04
type: execute
wave: 4
depends_on: [02-01, 02-02, 02-03]
files_modified:
  - scripts/train_baseline.py
autonomous: false
user_setup: []

must_haves:
  truths:
    - "Training script runs end-to-end without errors"
    - "Training loss decreases over epochs"
    - "Validation PSNR > 25 dB achieved"
    - "TensorBoard shows convergent loss curves and reconstruction images"
    - "Best model checkpoint saved to checkpoints/ directory"
  artifacts:
    - path: "scripts/train_baseline.py"
      provides: "Baseline training script"
      min_lines: 50
    - path: "checkpoints/*/best.pth"
      provides: "Trained baseline model checkpoint"
  key_links:
    - from: "scripts/train_baseline.py"
      to: "src/data/datamodule.py"
      via: "SARDataModule import"
      pattern: "from src.data.datamodule import SARDataModule"
    - from: "scripts/train_baseline.py"
      to: "src/training/trainer.py"
      via: "Trainer import"
      pattern: "from src.training.trainer import Trainer"
---

<objective>
Create a training script and execute baseline model training to verify the complete pipeline works and achieves the Phase 2 success criterion of PSNR > 25 dB.

Purpose: This is the integration test and actual training run. Without training the model, we cannot verify that all components work together and produce useful results.

Output: Trained baseline model checkpoint, TensorBoard logs showing convergent training, validation PSNR > 25 dB.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-model/02-CONTEXT.md
@.planning/phases/02-baseline-model/02-RESEARCH.md
@.planning/phases/02-baseline-model/02-01-SUMMARY.md
@.planning/phases/02-baseline-model/02-02-SUMMARY.md
@.planning/phases/02-baseline-model/02-03-SUMMARY.md
@src/data/datamodule.py
@src/training/trainer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create baseline training script</name>
  <files>scripts/train_baseline.py</files>
  <action>
Create a training script at scripts/train_baseline.py that trains the baseline autoencoder.

**Script structure:**
```python
#!/usr/bin/env python
"""
Train baseline SAR autoencoder (Phase 2).

Usage:
    python scripts/train_baseline.py
    python scripts/train_baseline.py --epochs 100 --latent_channels 16
    python scripts/train_baseline.py --resume checkpoints/baseline/latest.pth
"""

import argparse
import torch
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data.datamodule import SARDataModule
from src.models.autoencoder import SARAutoencoder
from src.losses.combined import CombinedLoss
from src.training.trainer import Trainer


def parse_args():
    parser = argparse.ArgumentParser(description='Train baseline SAR autoencoder')

    # Data
    parser.add_argument('--data_path', type=str,
                        default='D:/Projects/CNNAutoencoderProject/data/processed/metadata.npy',
                        help='Path to metadata.npy or patches.npy')
    parser.add_argument('--batch_size', type=int, default=8,
                        help='Batch size (default: 8 for 8GB VRAM)')
    parser.add_argument('--num_workers', type=int, default=0,
                        help='DataLoader workers (default: 0 for Windows)')

    # Model
    parser.add_argument('--latent_channels', type=int, default=16,
                        help='Latent channels (16 = 16x compression)')
    parser.add_argument('--base_channels', type=int, default=64,
                        help='Base channel count')

    # Loss
    parser.add_argument('--mse_weight', type=float, default=0.5,
                        help='MSE loss weight')
    parser.add_argument('--ssim_weight', type=float, default=0.5,
                        help='SSIM loss weight')

    # Training
    parser.add_argument('--epochs', type=int, default=50,
                        help='Number of epochs')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                        help='Learning rate')
    parser.add_argument('--early_stopping', type=int, default=20,
                        help='Early stopping patience')

    # Resume
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint to resume from')

    # Output
    parser.add_argument('--run_name', type=str, default=None,
                        help='Run name (default: auto-generated)')

    return parser.parse_args()


def main():
    args = parse_args()

    # Print config
    print("=" * 60)
    print("Baseline SAR Autoencoder Training")
    print("=" * 60)
    print(f"Data: {args.data_path}")
    print(f"Batch size: {args.batch_size}")
    print(f"Latent channels: {args.latent_channels} ({256*256 // (16*16*args.latent_channels):.0f}x compression)")
    print(f"Loss weights: MSE={args.mse_weight}, SSIM={args.ssim_weight}")
    print(f"Learning rate: {args.learning_rate}")
    print(f"Epochs: {args.epochs}")
    print("=" * 60)

    # Data
    print("\nLoading data...")
    dm = SARDataModule(
        patches_path=args.data_path,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        val_fraction=0.1,
    )
    train_loader = dm.train_dataloader()
    val_loader = dm.val_dataloader()

    print(f"Train batches: {len(train_loader)}")
    print(f"Val batches: {len(val_loader)}")

    # Model
    print("\nCreating model...")
    model = SARAutoencoder(
        latent_channels=args.latent_channels,
        base_channels=args.base_channels,
    )
    params = model.count_parameters()
    print(f"Model parameters: {params['total']:,}")
    print(f"  Encoder: {params['encoder']:,}")
    print(f"  Decoder: {params['decoder']:,}")
    print(f"Compression ratio: {model.get_compression_ratio():.1f}x")

    # Loss
    loss_fn = CombinedLoss(
        mse_weight=args.mse_weight,
        ssim_weight=args.ssim_weight,
    )

    # Config for trainer
    config = {
        'learning_rate': args.learning_rate,
        'lr_patience': 10,
        'lr_factor': 0.5,
        'max_grad_norm': 1.0,
        'run_name': args.run_name or f'baseline_c{args.latent_channels}',
        'preprocessing_params': dm.preprocessing_params,
        # Store hyperparams for reproducibility
        'latent_channels': args.latent_channels,
        'base_channels': args.base_channels,
        'mse_weight': args.mse_weight,
        'ssim_weight': args.ssim_weight,
        'batch_size': args.batch_size,
    }

    # Trainer
    print("\nInitializing trainer...")
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        loss_fn=loss_fn,
        config=config,
    )

    # Resume if specified
    if args.resume:
        print(f"\nResuming from: {args.resume}")
        trainer.load_checkpoint(args.resume)

    # Train
    print("\nStarting training...")
    print(f"TensorBoard: tensorboard --logdir={trainer.log_dir.parent}")
    print(f"Checkpoints: {trainer.checkpoint_dir}")
    print()

    history = trainer.train(
        epochs=args.epochs,
        early_stopping_patience=args.early_stopping,
    )

    # Summary
    print("\n" + "=" * 60)
    print("Training Complete")
    print("=" * 60)
    if history:
        final = history[-1]
        print(f"Final epoch: {final['epoch'] + 1}")
        print(f"Best val loss: {trainer.best_val_loss:.4f}")
        print(f"Final val PSNR: {final['val_psnr']:.2f} dB")
        print(f"Final val SSIM: {final['val_ssim']:.4f}")

        # Check success criterion
        if final['val_psnr'] >= 25:
            print("\n[SUCCESS] PSNR > 25 dB achieved!")
        else:
            print(f"\n[WARNING] PSNR {final['val_psnr']:.2f} < 25 dB target")

    print(f"\nBest checkpoint: {trainer.checkpoint_dir / 'best.pth'}")


if __name__ == '__main__':
    main()
```

Create the scripts/ directory if it doesn't exist.
  </action>
  <verify>
Run `python scripts/train_baseline.py --help` to verify script loads without errors.
  </verify>
  <done>Training script created with CLI arguments for all hyperparameters, resume support, and clear output.</done>
</task>

<task type="auto">
  <name>Task 2: Execute baseline training</name>
  <files>None (training run)</files>
  <action>
Run the training script to train the baseline model.

**Training command:**
```bash
python scripts/train_baseline.py --epochs 50 --latent_channels 16
```

**Expected behavior:**
1. Data loading completes (696k patches, ~626k train, ~70k val)
2. Model created (~1.5M parameters)
3. Training starts with tqdm progress bars
4. Each epoch: ~78k batches train, ~8.7k batches val (at batch_size=8)
5. Loss decreases over epochs
6. PSNR should reach >25 dB within 50 epochs

**Monitoring:**
- Watch for NaN losses (would indicate gradient explosion)
- Watch for loss plateau (may need LR adjustment)
- GPU memory should stay under 8GB

**If training fails:**
- If OOM: reduce batch_size to 4
- If NaN: check gradient clipping is working, reduce learning rate
- If loss doesn't decrease: check loss function returns correct values

**Training duration:** ~2-4 hours for 50 epochs on RTX 3070 (estimate based on dataset size)

Note: This is a long-running task. Start training and monitor first few epochs to ensure it's working correctly, then let it run.
  </action>
  <verify>
After training completes (or first 5 epochs):
1. Check TensorBoard: `tensorboard --logdir=runs`
2. Verify loss curve is decreasing
3. Verify reconstruction images show recognizable SAR patterns
4. Check final PSNR > 25 dB
  </verify>
  <done>Training completes without errors. Val PSNR > 25 dB achieved. Best checkpoint saved.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Phase 2 baseline model training pipeline:
1. ConvBlock/DeconvBlock building blocks
2. SAREncoder/SARDecoder/SARAutoencoder model
3. CombinedLoss (0.5 MSE + 0.5 SSIM)
4. Trainer with TensorBoard, checkpointing, early stopping
5. Trained baseline model at 16x compression
  </what-built>
  <how-to-verify>
1. **Launch TensorBoard:**
   ```bash
   tensorboard --logdir=runs
   ```
   Open http://localhost:6006 in browser.

2. **Check loss curves (SCALARS tab):**
   - train/loss and val/loss should decrease over epochs
   - val/loss should follow train/loss (no severe overfitting)
   - train/psnr and val/psnr should increase

3. **Check reconstruction images (IMAGES tab):**
   - Look at 'Reconstructions/triple_view'
   - Top row: original SAR patches (should show speckle texture)
   - Middle row: reconstructions (should be recognizable)
   - Bottom row: difference (should be low-intensity, mostly gray)

4. **Verify metrics meet criteria:**
   - Final val PSNR should be > 25 dB
   - Final val SSIM should be > 0.7 (typical for this compression)

5. **Check checkpoint exists:**
   ```bash
   dir checkpoints\baseline_c16\best.pth
   ```
  </how-to-verify>
  <resume-signal>
Type "approved" if:
- Loss curves show convergence (decreasing trend)
- Reconstructions are recognizable SAR images
- PSNR > 25 dB achieved

Or describe issues if training did not converge or metrics are poor.
  </resume-signal>
</task>

</tasks>

<verification>
After all tasks complete and checkpoint approved:

1. Verify checkpoint contents:
```bash
python -c "
import torch
ckpt = torch.load('checkpoints/baseline_c16/best.pth', map_location='cpu')
print('Checkpoint keys:', list(ckpt.keys()))
print('Epoch:', ckpt['epoch'])
print('Best val loss:', ckpt['best_val_loss'])
print('Preprocessing params:', ckpt.get('preprocessing_params', 'MISSING'))
assert 'preprocessing_params' in ckpt, 'Missing preprocessing params!'
print('Checkpoint valid: PASS')
"
```

2. Test inference with trained model:
```bash
python -c "
import torch
from src.models.autoencoder import SARAutoencoder

# Load checkpoint
ckpt = torch.load('checkpoints/baseline_c16/best.pth', map_location='cpu')

# Create model and load weights
model = SARAutoencoder(latent_channels=16)
model.load_state_dict(ckpt['model_state_dict'])
model.eval()

# Test inference
x = torch.rand(1, 1, 256, 256)
with torch.no_grad():
    x_hat, z = model(x)

print(f'Input: {x.shape}')
print(f'Latent: {z.shape}')
print(f'Output: {x_hat.shape}')
print(f'Output range: [{x_hat.min():.3f}, {x_hat.max():.3f}]')
print('Inference test: PASS')
"
```
</verification>

<success_criteria>
- Training script runs without errors
- Training loss decreases over epochs (no NaN, no explosion)
- Validation loss follows training loss (no severe overfitting)
- TensorBoard shows logged scalars and reconstruction images
- Reconstruction images show recognizable SAR patterns
- Final validation PSNR > 25 dB
- Best model checkpoint saved with preprocessing_params
- Checkpoint can be loaded and used for inference
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-model/02-04-SUMMARY.md`
</output>
