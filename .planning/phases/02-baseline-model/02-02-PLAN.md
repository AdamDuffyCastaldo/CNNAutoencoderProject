---
phase: 02-baseline-model
plan: 02
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - src/models/encoder.py
  - src/models/decoder.py
  - src/models/autoencoder.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Encoder compresses (N,1,256,256) to (N,C,16,16) where C=latent_channels"
    - "Decoder reconstructs (N,C,16,16) to (N,1,256,256) with sigmoid output in [0,1]"
    - "Autoencoder.forward() returns (x_hat, z) tuple"
    - "Compression ratio 16x achieved with latent_channels=16"
  artifacts:
    - path: "src/models/encoder.py"
      provides: "SAREncoder with 4 ConvBlock layers"
      exports: ["SAREncoder"]
    - path: "src/models/decoder.py"
      provides: "SARDecoder with 4 DeconvBlock layers"
      exports: ["SARDecoder"]
    - path: "src/models/autoencoder.py"
      provides: "SARAutoencoder combining encoder+decoder"
      exports: ["SARAutoencoder"]
  key_links:
    - from: "src/models/encoder.py"
      to: "src/models/blocks.py"
      via: "ConvBlock import"
      pattern: "from .blocks import ConvBlock"
    - from: "src/models/decoder.py"
      to: "src/models/blocks.py"
      via: "DeconvBlock import"
      pattern: "from .blocks import DeconvBlock"
---

<objective>
Implement the complete autoencoder architecture (Encoder, Decoder, Autoencoder wrapper) using the ConvBlock and DeconvBlock from Plan 01.

Purpose: Create the neural network that compresses 256x256 SAR patches to a 16x16xC latent space and reconstructs them. This is the core model for Phase 2.

Output: Working SAREncoder (256->16 spatial), SARDecoder (16->256 spatial), SARAutoencoder with encode/decode/forward methods and compression ratio calculation.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-model/02-CONTEXT.md
@.planning/phases/02-baseline-model/02-RESEARCH.md
@.planning/phases/02-baseline-model/02-01-SUMMARY.md
@src/models/encoder.py
@src/models/decoder.py
@src/models/autoencoder.py
@src/models/blocks.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement SAREncoder</name>
  <files>src/models/encoder.py</files>
  <action>
Implement SAREncoder class in encoder.py, replacing NotImplementedError stubs.

**Architecture (4 strided convolutions, 16x spatial reduction):**
- Layer 1: in_channels (1) -> base_channels (64), 256->128
- Layer 2: base_channels (64) -> base_channels*2 (128), 128->64
- Layer 3: base_channels*2 (128) -> base_channels*4 (256), 64->32
- Layer 4: base_channels*4 (256) -> latent_channels, 32->16 (NO activation, NO batchnorm)

**__init__ implementation:**
```python
self.layer1 = ConvBlock(in_channels, base_channels, kernel_size=5, stride=2, padding=2, use_bn=use_bn)
self.layer2 = ConvBlock(base_channels, base_channels * 2, kernel_size=5, stride=2, padding=2, use_bn=use_bn)
self.layer3 = ConvBlock(base_channels * 2, base_channels * 4, kernel_size=5, stride=2, padding=2, use_bn=use_bn)
# Final layer: plain Conv2d, no activation (FR2.9)
self.layer4 = nn.Conv2d(base_channels * 4, latent_channels, kernel_size=5, stride=2, padding=2)
```

**forward(x):** Sequential application of all 4 layers.

**_initialize_weights():** Kaiming initialization for all Conv2d layers:
```python
for m in self.modules():
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, a=0.2, mode='fan_out', nonlinearity='leaky_relu')
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.ones_(m.weight)
        nn.init.zeros_(m.bias)
```

**get_receptive_field():** Return 61 (calculated: 5 + 4*2 + 4*4 + 4*8 = 61)

Move the `self._initialize_weights()` call AFTER all layers are defined (it's currently placed where it would execute before layers exist).
  </action>
  <verify>
Run `python -c "from src.models.encoder import SAREncoder; import torch; enc = SAREncoder(latent_channels=16); x = torch.randn(2, 1, 256, 256); z = enc(x); print(f'Input: {x.shape}, Latent: {z.shape}'); assert z.shape == (2, 16, 16, 16); print(f'RF: {enc.get_receptive_field()}'); print('PASS')"`
  </verify>
  <done>SAREncoder compresses (2,1,256,256) to (2,16,16,16). Receptive field is 61 pixels.</done>
</task>

<task type="auto">
  <name>Task 2: Implement SARDecoder</name>
  <files>src/models/decoder.py</files>
  <action>
Implement SARDecoder class in decoder.py, replacing NotImplementedError stubs.

**Architecture (mirrors encoder, 16x spatial expansion):**
- Layer 1: latent_channels -> base_channels*4 (256), 16->32
- Layer 2: base_channels*4 (256) -> base_channels*2 (128), 32->64
- Layer 3: base_channels*2 (128) -> base_channels (64), 64->128
- Layer 4: base_channels (64) -> out_channels (1), 128->256 (NO batchnorm, sigmoid applied separately)

**__init__ implementation:**
```python
self.layer1 = DeconvBlock(latent_channels, base_channels * 4, kernel_size=5, stride=2, padding=2, output_padding=1, use_bn=use_bn)
self.layer2 = DeconvBlock(base_channels * 4, base_channels * 2, kernel_size=5, stride=2, padding=2, output_padding=1, use_bn=use_bn)
self.layer3 = DeconvBlock(base_channels * 2, base_channels, kernel_size=5, stride=2, padding=2, output_padding=1, use_bn=use_bn)
# Final layer: plain ConvTranspose2d, no BN (sigmoid applied in forward)
self.layer4 = nn.ConvTranspose2d(base_channels, out_channels, kernel_size=5, stride=2, padding=2, output_padding=1)
```

**forward(z):**
```python
x = self.layer1(z)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
x = torch.sigmoid(x)  # FR2.8: Sigmoid output for [0,1] bounded output
return x
```

**_initialize_weights():** Same Kaiming initialization pattern as encoder, but use nonlinearity='relu' for ConvTranspose2d (decoder uses ReLU in DeconvBlock):
```python
for m in self.modules():
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.BatchNorm2d):
        nn.init.ones_(m.weight)
        nn.init.zeros_(m.bias)
```

Move the `self._initialize_weights()` call AFTER all layers are defined.
  </action>
  <verify>
Run `python -c "from src.models.decoder import SARDecoder; import torch; dec = SARDecoder(latent_channels=16); z = torch.randn(2, 16, 16, 16); x = dec(z); print(f'Latent: {z.shape}, Output: {x.shape}'); assert x.shape == (2, 1, 256, 256); assert x.min() >= 0 and x.max() <= 1; print('Output in [0,1]: PASS')"`
  </verify>
  <done>SARDecoder expands (2,16,16,16) to (2,1,256,256). Output bounded in [0,1] via sigmoid.</done>
</task>

<task type="auto">
  <name>Task 3: Implement SARAutoencoder wrapper</name>
  <files>src/models/autoencoder.py</files>
  <action>
Implement SARAutoencoder class in autoencoder.py, replacing NotImplementedError stubs.

**__init__:** Create encoder and decoder:
```python
self.encoder = SAREncoder(
    in_channels=1,
    latent_channels=latent_channels,
    base_channels=base_channels,
    use_bn=use_bn
)
self.decoder = SARDecoder(
    out_channels=1,
    latent_channels=latent_channels,
    base_channels=base_channels,
    use_bn=use_bn
)
```

**forward(x) -> Tuple[Tensor, Tensor]:**
```python
z = self.encoder(x)
x_hat = self.decoder(z)
return x_hat, z
```

**encode(x):** Return self.encoder(x)

**decode(z):** Return self.decoder(z)

**get_compression_ratio(input_size=256):**
```python
input_elements = input_size * input_size * 1  # 65536 for 256x256x1
latent_size = input_size // 16  # 16 for 256 input
latent_elements = latent_size * latent_size * self.latent_channels
return input_elements / latent_elements
```
For latent_channels=16: 65536 / (16*16*16) = 65536 / 4096 = 16x compression

**get_latent_size(input_size=256):**
```python
latent_size = input_size // 16
return (self.latent_channels, latent_size, latent_size)
```

**count_parameters():**
```python
encoder_params = sum(p.numel() for p in self.encoder.parameters())
decoder_params = sum(p.numel() for p in self.decoder.parameters())
return {'encoder': encoder_params, 'decoder': decoder_params, 'total': encoder_params + decoder_params}
```

**analyze_latent(z):**
```python
return {
    'mean': z.mean().item(),
    'std': z.std().item(),
    'min': z.min().item(),
    'max': z.max().item(),
    'sparsity': (z.abs() < 0.1).float().mean().item()
}
```
  </action>
  <verify>
Run `python -c "from src.models.autoencoder import SARAutoencoder; import torch; model = SARAutoencoder(latent_channels=16); x = torch.rand(2, 1, 256, 256); x_hat, z = model(x); print(f'Input: {x.shape}, Latent: {z.shape}, Output: {x_hat.shape}'); print(f'Compression: {model.get_compression_ratio()}x'); print(f'Params: {model.count_parameters()}'); assert x_hat.shape == x.shape; assert z.shape == (2, 16, 16, 16); assert model.get_compression_ratio() == 16.0; print('PASS')"`
  </verify>
  <done>SARAutoencoder.forward() returns (x_hat, z). Compression ratio is 16x with latent_channels=16. All utility methods work.</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Full forward pass test:
```bash
python -c "from src.models.autoencoder import SARAutoencoder; import torch; model = SARAutoencoder(latent_channels=16); x = torch.rand(4, 1, 256, 256); x_hat, z = model(x); loss = ((x - x_hat)**2).mean(); loss.backward(); print(f'Shapes OK: {x.shape} -> {z.shape} -> {x_hat.shape}'); print(f'Gradients flow: {model.encoder.layer1.conv.weight.grad is not None}'); print('PASS')"
```

2. Different compression ratios:
```bash
python -c "from src.models.autoencoder import SARAutoencoder; ratios = [(8, 32), (16, 16), (32, 8), (64, 4)]; [print(f'C={c}: {SARAutoencoder(latent_channels=c).get_compression_ratio():.1f}x') for c, expected in ratios]"
```
Expected: 8x, 16x, 32x, 64x
</verification>

<success_criteria>
- SAREncoder: (N,1,256,256) -> (N,C,16,16) with 4 ConvBlock layers
- SARDecoder: (N,C,16,16) -> (N,1,256,256) with 4 DeconvBlock layers + sigmoid
- Decoder output bounded in [0,1]
- No activation on encoder's final layer
- SARAutoencoder.forward() returns (x_hat, z) tuple
- get_compression_ratio() returns correct value (16x for 16 latent channels)
- Gradients flow through entire model (backward pass works)
- Kaiming initialization applied to all conv layers
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-model/02-02-SUMMARY.md`
</output>
