---
phase: 02-baseline-model
plan: 03
type: execute
wave: 3
depends_on: [02-01, 02-02]
files_modified:
  - src/training/trainer.py
autonomous: true
user_setup: []
source: learningnotebooks/phase4_sar_codec/day2_no_references.ipynb

must_haves:
  truths:
    - "Trainer.train() runs epochs with train_epoch() and validate() loops"
    - "TensorBoard logs scalars (loss, PSNR, SSIM) and images (reconstructions) every epoch"
    - "Checkpoints save model + optimizer + scheduler + config + preprocessing_params"
    - "Resume from checkpoint restores exact training state"
    - "Progress bar shows loss, PSNR, SSIM during batch processing"
  artifacts:
    - path: "src/training/trainer.py"
      provides: "Complete Trainer class with all training features"
      exports: ["Trainer"]
  key_links:
    - from: "src/training/trainer.py"
      to: "torch.utils.tensorboard"
      via: "SummaryWriter for logging"
      pattern: "SummaryWriter"
    - from: "src/training/trainer.py"
      to: "torch.optim.lr_scheduler"
      via: "ReduceLROnPlateau scheduler"
      pattern: "ReduceLROnPlateau"
---

<objective>
Migrate the Trainer class from the notebook implementation, extending it with additional features per CONTEXT.md decisions.

Source: learningnotebooks/phase4_sar_codec/day2_no_references.ipynb (Cell 34 and related)

Purpose: The notebook has a working Trainer class. Migrate it to src/training/trainer.py while adding features from CONTEXT.md: triple view visualization, weight histograms every 10 epochs, GPU memory logging, training log files.

Output: Working Trainer class that can train the autoencoder, log to TensorBoard, save/load checkpoints, and support training resumption.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-baseline-model/02-CONTEXT.md
@.planning/phases/02-baseline-model/02-RESEARCH.md
@.planning/phases/02-baseline-model/02-01-SUMMARY.md
@.planning/phases/02-baseline-model/02-02-SUMMARY.md
@src/training/trainer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate Trainer initialization and core loops from notebook</name>
  <files>src/training/trainer.py</files>
  <action>
Migrate the Trainer class __init__, train_epoch(), and validate() methods from day2_no_references.ipynb (Cell 34) to trainer.py.

**Add imports at top of file:**
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import torchvision.utils as vutils
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, List, Tuple
from collections import defaultdict
import json
from tqdm import tqdm
import logging
```

**__init__ method (replace NotImplementedError):**

```python
# Device setup (existing code is fine)

# Move model to device
self.model = model.to(self.device)
self.train_loader = train_loader
self.val_loader = val_loader
self.loss_fn = loss_fn.to(self.device)
self.config = config

# Store preprocessing params (critical for SAR - from RESEARCH.md pitfall #5)
self.preprocessing_params = config.get('preprocessing_params', None)

# Optimizer (FR3.3: Adam with configurable lr, default 1e-4)
self.optimizer = optim.Adam(
    model.parameters(),
    lr=config.get('learning_rate', 1e-4),
    betas=config.get('betas', (0.9, 0.999)),
    weight_decay=config.get('weight_decay', 0),
)

# Scheduler (FR3.4: ReduceLROnPlateau)
self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    self.optimizer,
    mode='min',
    factor=config.get('lr_factor', 0.5),
    patience=config.get('lr_patience', 10),
    verbose=False,  # Handle logging manually via TensorBoard
)

# Logging setup
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
self.run_name = config.get('run_name', f'baseline_{timestamp}')
self.log_dir = Path(config.get('log_dir', 'runs')) / self.run_name
self.log_dir.mkdir(parents=True, exist_ok=True)
self.writer = SummaryWriter(self.log_dir)

# File logging (per CONTEXT.md: save training logs to file)
self.log_file = self.log_dir / 'training.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    handlers=[
        logging.FileHandler(self.log_file),
        logging.StreamHandler()
    ]
)
self.logger = logging.getLogger(__name__)

# Checkpointing
self.checkpoint_dir = Path(config.get('checkpoint_dir', 'checkpoints')) / self.run_name
self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

# Training state (keep existing initialization)
self.epoch = 0
self.global_step = 0
self.best_val_loss = float('inf')
self.epochs_without_improvement = 0
self.history = []

# Store fixed sample batch for consistent visualization
self._sample_batch = None
```

**train_epoch() method:**
```python
self.model.train()
epoch_metrics = defaultdict(float)
num_batches = 0

pbar = tqdm(self.train_loader, desc=f"Epoch {self.epoch+1} [Train]",
            leave=True, dynamic_ncols=True)

for batch in pbar:
    x = batch.to(self.device)

    # Forward
    self.optimizer.zero_grad()
    x_hat, z = self.model(x)
    loss, metrics = self.loss_fn(x_hat, x)

    # Backward
    loss.backward()

    # Gradient clipping (FR3.5: max norm 1.0)
    torch.nn.utils.clip_grad_norm_(
        self.model.parameters(),
        self.config.get('max_grad_norm', 1.0)
    )

    # Update
    self.optimizer.step()

    # Accumulate metrics
    for key, value in metrics.items():
        epoch_metrics[key] += value
    num_batches += 1
    self.global_step += 1

    # Update progress bar (per CONTEXT.md: show loss + PSNR + SSIM)
    pbar.set_postfix({
        'loss': f"{metrics['loss']:.4f}",
        'psnr': f"{metrics['psnr']:.2f}",
        'ssim': f"{metrics['ssim']:.4f}"
    })

return {k: v / num_batches for k, v in epoch_metrics.items()}
```

**validate() method:**
```python
self.model.eval()
epoch_metrics = defaultdict(float)
num_batches = 0

pbar = tqdm(self.val_loader, desc=f"Epoch {self.epoch+1} [Val]",
            leave=True, dynamic_ncols=True)

for batch in pbar:
    x = batch.to(self.device)
    x_hat, z = self.model(x)
    loss, metrics = self.loss_fn(x_hat, x)

    for key, value in metrics.items():
        epoch_metrics[key] += value
    num_batches += 1

    pbar.set_postfix({
        'loss': f"{metrics['loss']:.4f}",
        'psnr': f"{metrics['psnr']:.2f}",
        'ssim': f"{metrics['ssim']:.4f}"
    })

return {k: v / num_batches for k, v in epoch_metrics.items()}
```
  </action>
  <verify>
Run a syntax check: `python -c "from src.training.trainer import Trainer; print('Import OK')"`
Note: Full functionality test requires model and data, tested in Task 3.
  </verify>
  <done>Trainer __init__, train_epoch(), validate() implemented with progress bars showing loss/PSNR/SSIM.</done>
</task>

<task type="auto">
  <name>Task 2: Implement checkpointing, logging, and image visualization</name>
  <files>src/training/trainer.py</files>
  <action>
Implement save_checkpoint(), load_checkpoint(), log_images(), and helper methods.

**save_checkpoint(is_best=False):**
```python
checkpoint = {
    'epoch': self.epoch,
    'global_step': self.global_step,
    'model_state_dict': self.model.state_dict(),
    'optimizer_state_dict': self.optimizer.state_dict(),
    'scheduler_state_dict': self.scheduler.state_dict(),
    'best_val_loss': self.best_val_loss,
    'epochs_without_improvement': self.epochs_without_improvement,
    'config': self.config,
    'preprocessing_params': self.preprocessing_params,  # Critical for SAR!
    'history': self.history,
}

# Save latest
latest_path = self.checkpoint_dir / 'latest.pth'
torch.save(checkpoint, latest_path)
self.logger.info(f"Saved checkpoint: {latest_path}")

# Save best
if is_best:
    best_path = self.checkpoint_dir / 'best.pth'
    torch.save(checkpoint, best_path)
    self.logger.info(f"Saved best checkpoint: {best_path}")
```

**load_checkpoint(path: str):**
```python
checkpoint = torch.load(path, map_location=self.device)

self.model.load_state_dict(checkpoint['model_state_dict'])
self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

self.epoch = checkpoint['epoch'] + 1  # Resume from next epoch
self.global_step = checkpoint['global_step']
self.best_val_loss = checkpoint['best_val_loss']
self.epochs_without_improvement = checkpoint.get('epochs_without_improvement', 0)
self.history = checkpoint.get('history', [])

self.logger.info(f"Resumed from epoch {checkpoint['epoch']} (best_val_loss={self.best_val_loss:.4f})")
```

**log_images(num_images: int = 4):**
```python
self.model.eval()

# Use fixed sample batch for consistent visualization
if self._sample_batch is None:
    self._sample_batch = next(iter(self.val_loader))[:num_images]

x = self._sample_batch.to(self.device)

with torch.no_grad():
    x_hat, z = self.model(x)

# Create triple view: original | reconstructed | difference (per CONTEXT.md)
diff = torch.abs(x - x_hat)

# Stack vertically: [originals, reconstructions, differences]
# Each row is num_images wide
combined = torch.cat([x, x_hat, diff], dim=0)

# Make grid: nrow=num_images means each row has num_images samples
grid = vutils.make_grid(combined, nrow=num_images, normalize=True, padding=2)
self.writer.add_image('Reconstructions/triple_view', grid, self.epoch)
```

**_log_weight_histograms():** (per CONTEXT.md: every 10 epochs)
```python
for name, param in self.model.named_parameters():
    self.writer.add_histogram(f'weights/{name}', param, self.epoch)
    if param.grad is not None:
        self.writer.add_histogram(f'gradients/{name}', param.grad, self.epoch)
```

**_log_gpu_memory():** (per CONTEXT.md: log GPU memory in epoch summary)
```python
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(self.device) / 1024**3
    reserved = torch.cuda.memory_reserved(self.device) / 1024**3
    return {'gpu_allocated_gb': allocated, 'gpu_reserved_gb': reserved}
return {}
```

**_get_lr():**
```python
return self.optimizer.param_groups[0]['lr']
```
  </action>
  <verify>
Run `python -c "from src.training.trainer import Trainer; import inspect; methods = ['save_checkpoint', 'load_checkpoint', 'log_images']; missing = [m for m in methods if not hasattr(Trainer, m)]; print(f'Methods present: {len(methods) - len(missing)}/{len(methods)}'); assert len(missing) == 0, f'Missing: {missing}'"`
  </verify>
  <done>Checkpointing saves/loads full training state including preprocessing_params. Image logging creates triple view.</done>
</task>

<task type="auto">
  <name>Task 3: Implement main training loop with early stopping</name>
  <files>src/training/trainer.py</files>
  <action>
Implement the main train() method that orchestrates everything.

**train(epochs: int, early_stopping_patience: int = 20):**
```python
self.logger.info(f"Starting training for {epochs} epochs")
self.logger.info(f"Model: {self.model.__class__.__name__}")
self.logger.info(f"Config: {self.config}")

start_epoch = self.epoch

for epoch in range(start_epoch, epochs):
    self.epoch = epoch

    # Train
    train_metrics = self.train_epoch()

    # Validate
    val_metrics = self.validate()

    # Log scalars to TensorBoard (per CONTEXT.md: every epoch)
    for key, value in train_metrics.items():
        self.writer.add_scalar(f'train/{key}', value, epoch)
    for key, value in val_metrics.items():
        self.writer.add_scalar(f'val/{key}', value, epoch)

    # Log learning rate
    current_lr = self._get_lr()
    self.writer.add_scalar('train/learning_rate', current_lr, epoch)

    # Log GPU memory (per CONTEXT.md)
    gpu_mem = self._log_gpu_memory()
    for key, value in gpu_mem.items():
        self.writer.add_scalar(f'system/{key}', value, epoch)

    # Log reconstruction images (per CONTEXT.md: every epoch)
    self.log_images(num_images=4)

    # Log weight histograms (per CONTEXT.md: every 10 epochs)
    if epoch % 10 == 0:
        self._log_weight_histograms()

    # Update scheduler (FR3.4)
    self.scheduler.step(val_metrics['loss'])

    # Check for improvement (FR3.8: best model tracking)
    is_best = val_metrics['loss'] < self.best_val_loss
    if is_best:
        self.best_val_loss = val_metrics['loss']
        self.epochs_without_improvement = 0
    else:
        self.epochs_without_improvement += 1

    # Save checkpoint (FR3.7)
    self.save_checkpoint(is_best=is_best)

    # Store history
    self.history.append({
        'epoch': epoch,
        'train_loss': train_metrics['loss'],
        'val_loss': val_metrics['loss'],
        'train_psnr': train_metrics.get('psnr', 0),
        'val_psnr': val_metrics.get('psnr', 0),
        'train_ssim': train_metrics.get('ssim', 0),
        'val_ssim': val_metrics.get('ssim', 0),
        'learning_rate': current_lr,
    })

    # Print epoch summary (per CONTEXT.md: with GPU memory)
    self.logger.info(
        f"Epoch {epoch+1}/{epochs} | "
        f"Train: loss={train_metrics['loss']:.4f}, psnr={train_metrics.get('psnr', 0):.2f}, ssim={train_metrics.get('ssim', 0):.4f} | "
        f"Val: loss={val_metrics['loss']:.4f}, psnr={val_metrics.get('psnr', 0):.2f}, ssim={val_metrics.get('ssim', 0):.4f} | "
        f"LR: {current_lr:.2e}" +
        (f" | GPU: {gpu_mem.get('gpu_allocated_gb', 0):.2f}GB" if gpu_mem else "")
    )

    # Early stopping (FR3.6)
    if self.epochs_without_improvement >= early_stopping_patience:
        self.logger.info(f"Early stopping triggered after {epoch+1} epochs (no improvement for {early_stopping_patience} epochs)")
        break

# Cleanup
self.writer.close()
self.logger.info(f"Training complete. Best val loss: {self.best_val_loss:.4f}")

return self.history
```

Also add the return type hint to the train() method signature:
```python
def train(self, epochs: int, early_stopping_patience: int = 20) -> List[Dict]:
```
  </action>
  <verify>
Create a minimal integration test:
```bash
python -c "
from src.models.autoencoder import SARAutoencoder
from src.losses.combined import CombinedLoss
from src.training.trainer import Trainer
import torch
from torch.utils.data import DataLoader, TensorDataset

# Create tiny test data
x = torch.rand(16, 1, 256, 256)
train_ds = TensorDataset(x[:12])
val_ds = TensorDataset(x[12:])
train_loader = DataLoader(train_ds, batch_size=4)
val_loader = DataLoader(val_ds, batch_size=4)

# Monkey-patch to return tensor directly
class SimpleLoader:
    def __init__(self, ds, bs):
        self.ds = ds
        self.bs = bs
    def __iter__(self):
        for i in range(0, len(self.ds), self.bs):
            yield torch.stack([self.ds[j][0] for j in range(i, min(i+self.bs, len(self.ds)))])
    def __len__(self):
        return (len(self.ds) + self.bs - 1) // self.bs

train_loader = SimpleLoader(train_ds, 4)
val_loader = SimpleLoader(val_ds, 4)

model = SARAutoencoder(latent_channels=16)
loss_fn = CombinedLoss()
config = {'learning_rate': 1e-3, 'log_dir': 'test_runs', 'checkpoint_dir': 'test_checkpoints'}

trainer = Trainer(model, train_loader, val_loader, loss_fn, config)
history = trainer.train(epochs=2, early_stopping_patience=100)

print(f'Epochs trained: {len(history)}')
print(f'Final val loss: {history[-1][\"val_loss\"]:.4f}')
print(f'Final val PSNR: {history[-1][\"val_psnr\"]:.2f}')
print('PASS')
"
```
  </verify>
  <done>Trainer.train() runs complete training loop with TensorBoard logging, checkpointing, early stopping, and progress bars.</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Verify checkpoint save/load:
```bash
python -c "
from src.models.autoencoder import SARAutoencoder
from src.losses.combined import CombinedLoss
from src.training.trainer import Trainer
import torch
import os
import shutil

# Setup
model = SARAutoencoder(latent_channels=16)
loss_fn = CombinedLoss()

# Create minimal loaders
class FakeLoader:
    def __init__(self, n, bs): self.n, self.bs = n, bs
    def __iter__(self):
        for _ in range(self.n // self.bs):
            yield torch.rand(self.bs, 1, 256, 256)
    def __len__(self): return self.n // self.bs

config = {'learning_rate': 1e-3, 'log_dir': 'test_runs', 'checkpoint_dir': 'test_ckpt', 'preprocessing_params': {'vmin': 14.77, 'vmax': 24.54}}
trainer = Trainer(model, FakeLoader(8, 4), FakeLoader(4, 4), loss_fn, config)

# Train 1 epoch
trainer.train(epochs=1, early_stopping_patience=100)
ckpt_path = str(trainer.checkpoint_dir / 'latest.pth')

# Load and verify
ckpt = torch.load(ckpt_path)
assert 'preprocessing_params' in ckpt, 'Missing preprocessing_params!'
assert ckpt['preprocessing_params']['vmin'] == 14.77
print('Checkpoint contains preprocessing_params: PASS')

# Cleanup
shutil.rmtree('test_runs', ignore_errors=True)
shutil.rmtree('test_ckpt', ignore_errors=True)
"
```

2. Verify TensorBoard logs created:
Check that runs/ directory contains logs after training.
</verification>

<success_criteria>
- Trainer initializes with optimizer (Adam, lr=1e-4), scheduler (ReduceLROnPlateau), TensorBoard writer
- train_epoch() iterates through train_loader with tqdm showing loss/PSNR/SSIM
- validate() evaluates on val_loader without gradients
- Progress bars display loss, PSNR, SSIM per CONTEXT.md
- TensorBoard logs scalars (train/val loss, PSNR, SSIM, LR) every epoch
- TensorBoard logs reconstruction images (triple view) every epoch
- TensorBoard logs weight histograms every 10 epochs
- Checkpoints include: model, optimizer, scheduler, config, preprocessing_params, history
- load_checkpoint() restores complete training state for resume
- Early stopping triggers after patience epochs without improvement
- GPU memory logged in epoch summary
- Training logs saved to file
</success_criteria>

<output>
After completion, create `.planning/phases/02-baseline-model/02-03-SUMMARY.md`
</output>
